<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
<title>The Data Science arXiv: Articles from 2019-12-31</title>

<meta property="description" itemprop="description" content="53 new data science research articles were published on 2019-12-31. 20 discussed machine learning."/>

<link rel="canonical" href="bryanwhiting.github.io/ds-arxiv/posts/2020-01-01/"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2020-01-01"/>
<meta property="article:created" itemprop="dateCreated" content="2020-01-01"/>
<meta name="article:author" content="Bryan Whiting"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="The Data Science arXiv: Articles from 2019-12-31"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="53 new data science research articles were published on 2019-12-31. 20 discussed machine learning."/>
<meta property="og:url" content="bryanwhiting.github.io/ds-arxiv/posts/2020-01-01/"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="The Data Science arXiv"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary"/>
<meta property="twitter:title" content="The Data Science arXiv: Articles from 2019-12-31"/>
<meta property="twitter:description" content="53 new data science research articles were published on 2019-12-31. 20 discussed machine learning."/>
<meta property="twitter:url" content="bryanwhiting.github.io/ds-arxiv/posts/2020-01-01/"/>

<!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","date","author","output","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["Articles from 2019-12-31"]},{"type":"character","attributes":{},"value":["53 new data science research articles were published on 2019-12-31. 20 discussed machine learning."]},{"type":"character","attributes":{},"value":["2020-01-01"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Bryan Whiting"]},{"type":"character","attributes":{},"value":["https://www.bryanwhiting.com"]}]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]}]}]},{"type":"character","attributes":{},"value":["bryanwhiting.github.io/ds-arxiv/posts/2020-01-01/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["arxiv.csv","news_files/bowser-1.9.3/bowser.min.js","news_files/distill-2.2.21/template.v2.js","news_files/jquery-1.11.3/jquery.min.js","news_files/kePrint-0.0.1/kePrint.js","news_files/webcomponents-2.0.0/webcomponents.js","news.Rmd.bak","output_df_summary.Rda","tweet.txt"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #455a64;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative;}
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

</style>

<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for table of contents */

.d-toc {
  color: rgba(0,0,0,0.8);
  font-size: 0.8em;
  line-height: 1em;
}

.d-toc-header {
  font-size: 0.6rem;
  font-weight: 400;
  color: rgba(0, 0, 0, 0.5);
  text-transform: uppercase;
  margin-top: 0;
  margin-bottom: 1.3em;
}

.d-toc a {
  border-bottom: none;
}

.d-toc ul {
  padding-left: 0;
}

.d-toc li>ul {
  padding-top: 0.8em;
  padding-left: 16px;
  margin-bottom: 0.6em;
}

.d-toc ul,
.d-toc li {
  list-style-type: none;
}

.d-toc li {
  margin-bottom: 0.9em;
}

.d-toc-separator {
  margin-top: 20px;
  margin-bottom: 2em;
}

.d-article-with-toc {
  border-top: none;
  padding-top: 0;
}



/* Tweak code blocks (note that this CSS is repeated above in an injection
   into the d-code shadow dom) */

d-code {
  overflow-x: auto !important;
}

pre.d-code code.d-code {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

pre.text-output {

  font-size: 12px;
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

@media(min-width: 768px) {

d-code {
  overflow-x: visible !important;
}

pre.d-code code.d-code  {
    padding-left: 18px;
    font-size: 14px;
}
pre.text-output {
  font-size: 14px;
}
}

/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}



/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}


/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // create d-bibliography
  var bibliography = $('<d-bibliography></d-bibliography>');
  $('#distill-bibliography').wrap(bibliography);

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace citations with <d-cite>
  $('.citation').each(function(i, val) {
    appendix = true;
    var cites = $(this).attr('data-cites').split(" ");
    var dt_cite = $('<d-cite></d-cite>');
    dt_cite.attr('key', cites.join());
    $(this).replaceWith(dt_cite);
  });
  // remove refs
  $('#refs').remove();

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-toc a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // replace code blocks with d-code
  $('pre>code').each(function(i, val) {
    var code = $(this);
    var pre = code.parent();
    var clz = "";
    var language = pre.attr('class');
    if (language) {
      // map unknown languages to "clike" (without this they just dissapear)
      if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                               "javascript", "js", "julia", "lua", "markdown",
                               "markup", "mathml", "python", "svg", "xml"]) == -1)
        language = "clike";
      language = ' language="' + language + '"';
      var dt_code = $('<d-code block' + language + clz + '></d-code>');
      dt_code.text(code.text());
      pre.replaceWith(dt_code);
    } else {
      code.addClass('text-output').unwrap().changeElementType('pre');
    }
  });

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('d-code, pre.text-output, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // table of contents
    if (have_authors) // adjust border if we are in authors
      $('.d-toc').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
    $('d-code').each(function(i, val) {
      var style = document.createElement('style');
      style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                        '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
      if (this.shadowRoot)
        this.shadowRoot.appendChild(style);
    });

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-toc-header').remove();
  $('.d-toc').remove();
  $('.d-toc-separator').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/kePrint-0.0.1/kePrint.js"></script>
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<style type="text/css">
.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #455a64;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

.distill-site-header {
}

.distill-site-footer {
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

a {
  color: #d9230f;
  text-decoration: none;
}
</style>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Articles from 2019-12-31","description":"53 new data science research articles were published on 2019-12-31. 20 discussed machine learning.","authors":[{"author":"Bryan Whiting","authorURL":"https://www.bryanwhiting.com","affiliation":"&nbsp;","affiliationURL":"#"}],"publishedDate":"2020-01-01T00:00:00.000-05:00","citationText":"Whiting, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a href="../../index.html" class="title">The Data Science arXiv</a>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../about.html">About</a>
<a href="https://github.com/bryanwhiting/ds-arxiv">GitHub</a>
<a href="../../index.xml">
<i class="fa fa-rss"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Articles from 2019-12-31</h1>
<p>53 new data science research articles were published on 2019-12-31. 20 discussed machine learning.</p>
</div>

<div class="d-byline">
  Bryan Whiting <a href="https://www.bryanwhiting.com" class="uri">https://www.bryanwhiting.com</a> 
  
<br/>2020-01-01
</div>

<div class="d-article">
<h3 class="d-toc-header">Table of Contents</h3>
<nav class="d-toc" id="TOC">
<ul>
<li><a href="#breakdown-of-arxiv-publication-counts">Breakdown of arXiv Publication Counts</a></li>
<li><a href="#articles-for-statitstics-machine-learning-econonmetrics-and-finance">Articles for Statitstics, Machine Learning Econonmetrics, and Finance</a><ul>
<li><a href="#applications--stat-ap-">Applications (stat.AP): 4 new</a></li>
<li><a href="#machine-learning--stat-ml-">Machine Learning (stat.ML): 16 new</a></li>
<li><a href="#machine-learning--cs-lg-">Machine Learning (cs.LG): 19 new</a></li>
</ul></li>
<li><a href="#data-science-arxiv-by-primary-tag">Data Science arXiv by Primary Tag</a><ul>
<li><a href="#computer-science">Computer Science</a></li>
<li><a href="#statistics">Statistics</a></li>
<li><a href="#mathematics">Mathematics</a></li>
<li><a href="#elec.-eng.-and-systems-science">Elec. Eng. and Systems Science</a></li>
<li><a href="#physics">Physics</a></li>
<li><a href="#quantitative-biology">Quantitative Biology</a></li>
<li><a href="#other">Other</a></li>
<li><a href="#quantum-physics">Quantum Physics</a></li>
</ul></li>
</ul>
</nav>
<hr class="d-toc-separator"/>
<h2 id="breakdown-of-arxiv-publication-counts">Breakdown of arXiv Publication Counts</h2>
<p>Yesterday’s counts of submitted papers on www.arxiv.org grouped by primary subject. Click the links in the table to be re-directed to the abstracts below. The links under <code>Subject</code> will redirect you to abstracts with the primary subject (there can only be one primary subject on arXiv). The links under <code>Category</code> will redirect you to all publications yesterday with a given tag (primary or secondary).</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:summary-table-with-counts">Table 1: </span>Number of articles by subject and primary category. Colored titles represent hyperlinks that take you below to abstracts. Key - Subject: Computer Science (5) means there were 5 articles with primary tag CS. Category: Machine Learning (cs.LG) N = 8 (16) means there were 8 primary articles with the (cs.LG) tag but 16 articles had it as a secondary tag, so there should be 24 in total. Click this link to be taken to all 24. Only select categories are highlighted because they are of particular interest to applied data scientists.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Subject
</th>
<th style="text-align:left;">
Category
</th>
<th style="text-align:left;">
N
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="8">
<a href="#computer-science" style=" font-weight: bold;    color: #d9230f !important;">Computer Science (26)</a>
</td>
<td style="text-align:left;">
<a href="#machine-learning--cs-lg-" style=" font-weight: bold;    color: #d9230f !important;">Machine Learning (cs.LG)</a>
</td>
<td style="text-align:left;">
8 (11)
</td>
</tr>
<tr>
<td style="text-align:left;">
Computer Vision and Pattern Recognition (cs.CV)
</td>
<td style="text-align:left;">
6 (6)
</td>
</tr>
<tr>
<td style="text-align:left;">
Artificial Intelligence (cs.AI)
</td>
<td style="text-align:left;">
6 (2)
</td>
</tr>
<tr>
<td style="text-align:left;">
Software Engineering (cs.SE)
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
Robotics (cs.RO)
</td>
<td style="text-align:left;">
1 (2)
</td>
</tr>
<tr>
<td style="text-align:left;">
Computation and Language (cs.CL)
</td>
<td style="text-align:left;">
1 (1)
</td>
</tr>
<tr>
<td style="text-align:left;">
Digital Libraries (cs.DL)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Emerging Technologies (cs.ET)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="4">
<a href="#statistics" style=" font-weight: bold;    color: #d9230f !important;">Statistics (11)</a>
</td>
<td style="text-align:left;">
<a href="#machine-learning--stat-ml-" style=" font-weight: bold;    color: #d9230f !important;">Machine Learning (stat.ML)</a>
</td>
<td style="text-align:left;">
4 (12)
</td>
</tr>
<tr>
<td style="text-align:left;">
Methodology (stat.ME)
</td>
<td style="text-align:left;">
3 (1)
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="#applications--stat-ap-" style=" font-weight: bold;    color: #d9230f !important;">Applications (stat.AP)</a>
</td>
<td style="text-align:left;">
2 (2)
</td>
</tr>
<tr>
<td style="text-align:left;">
Computation (stat.CO)
</td>
<td style="text-align:left;">
2 (1)
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="3">
<a href="#mathematics" style=" font-weight: bold;    color: #d9230f !important;">Mathematics (6)</a>
</td>
<td style="text-align:left;">
Statistics Theory (math.ST)
</td>
<td style="text-align:left;">
4 (1)
</td>
</tr>
<tr>
<td style="text-align:left;">
Optimization and Control (math.OC)
</td>
<td style="text-align:left;">
1 (3)
</td>
</tr>
<tr>
<td style="text-align:left;">
Numerical Analysis (math.NA)
</td>
<td style="text-align:left;">
1 (1)
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
<a href="#elec.-eng.%20and%20systems%20science" style=" font-weight: bold;    color: #d9230f !important;">Elec. Eng. and Systems Science (4)</a>
</td>
<td style="text-align:left;">
Image and Video Processing (eess.IV)
</td>
<td style="text-align:left;">
4 (1)
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="2">
<a href="#physics" style=" font-weight: bold;    color: #d9230f !important;">Physics (2)</a>
</td>
<td style="text-align:left;">
Computational Physics (physics.comp-ph)
</td>
<td style="text-align:left;">
1 (2)
</td>
</tr>
<tr>
<td style="text-align:left;">
Physics and Society (physics.soc-ph)
</td>
<td style="text-align:left;">
1 (1)
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="2">
<a href="#quantitative-biology" style=" font-weight: bold;    color: #d9230f !important;">Quantitative Biology (2)</a>
</td>
<td style="text-align:left;">
Biomolecules (q-bio.BM)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Populations and Evolution (q-bio.PE)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
<a href="#other" style=" font-weight: bold;    color: #d9230f !important;">Other (1)</a>
</td>
<td style="text-align:left;">
Earth and Planetary Astrophysics (astro-ph.EP)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
<a href="#quantum-physics" style=" font-weight: bold;    color: #d9230f !important;">Quantum Physics (1)</a>
</td>
<td style="text-align:left;">
Quantum Physics (quant-ph)
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
</table>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<h2 id="articles-for-statitstics-machine-learning-econonmetrics-and-finance">Articles for Statitstics, Machine Learning Econonmetrics, and Finance</h2>
<p>This section contains all articles with any tag of <code>stat.AP</code>, <code>stat.co</code>, <code>stat.ML</code>, <code>cs.LG</code>, <code>q-fin.ST</code>, <code>q-fin.EC</code>, or <code>econ-EM</code>. Only the first two sentences are shown - click the links for more detail.</p>
<div class="layout-chunk" data-layout="l-screen-inset">

<h3 id="applications--stat-ap-">Applications (stat.AP): 4 new</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="4">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Applications (stat.AP)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13242v1" style="color: #d9230f">Statistical Models in Forensic Voice Comparison</a></b><br><em>Audio and Speech Processing, Sound, Applications</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13242v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>This chapter describes a number of signal-processing and statistical-modeling techniques that are commonly used to calculate likelihood ratios in human-supervised automatic approaches to forensic voice comparison. Techniques described include mel-frequency cepstral coefficients (MFCCs) feature extraction, Gaussian mixture model - universal background model (GMM-UBM) systems, i-vector - probabilistic linear discriminant analysis (i-vector PLDA) systems, deep neural network (DNN) based systems (including senone posterior i-vectors, bottleneck features, and embeddings / x-vectors), mismatch compensation, and score-to-likelihood-ratio conversion (aka calibration). …</summary><br> Empirical validation of forensic-voice-comparison systems is also covered. The aim of the chapter is to bridge the gap between general introductions to forensic voice comparison and the highly technical automatic-speaker-recognition literature from which the signal-processing and statistical-modeling techniques are mostly drawn. Knowledge of the likelihood-ratio framework for the evaluation of forensic evidence is assumed. It is hoped that the material presented here will be of value to students of forensic voice comparison and to researchers interested in learning about statistical modeling techniques that could potentially also be applied to data from other branches of forensic science.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13188v1" style="color: #d9230f">On Testing for Biases in Peer Review</a></b><br><em>Statistics Theory, Methodology, Statistics Theory, Applications</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13188v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider the issue of biases in scholarly research, specifically, in peer review. There is a long standing debate on whether exposing author identities to reviewers induces biases against certain groups, and our focus is on designing tests to detect the presence of such biases. …</summary><br> Our starting point is a remarkable recent work by Tomkins, Zhang and Heavlin which conducted a controlled, large-scale experiment to investigate existence of biases in the peer reviewing of the WSDM conference. We present two sets of results in this paper. The first set of results is negative, and pertains to the statistical tests and the experimental setup used in the work of Tomkins et al. We show that the test employed therein does not guarantee control over false alarm probability and under correlations between relevant variables coupled with any of the following conditions, with high probability, can declare a presence of bias when it is in fact absent: (a) measurement error, (b) model mismatch, (c) reviewer calibration. Moreover, we show that the setup of their experiment may itself inflate false alarm probability if (d) bidding is performed in non-blind manner or (e) popular reviewer assignment procedure is employed. Our second set of results is positive and is built around a novel approach to testing for biases that we propose. We present a general framework for testing for biases in (single vs. double blind) peer review. We then design hypothesis tests that under minimal assumptions guarantee control over false alarm probability and non-trivial power even under conditions (a)–(c) as well as propose an alternative experimental setup which mitigates issues (d) and (e). Finally, we show that no statistical test can improve over the non-parametric tests we consider in terms of the assumptions required to control for the false alarm probability.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13351v1" style="color: #d9230f">Driver fatigue EEG signals detection by using robust univariate analysis</a></b><br><em>Signal Processing, Methodology, Applications</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13351v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Driver fatigue is a major cause of traffic accidents and the electroencephalogram (EEG) is considered one of the most reliable predictors of fatigue. This paper proposes a novel, simple and fast method for driver fatigue detection that can be implemented in real-time systems by using a single-channel on the scalp. …</summary><br> The method based on the robust univariate analysis of EEG signals is composed of two stages. First, the most significant channel from EEG raw is selected according to the maximum variance. In the second stage, this single channel will be used to detect the fatigue EEG signal by extracting four feature parameters. Two parameters estimated from the robust univariate analysis, namely mean and covariance, and two classical statistics parameters such as variance and covariance that help to tune the robust analysis. Next, an ensemble bagged decision trees classifier is used in order to discriminate fatigue signals from alert signals. The proposed algorithm is demonstrated on 24 EEG signals from the Jiangxi University of Technology database using only the most significant channel found, which is located in the left tempo-parietal region where spatial awareness and visual-spatial navigation are shared, in terms of 92.7% accuracy with 1.8 seconds of time delay.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13336v1" style="color: #d9230f">Numerical Linear Algebra in Data Assimilation</a></b><br><em>Numerical Analysis, Numerical Analysis, Computation, Applications</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13336v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Data assimilation is a method that combines observations (e.g. …</summary><br> real world data) of a state of a system with model output for that system in order to improve the estimate of the state of the system and thereby the model output. The model is usually represented by a discretised partial differential equation. The data assimilation problem can be formulated as a large scale Bayesian inverse problem. Based on this interpretation we will derive the most important variational and sequential data assimilation approaches, in particular three-dimensional and four-dimensional variational data assimilation (3D-Var and 4D-Var) and the Kalman filter. We will then consider more advanced methods which are extensions of the Kalman filter and variational data assimilation and will pay particular attention to their advantages and disadvantages. The data assimilation problem usually results in a very large optimisation problem and/or a very large linear system to solve (due to inclusion of time and space dimensions). Therefore, the second part of this article aims to review advances and challenges, in particular from the numerical linear algebra perspective, within the various data assimilation approaches.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="machine-learning--stat-ml-">Machine Learning (stat.ML): 16 new</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="16">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Machine Learning (stat.ML)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13230v1" style="color: #d9230f">Leveraging Semi-Supervised Learning for Fairness using Neural Networks</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13230v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>There has been a growing concern about the fairness of decision-making systems based on machine learning. The shortage of labeled data has been always a challenging problem facing machine learning based systems. …</summary><br> In such scenarios, semi-supervised learning has shown to be an effective way of exploiting unlabeled data to improve upon the performance of model. Notably, unlabeled data do not contain label information which itself can be a significant source of bias in training machine learning systems. This inspired us to tackle the challenge of fairness by formulating the problem in a semi-supervised framework. In this paper, we propose a semi-supervised algorithm using neural networks benefiting from unlabeled data to not just improve the performance but also improve the fairness of the decision-making process. The proposed model, called SSFair, exploits the information in the unlabeled data to mitigate the bias in the training data.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13335v1" style="color: #d9230f">Volumetric Lung Nodule Segmentation using Adaptive ROI with Multi-View Residual Learning</a></b><br><em>Image and Video Processing, Machine Learning, Computer Vision and Pattern Recognition, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13335v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Accurate quantification of pulmonary nodules can greatly assist the early diagnosis of lung cancer, which can enhance patient survival possibilities. A number of nodule segmentation techniques have been proposed, however, all of the existing techniques rely on radiologist 3-D volume of interest (VOI) input or use the constant region of interest (ROI) and only investigate the presence of nodule voxels within the given VOI. …</summary><br> Such approaches restrain the solutions to investigate the nodule presence outside the given VOI and also include the redundant structures into VOI, which may lead to inaccurate nodule segmentation. In this work, a novel semi-automated approach for 3-D segmentation of nodule in volumetric computerized tomography (CT) lung scans has been proposed. The proposed technique can be segregated into two stages, at the first stage, it takes a 2-D ROI containing the nodule as input and it performs patch-wise investigation along the axial axis with a novel adaptive ROI strategy. The adaptive ROI algorithm enables the solution to dynamically select the ROI for the surrounding slices to investigate the presence of nodule using deep residual U-Net architecture. The first stage provides the initial estimation of nodule which is further utilized to extract the VOI. At the second stage, the extracted VOI is further investigated along the coronal and sagittal axis with two different networks and finally, all the estimated masks are fed into the consensus module to produce the final volumetric segmentation of nodule. The proposed approach has been rigorously evaluated on the LIDC dataset, which is the largest publicly available dataset. The result suggests that the approach is significantly robust and accurate as compared to the previous state of the art techniques.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13408v1" style="color: #d9230f">On the Role of Weight Sharing During Deep Option Learning</a></b><br><em>Machine Learning, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13408v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>The options framework is a popular approach for building temporally extended actions in reinforcement learning. In particular, the option-critic architecture provides general purpose policy gradient theorems for learning actions from scratch that are extended in time. …</summary><br> However, past work makes the key assumption that each of the components of option-critic has independent parameters. In this work we note that while this key assumption of the policy gradient theorems of option-critic holds in the tabular case, it is always violated in practice for the deep function approximation setting. We thus reconsider this assumption and consider more general extensions of option-critic and hierarchical option-critic training that optimize for the full architecture with each update. It turns out that not assuming parameter independence challenges a belief in prior work that training the policy over options can be disentangled from the dynamics of the underlying options. In fact, learning can be sped up by focusing the policy over options on states where options are actually likely to terminate. We put our new algorithms to the test in application to sample efficient learning of Atari games, and demonstrate significantly improved stability and faster convergence when learning long options.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13170v1" style="color: #d9230f">Schrödinger Bridge Samplers</a></b><br><em>Computation, Machine Learning</em>. 4 authors. <a href="http://arxiv.org/pdf/1912.13170v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Consider a reference Markov process with initial distribution <span class="math inline">\(\pi_{0}\)</span> and transition kernels <span class="math inline">\(\{M_{t}\}_{t\in[1:T]}\)</span>, for some <span class="math inline">\(T\in\mathbb{N}\)</span>. Assume that you are given distribution <span class="math inline">\(\pi_{T}\)</span>, which is not equal to the marginal distribution of the reference process at time <span class="math inline">\(T\)</span>. …</summary><br> In this scenario, Schr&quot;odinger addressed the problem of identifying the Markov process with initial distribution <span class="math inline">\(\pi_{0}\)</span> and terminal distribution equal to <span class="math inline">\(\pi_{T}\)</span> which is the closest to the reference process in terms of Kullback–Leibler divergence. This special case of the so-called Schr&quot;odinger bridge problem can be solved using iterative proportional fitting, also known as the Sinkhorn algorithm. We leverage these ideas to develop novel Monte Carlo schemes, termed Schr&quot;odinger bridge samplers, to approximate a target distribution <span class="math inline">\(\pi\)</span> on <span class="math inline">\(\mathbb{R}^{d}\)</span> and to estimate its normalizing constant. This is achieved by iteratively modifying the transition kernels of the reference Markov chain to obtain a process whose marginal distribution at time <span class="math inline">\(T\)</span> becomes closer to <span class="math inline">\(\pi_T = \pi\)</span>, via regression-based approximations of the corresponding iterative proportional fitting recursion. We report preliminary experiments and make connections with other problems arising in the optimal transport, optimal control and physics literatures.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13515v1" style="color: #d9230f">Stochastic Recursive Variance Reduction for Efficient Smooth Non-Convex Compositional Optimization</a></b><br><em>Machine Learning, Machine Learning, Optimization and Control</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13515v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Stochastic compositional optimization arises in many important machine learning tasks such as value function evaluation in reinforcement learning and portfolio management. The objective function is the composition of two expectations of stochastic functions, and is more challenging to optimize than vanilla stochastic optimization problems. …</summary><br> In this paper, we investigate the stochastic compositional optimization in the general smooth non-convex setting. We employ a recently developed idea of  to design a novel algorithm named SARAH-Compositional, and prove a sharp Incremental First-order Oracle (IFO) complexity upper bound for stochastic compositional optimization: <span class="math inline">\(\mathcal{O}((n+m)^{1/2} \varepsilon^{-2})\)</span> in the finite-sum case and <span class="math inline">\(\mathcal{O}(\varepsilon^{-3})\)</span> in the online case. Such a complexity is known to be the best one among IFO complexity results for non-convex stochastic compositional optimization, and is believed to be optimal. Our experiments validate the theoretical performance of our algorithm.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13472v1" style="color: #d9230f">Revisiting Landscape Analysis in Deep Neural Networks: Eliminating Decreasing Paths to Infinity</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13472v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Traditional landscape analysis of deep neural networks aims to show that no sub-optimal local minima exist in some appropriate sense. From this, one may be tempted to conclude that descent algorithms which escape saddle points will reach a good local minimum. …</summary><br> However, basic optimization theory tell us that it is also possible for a descent algorithm to diverge to infinity if there are paths leading to infinity, along which the loss function decreases. It is not clear whether for non-linear neural networks there exists one setting that no bad local-min and no decreasing paths to infinity can be simultaneously achieved. In this paper, we give the first positive answer to this question. More specifically, for a large class of over-parameterized deep neural networks with appropriate regularizers, the loss function has no bad local minima and no decreasing paths to infinity. The key mathematical trick is to show that the set of regularizers which may be undesirable can be viewed as the image of a Lipschitz continuous mapping from a lower-dimensional Euclidean space to a higher-dimensional Euclidean space, and thus has zero measure.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13465v1" style="color: #d9230f">Reward-Conditioned Policies</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13465v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Reinforcement learning offers the promise of automating the acquisition of complex behavioral skills. However, compared to commonly used and well-understood supervised learning methods, reinforcement learning algorithms can be brittle, difficult to use and tune, and sensitive to seemingly innocuous implementation decisions. …</summary><br> In contrast, imitation learning utilizes standard and well-understood supervised learning methods, but requires near-optimal expert data. Can we learn effective policies via supervised learning without demonstrations? The main idea that we explore in this work is that non-expert trajectories collected from sub-optimal policies can be viewed as optimal supervision, not for maximizing the reward, but for matching the reward of the given trajectory. By then conditioning the policy on the numerical value of the reward, we can obtain a policy that generalizes to larger returns. We show how such an approach can be derived as a principled method for policy search, discuss several variants, and compare the method experimentally to a variety of current reinforcement learning methods on standard benchmarks.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13445v1" style="color: #d9230f">Robust Aggregation for Federated Learning</a></b><br><em>Cryptography and Security, Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13445v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We present a robust aggregation approach to make federated learning robust to settings when a fraction of the devices may be sending corrupted updates to the server. The proposed approach relies on a robust secure aggregation oracle based on the geometric median, which returns a robust aggregate using a constant number of calls to a regular non-robust secure average oracle. …</summary><br> The robust aggregation oracle is privacy-preserving, similar to the secure average oracle it builds upon. We provide experimental results of the proposed approach with linear models and deep networks for two tasks in computer vision and natural language processing. The robust aggregation approach is agnostic to the level of corruption; it outperforms the classical aggregation approach in terms of robustness when the level of corruption is high, while being competitive in the regime of low corruption.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13490v1" style="color: #d9230f">Representation Internal-Manipulation (RIM): A Neuro-Inspired Computational Theory of Consciousness</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning, Neurons and Cognition, Neural and Evolutionary Computing</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13490v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Many theories, based on neuroscientific and psychological empirical evidence and on computational concepts, have been elaborated to explain the emergence of consciousness in the central nervous system. These theories propose key fundamental mechanisms to explain consciousness, but they only partially connect such mechanisms to the possible functional and adaptive role of consciousness. …</summary><br> Recently, some cognitive and neuroscientific models try to solve this gap by linking consciousness to various aspects of goal-directed behaviour, the pivotal cognitive process that allows mammals to flexibly act in challenging environments. Here we propose the Representation Internal-Manipulation (RIM) theory of consciousness, a theory that links the main elements of consciousness theories to components and functions of goal-directed behaviour, ascribing a central role for consciousness to the goal-directed manipulation of internal representations. This manipulation relies on four specific computational operations to perform the flexible internal adaptation of all key elements of goal-directed computation, from the representations of objects to those of goals, actions, and plans. Finally, we propose the concept of `manipulation agency’ relating the sense of agency to the internal manipulation of representations. This allows us to propose that the subjective experience of consciousness is associated to the human capacity to generate and control a simulated internal reality that is vividly perceived and felt through the same perceptual and emotional mechanisms used to tackle the external world.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13480v1" style="color: #d9230f">On the Difference Between the Information Bottleneck and the Deep Information Bottleneck</a></b><br><em>Information Theory, Machine Learning, Information Theory, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13480v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Combining the Information Bottleneck model with deep learning by replacing mutual information terms with deep neural nets has proved successful in areas ranging from generative modelling to interpreting deep neural networks. In this paper, we revisit the Deep Variational Information Bottleneck and the assumptions needed for its derivation. …</summary><br> The two assumed properties of the data <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and their latent representation <span class="math inline">\(T\)</span> take the form of two Markov chains <span class="math inline">\(T-X-Y\)</span> and <span class="math inline">\(X-T-Y\)</span>. Requiring both to hold during the optimisation process can be limiting for the set of potential joint distributions <span class="math inline">\(P(X,Y,T)\)</span>. We therefore show how to circumvent this limitation by optimising a lower bound for <span class="math inline">\(I(T;Y)\)</span> for which only the latter Markov chain has to be satisfied. The actual mutual information consists of the lower bound which is optimised in DVIB and cognate models in practice and of two terms measuring how much the former requirement <span class="math inline">\(T-X-Y\)</span> is violated. Finally, we propose to interpret the family of information bottleneck models as directed graphical models and show that in this framework the original and deep information bottlenecks are special cases of a fundamental IB model.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13464v1" style="color: #d9230f">Model Inversion Networks for Model-Based Optimization</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13464v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In this work, we aim to solve data-driven optimization problems, where the goal is to find an input that maximizes an unknown score function given access to a dataset of inputs with corresponding scores. When the inputs are high-dimensional and valid inputs constitute a small subset of this space (e. …</summary><br>g., valid protein sequences or valid natural images), such model-based optimization problems become exceptionally difficult, since the optimizer must avoid out-of-distribution and invalid inputs. We propose to address such problem with model inversion networks (MINs), which learn an inverse mapping from scores to inputs. MINs can scale to high-dimensional input spaces and leverage offline logged data for both contextual and non-contextual optimization problems. MINs can also handle both purely offline data sources and active data collection. We evaluate MINs on tasks from the Bayesian optimization literature, high-dimensional model-based optimization problems over images and protein designs, and contextual bandit optimization from logged data.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13440v1" style="color: #d9230f">Approximate Inference for Fully Bayesian Gaussian Process Regression</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13440v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Learning in Gaussian Process models occurs through the adaptation of hyperparameters of the mean and the covariance function. The classical approach entails maximizing the marginal likelihood yielding fixed point estimates (an approach called  or ML-II). …</summary><br> An alternative learning procedure is to infer the posterior over hyperparameters in a hierarchical specification of GPs we call  (GPR). This work considers two approximation schemes for the intractable hyperparameter posterior: 1) Hamiltonian Monte Carlo (HMC) yielding a sampling-based approximation and 2) Variational Inference (VI) where the posterior over hyperparameters is approximated by a factorized Gaussian (mean-field) or a full-rank Gaussian accounting for correlations between hyperparameters. We analyze the predictive performance for fully Bayesian GPR on a range of benchmark data sets.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13421v1" style="color: #d9230f">Asymptotic Risk of Least Squares Minimum Norm Estimator under the Spike Covariance Model</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13421v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>One of the recent approaches to explain good performance of neural networks has focused on their ability to fit training data perfectly (interpolate) without overfitting. It has been shown that this is not unique to neural nets, and that it happens with simpler models such as kernel regression, too Belkin et al. …</summary><br> (2018b); Tengyuan Liang (2018). Consequently, there has been quite a few works that give conditions for low risk or optimality of interpolating models, see for example Belkin et al. (2018a, 2019b). One of the simpler models where interpolation has been studied recently is least squares solution for linear regression. In this case, interpolation is only guaranteed to happen in high dimensional setting where the number of parameters exceeds number of samples; therefore, least squares solution is not necessarily unique. However, minimum norm solution is unique, can be written in closed form, and gradient descent starting at the origin converges to it (Hastie et al., 2019). This has, at least partially, motivated several works that study risk of minimum norm least squares estimator for linear regression. Continuing in a similar vein, we study the asymptotic risk of minimum norm least squares estimator when number of parameters <span class="math inline">\(d\)</span> depends on <span class="math inline">\(n\)</span>, and <span class="math inline">\(\frac{d}{n} \rightarrow \infty\)</span>. In this high dimensional setting, to make inference feasible, it is usually assumed that true parameters or data have some underlying low dimensional structure such as sparsity, or vanishing eigenvalues of population covariance matrix. Here, we restrict ourselves to spike covariance matrices, where a fixed finite number of eigenvalues grow with <span class="math inline">\(n\)</span> and are much larger than the rest of the eigenvalues, which are (asymptotically) in the same order. We show that in this setting the risk can vanish.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13357v1" style="color: #d9230f">A Dynamic Sampling Adaptive-SGD Method for Machine Learning</a></b><br><em>Machine Learning, Machine Learning, Optimization and Control</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13357v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We propose a stochastic optimization method for minimizing loss functions, which can be expressed as an expected value, that adaptively controls the batch size used in the computation of gradient approximations and the step size used to move along such directions, eliminating the need for the user to tune the learning rate. The proposed method exploits local curvature information and ensures that search directions are descent directions with high probability using an acute-angle test. …</summary><br> The method is proved to have, under reasonable assumptions, a global linear rate of convergence on self-concordant functions with high probability. Numerical experiments show that this method is able to choose the best learning rates and compares favorably to fine-tuned SGD for training logistic regression and Deep Neural Networks (DNNs). We also propose an adaptive version of ADAM that eliminates the need to tune the base learning rate and compares favorably to fine-tuned ADAM for training DNNs.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13263v1" style="color: #d9230f">Intrinsic motivations and open-ended learning</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13263v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>There is a growing interest and literature on intrinsic motivations and open-ended learning in both cognitive robotics and machine learning on one side, and in psychology and neuroscience on the other. This paper aims to review some relevant contributions from the two literature threads and to draw links between them. …</summary><br> To this purpose, the paper starts by defining intrinsic motivations and by presenting a computationally-driven theoretical taxonomy of their different types. Then it presents relevant contributions from the psychological and neuroscientific literature related to intrinsic motivations, interpreting them based on the grid, and elucidates the mechanisms and functions they play in animals and humans. Endowed with such concepts and their biological underpinnings, the paper next presents a selection of models from cognitive robotics and machine learning that computationally operationalise the concepts of intrinsic motivations and links them to biology concepts. The contribution finally presents some of the open challenges of the field from both the psychological/neuroscientific and computational perspectives.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13463v1" style="color: #d9230f">Some compact notations for concentration inequalities and user-friendly results</a></b><br><em>Information Theory, Statistics Theory, Machine Learning, Information Theory, Machine Learning, Statistics Theory, Probability</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13463v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>This paper presents compact notations for concentration inequalities and convenient results to streamline probabilistic analysis. The new expressions describe the typical sizes and tails of random variables, allowing for simple operations without heavy use of inessential constants. …</summary><br> They bridge classical asymptotic notations and modern non-asymptotic tail bounds together. Examples of different kinds demonstrate their efficacy.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="machine-learning--cs-lg-">Machine Learning (cs.LG): 19 new</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="19">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Machine Learning (cs.LG)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13230v1" style="color: #d9230f">Leveraging Semi-Supervised Learning for Fairness using Neural Networks</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13230v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>There has been a growing concern about the fairness of decision-making systems based on machine learning. The shortage of labeled data has been always a challenging problem facing machine learning based systems. …</summary><br> In such scenarios, semi-supervised learning has shown to be an effective way of exploiting unlabeled data to improve upon the performance of model. Notably, unlabeled data do not contain label information which itself can be a significant source of bias in training machine learning systems. This inspired us to tackle the challenge of fairness by formulating the problem in a semi-supervised framework. In this paper, we propose a semi-supervised algorithm using neural networks benefiting from unlabeled data to not just improve the performance but also improve the fairness of the decision-making process. The proposed model, called SSFair, exploits the information in the unlabeled data to mitigate the bias in the training data.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13503v1" style="color: #d9230f">Side-Tuning: Network Adaptation via Additive Side Networks</a></b><br><em>Neural and Evolutionary Computing, Machine Learning, Computer Vision and Pattern Recognition, Robotics</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13503v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than start with a randomly initialized one – due to lacking enough training data, performing lifelong learning where the system has to learn a new task while being previously trained for other tasks, or wishing to encode priors in the network via preset weights. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others. …</summary><br> In this paper, we propose a straightforward alternative: Side-Tuning. Side-tuning adapts a pre-trained network by training a lightweight “side” network that is fused with the (unchanged) pre-trained network using summation. This simple method works as well as or better than existing solutions while it resolves some of the basic issues with fine-tuning, fixed features, and several other common baselines. In particular, side-tuning is less prone to overfitting when little training data is available, yields better results than using a fixed feature extractor, and does not suffer from catastrophic forgetting in lifelong learning. We demonstrate the performance of side-tuning under a diverse set of scenarios, including lifelong learning (iCIFAR, Taskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), with consistently promising results.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13335v1" style="color: #d9230f">Volumetric Lung Nodule Segmentation using Adaptive ROI with Multi-View Residual Learning</a></b><br><em>Image and Video Processing, Machine Learning, Computer Vision and Pattern Recognition, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13335v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Accurate quantification of pulmonary nodules can greatly assist the early diagnosis of lung cancer, which can enhance patient survival possibilities. A number of nodule segmentation techniques have been proposed, however, all of the existing techniques rely on radiologist 3-D volume of interest (VOI) input or use the constant region of interest (ROI) and only investigate the presence of nodule voxels within the given VOI. …</summary><br> Such approaches restrain the solutions to investigate the nodule presence outside the given VOI and also include the redundant structures into VOI, which may lead to inaccurate nodule segmentation. In this work, a novel semi-automated approach for 3-D segmentation of nodule in volumetric computerized tomography (CT) lung scans has been proposed. The proposed technique can be segregated into two stages, at the first stage, it takes a 2-D ROI containing the nodule as input and it performs patch-wise investigation along the axial axis with a novel adaptive ROI strategy. The adaptive ROI algorithm enables the solution to dynamically select the ROI for the surrounding slices to investigate the presence of nodule using deep residual U-Net architecture. The first stage provides the initial estimation of nodule which is further utilized to extract the VOI. At the second stage, the extracted VOI is further investigated along the coronal and sagittal axis with two different networks and finally, all the estimated masks are fed into the consensus module to produce the final volumetric segmentation of nodule. The proposed approach has been rigorously evaluated on the LIDC dataset, which is the largest publicly available dataset. The result suggests that the approach is significantly robust and accurate as compared to the previous state of the art techniques.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13408v1" style="color: #d9230f">On the Role of Weight Sharing During Deep Option Learning</a></b><br><em>Machine Learning, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13408v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>The options framework is a popular approach for building temporally extended actions in reinforcement learning. In particular, the option-critic architecture provides general purpose policy gradient theorems for learning actions from scratch that are extended in time. …</summary><br> However, past work makes the key assumption that each of the components of option-critic has independent parameters. In this work we note that while this key assumption of the policy gradient theorems of option-critic holds in the tabular case, it is always violated in practice for the deep function approximation setting. We thus reconsider this assumption and consider more general extensions of option-critic and hierarchical option-critic training that optimize for the full architecture with each update. It turns out that not assuming parameter independence challenges a belief in prior work that training the policy over options can be disentangled from the dynamics of the underlying options. In fact, learning can be sped up by focusing the policy over options on states where options are actually likely to terminate. We put our new algorithms to the test in application to sample efficient learning of Atari games, and demonstrate significantly improved stability and faster convergence when learning long options.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13283v1" style="color: #d9230f">oLMpics – On what Language Model Pre-training Captures</a></b><br><em>Computation and Language, Machine Learning, Artificial Intelligence</em>. 4 authors. <a href="http://arxiv.org/pdf/1912.13283v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. …</summary><br> In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13515v1" style="color: #d9230f">Stochastic Recursive Variance Reduction for Efficient Smooth Non-Convex Compositional Optimization</a></b><br><em>Machine Learning, Machine Learning, Optimization and Control</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13515v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Stochastic compositional optimization arises in many important machine learning tasks such as value function evaluation in reinforcement learning and portfolio management. The objective function is the composition of two expectations of stochastic functions, and is more challenging to optimize than vanilla stochastic optimization problems. …</summary><br> In this paper, we investigate the stochastic compositional optimization in the general smooth non-convex setting. We employ a recently developed idea of  to design a novel algorithm named SARAH-Compositional, and prove a sharp Incremental First-order Oracle (IFO) complexity upper bound for stochastic compositional optimization: <span class="math inline">\(\mathcal{O}((n+m)^{1/2} \varepsilon^{-2})\)</span> in the finite-sum case and <span class="math inline">\(\mathcal{O}(\varepsilon^{-3})\)</span> in the online case. Such a complexity is known to be the best one among IFO complexity results for non-convex stochastic compositional optimization, and is believed to be optimal. Our experiments validate the theoretical performance of our algorithm.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13472v1" style="color: #d9230f">Revisiting Landscape Analysis in Deep Neural Networks: Eliminating Decreasing Paths to Infinity</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13472v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Traditional landscape analysis of deep neural networks aims to show that no sub-optimal local minima exist in some appropriate sense. From this, one may be tempted to conclude that descent algorithms which escape saddle points will reach a good local minimum. …</summary><br> However, basic optimization theory tell us that it is also possible for a descent algorithm to diverge to infinity if there are paths leading to infinity, along which the loss function decreases. It is not clear whether for non-linear neural networks there exists one setting that no bad local-min and no decreasing paths to infinity can be simultaneously achieved. In this paper, we give the first positive answer to this question. More specifically, for a large class of over-parameterized deep neural networks with appropriate regularizers, the loss function has no bad local minima and no decreasing paths to infinity. The key mathematical trick is to show that the set of regularizers which may be undesirable can be viewed as the image of a Lipschitz continuous mapping from a lower-dimensional Euclidean space to a higher-dimensional Euclidean space, and thus has zero measure.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13465v1" style="color: #d9230f">Reward-Conditioned Policies</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13465v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Reinforcement learning offers the promise of automating the acquisition of complex behavioral skills. However, compared to commonly used and well-understood supervised learning methods, reinforcement learning algorithms can be brittle, difficult to use and tune, and sensitive to seemingly innocuous implementation decisions. …</summary><br> In contrast, imitation learning utilizes standard and well-understood supervised learning methods, but requires near-optimal expert data. Can we learn effective policies via supervised learning without demonstrations? The main idea that we explore in this work is that non-expert trajectories collected from sub-optimal policies can be viewed as optimal supervision, not for maximizing the reward, but for matching the reward of the given trajectory. By then conditioning the policy on the numerical value of the reward, we can obtain a policy that generalizes to larger returns. We show how such an approach can be derived as a principled method for policy search, discuss several variants, and compare the method experimentally to a variety of current reinforcement learning methods on standard benchmarks.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13445v1" style="color: #d9230f">Robust Aggregation for Federated Learning</a></b><br><em>Cryptography and Security, Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13445v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We present a robust aggregation approach to make federated learning robust to settings when a fraction of the devices may be sending corrupted updates to the server. The proposed approach relies on a robust secure aggregation oracle based on the geometric median, which returns a robust aggregate using a constant number of calls to a regular non-robust secure average oracle. …</summary><br> The robust aggregation oracle is privacy-preserving, similar to the secure average oracle it builds upon. We provide experimental results of the proposed approach with linear models and deep networks for two tasks in computer vision and natural language processing. The robust aggregation approach is agnostic to the level of corruption; it outperforms the classical aggregation approach in terms of robustness when the level of corruption is high, while being competitive in the regime of low corruption.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13490v1" style="color: #d9230f">Representation Internal-Manipulation (RIM): A Neuro-Inspired Computational Theory of Consciousness</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning, Neurons and Cognition, Neural and Evolutionary Computing</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13490v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Many theories, based on neuroscientific and psychological empirical evidence and on computational concepts, have been elaborated to explain the emergence of consciousness in the central nervous system. These theories propose key fundamental mechanisms to explain consciousness, but they only partially connect such mechanisms to the possible functional and adaptive role of consciousness. …</summary><br> Recently, some cognitive and neuroscientific models try to solve this gap by linking consciousness to various aspects of goal-directed behaviour, the pivotal cognitive process that allows mammals to flexibly act in challenging environments. Here we propose the Representation Internal-Manipulation (RIM) theory of consciousness, a theory that links the main elements of consciousness theories to components and functions of goal-directed behaviour, ascribing a central role for consciousness to the goal-directed manipulation of internal representations. This manipulation relies on four specific computational operations to perform the flexible internal adaptation of all key elements of goal-directed computation, from the representations of objects to those of goals, actions, and plans. Finally, we propose the concept of `manipulation agency’ relating the sense of agency to the internal manipulation of representations. This allows us to propose that the subjective experience of consciousness is associated to the human capacity to generate and control a simulated internal reality that is vividly perceived and felt through the same perceptual and emotional mechanisms used to tackle the external world.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13480v1" style="color: #d9230f">On the Difference Between the Information Bottleneck and the Deep Information Bottleneck</a></b><br><em>Information Theory, Machine Learning, Information Theory, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13480v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Combining the Information Bottleneck model with deep learning by replacing mutual information terms with deep neural nets has proved successful in areas ranging from generative modelling to interpreting deep neural networks. In this paper, we revisit the Deep Variational Information Bottleneck and the assumptions needed for its derivation. …</summary><br> The two assumed properties of the data <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and their latent representation <span class="math inline">\(T\)</span> take the form of two Markov chains <span class="math inline">\(T-X-Y\)</span> and <span class="math inline">\(X-T-Y\)</span>. Requiring both to hold during the optimisation process can be limiting for the set of potential joint distributions <span class="math inline">\(P(X,Y,T)\)</span>. We therefore show how to circumvent this limitation by optimising a lower bound for <span class="math inline">\(I(T;Y)\)</span> for which only the latter Markov chain has to be satisfied. The actual mutual information consists of the lower bound which is optimised in DVIB and cognate models in practice and of two terms measuring how much the former requirement <span class="math inline">\(T-X-Y\)</span> is violated. Finally, we propose to interpret the family of information bottleneck models as directed graphical models and show that in this framework the original and deep information bottlenecks are special cases of a fundamental IB model.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13464v1" style="color: #d9230f">Model Inversion Networks for Model-Based Optimization</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13464v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In this work, we aim to solve data-driven optimization problems, where the goal is to find an input that maximizes an unknown score function given access to a dataset of inputs with corresponding scores. When the inputs are high-dimensional and valid inputs constitute a small subset of this space (e. …</summary><br>g., valid protein sequences or valid natural images), such model-based optimization problems become exceptionally difficult, since the optimizer must avoid out-of-distribution and invalid inputs. We propose to address such problem with model inversion networks (MINs), which learn an inverse mapping from scores to inputs. MINs can scale to high-dimensional input spaces and leverage offline logged data for both contextual and non-contextual optimization problems. MINs can also handle both purely offline data sources and active data collection. We evaluate MINs on tasks from the Bayesian optimization literature, high-dimensional model-based optimization problems over images and protein designs, and contextual bandit optimization from logged data.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13440v1" style="color: #d9230f">Approximate Inference for Fully Bayesian Gaussian Process Regression</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13440v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Learning in Gaussian Process models occurs through the adaptation of hyperparameters of the mean and the covariance function. The classical approach entails maximizing the marginal likelihood yielding fixed point estimates (an approach called  or ML-II). …</summary><br> An alternative learning procedure is to infer the posterior over hyperparameters in a hierarchical specification of GPs we call  (GPR). This work considers two approximation schemes for the intractable hyperparameter posterior: 1) Hamiltonian Monte Carlo (HMC) yielding a sampling-based approximation and 2) Variational Inference (VI) where the posterior over hyperparameters is approximated by a factorized Gaussian (mean-field) or a full-rank Gaussian accounting for correlations between hyperparameters. We analyze the predictive performance for fully Bayesian GPR on a range of benchmark data sets.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13421v1" style="color: #d9230f">Asymptotic Risk of Least Squares Minimum Norm Estimator under the Spike Covariance Model</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13421v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>One of the recent approaches to explain good performance of neural networks has focused on their ability to fit training data perfectly (interpolate) without overfitting. It has been shown that this is not unique to neural nets, and that it happens with simpler models such as kernel regression, too Belkin et al. …</summary><br> (2018b); Tengyuan Liang (2018). Consequently, there has been quite a few works that give conditions for low risk or optimality of interpolating models, see for example Belkin et al. (2018a, 2019b). One of the simpler models where interpolation has been studied recently is least squares solution for linear regression. In this case, interpolation is only guaranteed to happen in high dimensional setting where the number of parameters exceeds number of samples; therefore, least squares solution is not necessarily unique. However, minimum norm solution is unique, can be written in closed form, and gradient descent starting at the origin converges to it (Hastie et al., 2019). This has, at least partially, motivated several works that study risk of minimum norm least squares estimator for linear regression. Continuing in a similar vein, we study the asymptotic risk of minimum norm least squares estimator when number of parameters <span class="math inline">\(d\)</span> depends on <span class="math inline">\(n\)</span>, and <span class="math inline">\(\frac{d}{n} \rightarrow \infty\)</span>. In this high dimensional setting, to make inference feasible, it is usually assumed that true parameters or data have some underlying low dimensional structure such as sparsity, or vanishing eigenvalues of population covariance matrix. Here, we restrict ourselves to spike covariance matrices, where a fixed finite number of eigenvalues grow with <span class="math inline">\(n\)</span> and are much larger than the rest of the eigenvalues, which are (asymptotically) in the same order. We show that in this setting the risk can vanish.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13357v1" style="color: #d9230f">A Dynamic Sampling Adaptive-SGD Method for Machine Learning</a></b><br><em>Machine Learning, Machine Learning, Optimization and Control</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13357v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We propose a stochastic optimization method for minimizing loss functions, which can be expressed as an expected value, that adaptively controls the batch size used in the computation of gradient approximations and the step size used to move along such directions, eliminating the need for the user to tune the learning rate. The proposed method exploits local curvature information and ensures that search directions are descent directions with high probability using an acute-angle test. …</summary><br> The method is proved to have, under reasonable assumptions, a global linear rate of convergence on self-concordant functions with high probability. Numerical experiments show that this method is able to choose the best learning rates and compares favorably to fine-tuned SGD for training logistic regression and Deep Neural Networks (DNNs). We also propose an adaptive version of ADAM that eliminates the need to tune the base learning rate and compares favorably to fine-tuned ADAM for training DNNs.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13263v1" style="color: #d9230f">Intrinsic motivations and open-ended learning</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13263v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>There is a growing interest and literature on intrinsic motivations and open-ended learning in both cognitive robotics and machine learning on one side, and in psychology and neuroscience on the other. This paper aims to review some relevant contributions from the two literature threads and to draw links between them. …</summary><br> To this purpose, the paper starts by defining intrinsic motivations and by presenting a computationally-driven theoretical taxonomy of their different types. Then it presents relevant contributions from the psychological and neuroscientific literature related to intrinsic motivations, interpreting them based on the grid, and elucidates the mechanisms and functions they play in animals and humans. Endowed with such concepts and their biological underpinnings, the paper next presents a selection of models from cognitive robotics and machine learning that computationally operationalise the concepts of intrinsic motivations and links them to biology concepts. The contribution finally presents some of the open challenges of the field from both the psychological/neuroscientific and computational perspectives.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13122v1" style="color: #d9230f">Towards Regulated Deep Learning</a></b><br><em>Machine Learning, Artificial Intelligence, Multiagent Systems, Logic in Computer Science, Programming Languages</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13122v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Regulation of Multi-Agent Systems (MAS) was a research topic of the past decade and one of these proposals was Electronic Institutions. However, with the recent reformulation of Artificial Neural Networks (ANN) as Deep Learning (DL), Security, Privacy, Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. …</summary><br> Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of ANN as Agent-based Training of a special type of regulated ANN that we call Institutional Neural Network. This paper introduces the former concept and provides <span class="math inline">\(\mathcal{I}\)</span>, a language previously used to model and extend Electronic Institutions, as a means to implement and regulate DL.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13494v1" style="color: #d9230f">A frequency-domain analysis of inexact gradient descent</a></b><br><em>Numerical Analysis, Optimization and Control, Machine Learning, Numerical Analysis, Systems and Control</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13494v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We study robustness properties of inexact gradient descent for strongly convex functions, as well as for the larger class of functions with sector-bounded gradients, under a relative error model. Proofs of the corresponding convergence rates are based on frequency-domain criteria for the stability of nonlinear systems perturbed by additive noise. …</summary><br>
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13463v1" style="color: #d9230f">Some compact notations for concentration inequalities and user-friendly results</a></b><br><em>Information Theory, Statistics Theory, Machine Learning, Information Theory, Machine Learning, Statistics Theory, Probability</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13463v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>This paper presents compact notations for concentration inequalities and convenient results to streamline probabilistic analysis. The new expressions describe the typical sizes and tails of random variables, allowing for simple operations without heavy use of inessential constants. …</summary><br> They bridge classical asymptotic notations and modern non-asymptotic tail bounds together. Examples of different kinds demonstrate their efficacy.
</details>
</td>
</tr>
</tbody>
</table>
</div>
<h2 id="data-science-arxiv-by-primary-tag">Data Science arXiv by Primary Tag</h2>
<p>The tables below show abstracts organized by category with hyperlinks back to the arXiv site.</p>
<div class="layout-chunk" data-layout="l-page">

<h3 id="computer-science">Computer Science</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="8">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Machine Learning (cs.LG)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13458v1" style="color: #d9230f">Face X-ray for More General Face Forgery Detection</a></b><br><em>Computer Vision and Pattern Recognition</em>. 7 authors. <a href="http://arxiv.org/pdf/1912.13458v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In this paper we propose a novel image representation called face X-ray for detecting forgery in face images. The face X-ray of an input face image is a greyscale image that reveals whether the input image can be decomposed into the blending of two images from different sources. …</summary><br> It does so by showing the blending boundary for a forged image and the absence of blending for a real image. We observe that most existing face manipulation methods share a common step: blending the altered face into an existing background image. For this reason, face X-ray provides an effective way for detecting forgery generated by most existing face manipulation algorithms. Face X-ray is general in the sense that it only assumes the existence of a blending step and does not rely on any knowledge of the artifacts associated with a specific face manipulation technique. Indeed, the algorithm for computing face X-ray can be trained without fake images generated by any of the state-of-the-art face manipulation methods. Extensive experiments show that face X-ray remains effective when applied to forgery generated by unseen face manipulation techniques, while most existing face forgery detection algorithms experience a significant performance drop.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13230v1" style="color: #d9230f">Leveraging Semi-Supervised Learning for Fairness using Neural Networks</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13230v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>There has been a growing concern about the fairness of decision-making systems based on machine learning. The shortage of labeled data has been always a challenging problem facing machine learning based systems. …</summary><br> In such scenarios, semi-supervised learning has shown to be an effective way of exploiting unlabeled data to improve upon the performance of model. Notably, unlabeled data do not contain label information which itself can be a significant source of bias in training machine learning systems. This inspired us to tackle the challenge of fairness by formulating the problem in a semi-supervised framework. In this paper, we propose a semi-supervised algorithm using neural networks benefiting from unlabeled data to not just improve the performance but also improve the fairness of the decision-making process. The proposed model, called SSFair, exploits the information in the unlabeled data to mitigate the bias in the training data.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13157v1" style="color: #d9230f">C. H. Robinson Uses Heuristics to Solve Rich Vehicle Routing Problems</a></b><br><em>Artificial Intelligence, Optimization and Control</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13157v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider a wide family of vehicle routing problem variants with many complex and practical constraints, known as rich vehicle routing problems, which are faced on a daily basis by C.H. …</summary><br> Robinson (CHR). Since CHR has many customers, each with distinct requirements, various routing problems with different objectives and constraints should be solved. We propose a set partitioning framework with a number of route generation algorithms, which have shown to be effective in solving a variety of different problems. The proposed algorithms have outperformed the existing technologies at CHR on 10 benchmark instances and since, have been embedded into the company’s transportation planning and execution technology platform.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13503v1" style="color: #d9230f">Side-Tuning: Network Adaptation via Additive Side Networks</a></b><br><em>Neural and Evolutionary Computing, Machine Learning, Computer Vision and Pattern Recognition, Robotics</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13503v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than start with a randomly initialized one – due to lacking enough training data, performing lifelong learning where the system has to learn a new task while being previously trained for other tasks, or wishing to encode priors in the network via preset weights. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others. …</summary><br> In this paper, we propose a straightforward alternative: Side-Tuning. Side-tuning adapts a pre-trained network by training a lightweight “side” network that is fused with the (unchanged) pre-trained network using summation. This simple method works as well as or better than existing solutions while it resolves some of the basic issues with fine-tuning, fixed features, and several other common baselines. In particular, side-tuning is less prone to overfitting when little training data is available, yields better results than using a fixed feature extractor, and does not suffer from catastrophic forgetting in lifelong learning. We demonstrate the performance of side-tuning under a diverse set of scenarios, including lifelong learning (iCIFAR, Taskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), with consistently promising results.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13457v1" style="color: #d9230f">FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping</a></b><br><em>Computer Vision and Pattern Recognition</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13457v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In this work, we propose a novel two-stage framework, called FaceShifter, for high fidelity and occlusion aware face swapping. Unlike many existing face swapping works that leverage only limited information from the target image when synthesizing the swapped face, our framework, in its first stage, generates the swapped face in high-fidelity by exploiting and integrating the target attributes thoroughly and adaptively. …</summary><br> We propose a novel attributes encoder for extracting multi-level target face attributes, and a new generator with carefully designed Adaptive Attentional Denormalization (AAD) layers to adaptively integrate the identity and the attributes for face synthesis. To address the challenging facial occlusions, we append a second stage consisting of a novel Heuristic Error Acknowledging Refinement Network (HEAR-Net). It is trained to recover anomaly regions in a self-supervised way without any manual annotations. Extensive experiments on wild faces demonstrate that our face swapping results are not only considerably more perceptually appealing, but also better identity preserving in comparison to other state-of-the-art methods.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13360v1" style="color: #d9230f">Morphology-Agnostic Visual Robotic Control</a></b><br><em>Computer Vision and Pattern Recognition, Robotics</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13360v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Existing approaches for visuomotor robotic control typically require characterizing the robot in advance by calibrating the camera or performing system identification. We propose MAVRIC, an approach that works with minimal prior knowledge of the robot’s morphology, and requires only a camera view containing the robot and its environment and an unknown control interface. …</summary><br> MAVRIC revolves around a mutual information-based method for self-recognition, which discovers visual “control points” on the robot body within a few seconds of exploratory interaction, and these control points in turn are then used for visual servoing. MAVRIC can control robots with imprecise actuation, no proprioceptive feedback, unknown morphologies including novel tools, unknown camera poses, and even unsteady handheld cameras. We demonstrate our method on visually-guided 3D point reaching, trajectory following, and robot-to-robot imitation.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13344v1" style="color: #d9230f">Learning 3D Human Shape and Pose from Dense Body Parts</a></b><br><em>Computer Vision and Pattern Recognition</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13344v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Reconstructing 3D human shape and pose from a monocular image is challenging despite the promising results achieved by the most recent learning-based methods. The commonly occurred misalignment comes from the facts that the mapping from images to the model space is highly non-linear and the rotation-based pose representation of the body model is prone to result in the drift of joint positions. …</summary><br> In this work, we investigate learning 3D human shape and pose from dense correspondences of body parts and propose a Decompose-and-aggregate Network (DaNet) to address these issues. DaNet adopts the dense correspondence maps, which densely build a bridge between 2D pixels and 3D vertexes, as intermediate representations to facilitate the learning of 2D-to-3D mapping. The prediction modules of DaNet are decomposed into one global stream and multiple local streams to enable global and fine-grained perceptions for the shape and pose predictions, respectively. Messages from local streams are further aggregated to enhance the robust prediction of the rotation-based poses, where a position-aided rotation feature refinement strategy is proposed to exploit spatial relationships between body joints. Moreover, a Part-based Dropout (PartDrop) strategy is introduced to drop out dense information from intermediate representations during training, encouraging the network to focus on more complementary body parts as well as adjacent position features. The effectiveness of our method is validated on both in-door and real-world datasets including the Human3.6M, UP3D, and DensePose-COCO datasets. Experimental results show that the proposed method significantly improves the reconstruction performance in comparison with previous state-of-the-art methods. Our code will be made publicly available at <a href="https://hongwenzhang.github.io/dense2mesh" class="uri">https://hongwenzhang.github.io/dense2mesh</a> .
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13408v1" style="color: #d9230f">On the Role of Weight Sharing During Deep Option Learning</a></b><br><em>Machine Learning, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13408v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>The options framework is a popular approach for building temporally extended actions in reinforcement learning. In particular, the option-critic architecture provides general purpose policy gradient theorems for learning actions from scratch that are extended in time. …</summary><br> However, past work makes the key assumption that each of the components of option-critic has independent parameters. In this work we note that while this key assumption of the policy gradient theorems of option-critic holds in the tabular case, it is always violated in practice for the deep function approximation setting. We thus reconsider this assumption and consider more general extensions of option-critic and hierarchical option-critic training that optimize for the full architecture with each update. It turns out that not assuming parameter independence challenges a belief in prior work that training the policy over options can be disentangled from the dynamics of the underlying options. In fact, learning can be sped up by focusing the policy over options on states where options are actually likely to terminate. We put our new algorithms to the test in application to sample efficient learning of Atari games, and demonstrate significantly improved stability and faster convergence when learning long options.
</details>
</td>
</tr>
<tr grouplength="6">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Artificial Intelligence (cs.AI)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13283v1" style="color: #d9230f">oLMpics – On what Language Model Pre-training Captures</a></b><br><em>Computation and Language, Machine Learning, Artificial Intelligence</em>. 4 authors. <a href="http://arxiv.org/pdf/1912.13283v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. …</summary><br> In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13470v1" style="color: #d9230f">GraspNet: A Large-Scale Clustered and Densely Annotated Datase for Object Grasping</a></b><br><em>Computer Vision and Pattern Recognition, Robotics</em>. 4 authors. <a href="http://arxiv.org/pdf/1912.13470v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Object grasping is critical for many applications, which is also a challenging computer vision problem. However, for the clustered scene, current researches suffer from the problems of insufficient training data and the lacking of evaluation benchmarks. …</summary><br> In this work, we contribute a large-scale grasp pose detection dataset with a unified evaluation system. Our dataset contains 87,040 RGBD images with over 370 million grasp poses. Meanwhile, our evaluation system directly reports whether a grasping is successful or not by analytic computation, which is able to evaluate any kind of grasp poses without exhausted labeling pose ground-truth. We conduct extensive experiments to show that our dataset and evaluation system can align well with real-world experiments. Our dataset, source code and models will be made publicly available.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13262v1" style="color: #d9230f">Fungal architecture</a></b><br><em>Emerging Technologies</em>. 4 authors. <a href="http://arxiv.org/pdf/1912.13262v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>As one of the primary consumers of environmental resource, the building industry faces unprecedented challenges in needing to reduce the environmental impact of current consumption practices. This applies to both the construction of the built environment and resource consumption during its occupation and use. …</summary><br> Where incremental improvements to current practices can be realised, the net benefits are often far outstripped by the burgeoning demands of rapidly increasing population growth and urbanisation. Against the backdrop of this grand societal challenge, it is necessary to explore approaches that envision a paradigm shift in how material is sourced, processed and assembled to address the magnitude of these challenges in a truly sustainable way, and which can even provide added value. We propose to develop a structural substrate by using live fungal mycelium, functionalise the substrate with nanoparticles and polymers to make a mycelium-based electronics, implement sensorial fusion and decision making in the fungal electronics and to growing monolithic buildings from the functionalised fungal substrate. Fungal buildings will self-grow, build, and repair themselves subject to substrate supplied, use natural adaptation to the environment, sense all what human can sense.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13349v1" style="color: #d9230f">Domain-topic models with chained dimensions: modeling the evolution of a major oncology conference (1995-2017)</a></b><br><em>Data Analysis, Statistics and Probability, Physics and Society, Digital Libraries</em>. 4 authors. <a href="http://arxiv.org/pdf/1912.13349v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In this paper we introduce a novel approach for the computational analysis of research activities and their dynamics. Named SASHIMI (Symmetrical And Sequential analysis from Hierarchical Inference of Multidimensional Information), our approach provides a multi-level description of the structure of scientific activities that offers numerous advantages over traditional methods such as topic models or network analyses. …</summary><br> Our method generates a dual description of corpora in terms of research domains (collections of documents) and topics (collections of words). It also extends this description to clusters of associated dimensions, such as time. SASHIMI only requires access to the textual content of individual documents, rather than specific metadata such as citations, authors, or keywords as is the case with other science-mapping approaches. We illustrate the analytical power of our method by applying it to the empirical analysis of an original dataset, namely the 1995-2017 collection of abstracts presented at ASCO, the largest annual oncology research conference. We show that SASHIMI is able to detect the presence of significant temporal patterns and to identify the major thematic transformations of oncology that underlie these patterns.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13472v1" style="color: #d9230f">Revisiting Landscape Analysis in Deep Neural Networks: Eliminating Decreasing Paths to Infinity</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13472v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Traditional landscape analysis of deep neural networks aims to show that no sub-optimal local minima exist in some appropriate sense. From this, one may be tempted to conclude that descent algorithms which escape saddle points will reach a good local minimum. …</summary><br> However, basic optimization theory tell us that it is also possible for a descent algorithm to diverge to infinity if there are paths leading to infinity, along which the loss function decreases. It is not clear whether for non-linear neural networks there exists one setting that no bad local-min and no decreasing paths to infinity can be simultaneously achieved. In this paper, we give the first positive answer to this question. More specifically, for a large class of over-parameterized deep neural networks with appropriate regularizers, the loss function has no bad local minima and no decreasing paths to infinity. The key mathematical trick is to show that the set of regularizers which may be undesirable can be viewed as the image of a Lipschitz continuous mapping from a lower-dimensional Euclidean space to a higher-dimensional Euclidean space, and thus has zero measure.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13465v1" style="color: #d9230f">Reward-Conditioned Policies</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13465v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Reinforcement learning offers the promise of automating the acquisition of complex behavioral skills. However, compared to commonly used and well-understood supervised learning methods, reinforcement learning algorithms can be brittle, difficult to use and tune, and sensitive to seemingly innocuous implementation decisions. …</summary><br> In contrast, imitation learning utilizes standard and well-understood supervised learning methods, but requires near-optimal expert data. Can we learn effective policies via supervised learning without demonstrations? The main idea that we explore in this work is that non-expert trajectories collected from sub-optimal policies can be viewed as optimal supervision, not for maximizing the reward, but for matching the reward of the given trajectory. By then conditioning the policy on the numerical value of the reward, we can obtain a policy that generalizes to larger returns. We show how such an approach can be derived as a principled method for policy search, discuss several variants, and compare the method experimentally to a variety of current reinforcement learning methods on standard benchmarks.
</details>
</td>
</tr>
<tr grouplength="6">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Computer Vision and Pattern Recognition (cs.CV)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13308v1" style="color: #d9230f">Building Confidence in Scientific Computing Software Via Assurance Cases</a></b><br><em>Software Engineering</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13308v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Assurance cases provide an organized and explicit argument for correctness. They can dramatically improve the certification of Scientific Computing Software (SCS). …</summary><br> Assurance cases have already been effectively used for safety cases for real time systems. Their advantages for SCS include engaging domain experts, producing only necessary documentation, and providing evidence that can be verified/replicated. This paper illustrates assurance cases for SCS through the correctness case for 3dfim+, an existing Medical Imaging Application (MIA) for analyzing activity in the brain. This example was partly chosen because of recent concerns about the validity of fMRI (Functional Magnetic Resonance Imaging) studies. The example justifies the value of assurance cases for SCS, since the existing documentation is shown to have ambiguities and omissions, such as an incompletely defined ranking function and missing details on the coordinate system. A serious concern for 3dfim+ is identified: running the software does not produce any warning about the necessity of using data that matches the parametric statistical model employed for the correlation calculations. Raising the bar for SCS in general, and MIA in particular, is both feasible and necessary - when software impacts safety, an assurance case methodology (or an equivalently rigorous confidence building methodology) should be employed.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13490v1" style="color: #d9230f">Representation Internal-Manipulation (RIM): A Neuro-Inspired Computational Theory of Consciousness</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning, Neurons and Cognition, Neural and Evolutionary Computing</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13490v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Many theories, based on neuroscientific and psychological empirical evidence and on computational concepts, have been elaborated to explain the emergence of consciousness in the central nervous system. These theories propose key fundamental mechanisms to explain consciousness, but they only partially connect such mechanisms to the possible functional and adaptive role of consciousness. …</summary><br> Recently, some cognitive and neuroscientific models try to solve this gap by linking consciousness to various aspects of goal-directed behaviour, the pivotal cognitive process that allows mammals to flexibly act in challenging environments. Here we propose the Representation Internal-Manipulation (RIM) theory of consciousness, a theory that links the main elements of consciousness theories to components and functions of goal-directed behaviour, ascribing a central role for consciousness to the goal-directed manipulation of internal representations. This manipulation relies on four specific computational operations to perform the flexible internal adaptation of all key elements of goal-directed computation, from the representations of objects to those of goals, actions, and plans. Finally, we propose the concept of `manipulation agency’ relating the sense of agency to the internal manipulation of representations. This allows us to propose that the subjective experience of consciousness is associated to the human capacity to generate and control a simulated internal reality that is vividly perceived and felt through the same perceptual and emotional mechanisms used to tackle the external world.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13430v1" style="color: #d9230f">Towards Neural-Guided Program Synthesis for Linear Temporal Logic Specifications</a></b><br><em>Computer Science and Game Theory, Formal Languages and Automata Theory, Artificial Intelligence, Logic in Computer Science, Neural and Evolutionary Computing</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13430v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Synthesizing a program that realizes a logical specification is a classical problem in computer science. We examine a particular type of program synthesis, where the objective is to synthesize a strategy that reacts to a potentially adversarial environment while ensuring that all executions satisfy a Linear Temporal Logic (LTL) specification. …</summary><br> Unfortunately, exact methods to solve so-called LTL synthesis via logical inference do not scale. In this work, we cast LTL synthesis as an optimization problem. We employ a neural network to learn a Q-function that is then used to guide search, and to construct programs that are subsequently verified for correctness. Our method is unique in combining search with deep learning to realize LTL synthesis. In our experiments the learned Q-function provides effective guidance for synthesis problems with relatively small specifications.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13471v1" style="color: #d9230f">OneGAN: Simultaneous Unsupervised Learning of Conditional Image Generation, Foreground Segmentation, and Fine-Grained Clustering</a></b><br><em>Computer Vision and Pattern Recognition</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13471v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We present a method for simultaneously learning, in an unsupervised manner, (i) a conditional image generator, (ii) foreground extraction and segmentation, (iii) clustering into a two-level class hierarchy, and (iv) object removal and background completion, all done without any use of annotation. The method combines a generative adversarial network and a variational autoencoder, with multiple encoders, generators and discriminators, and benefits from solving all tasks at once. …</summary><br> The input to the training scheme is a varied collection of unlabeled images from the same domain, as well as a set of background images without a foreground object. In addition, the image generator can mix the background from one image, with a foreground that is conditioned either on that of a second image or on the index of a desired cluster. The method obtains state of the art results in comparison to the literature methods, when compared to the current state of the art in each of the tasks.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13480v1" style="color: #d9230f">On the Difference Between the Information Bottleneck and the Deep Information Bottleneck</a></b><br><em>Information Theory, Machine Learning, Information Theory, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13480v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Combining the Information Bottleneck model with deep learning by replacing mutual information terms with deep neural nets has proved successful in areas ranging from generative modelling to interpreting deep neural networks. In this paper, we revisit the Deep Variational Information Bottleneck and the assumptions needed for its derivation. …</summary><br> The two assumed properties of the data <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and their latent representation <span class="math inline">\(T\)</span> take the form of two Markov chains <span class="math inline">\(T-X-Y\)</span> and <span class="math inline">\(X-T-Y\)</span>. Requiring both to hold during the optimisation process can be limiting for the set of potential joint distributions <span class="math inline">\(P(X,Y,T)\)</span>. We therefore show how to circumvent this limitation by optimising a lower bound for <span class="math inline">\(I(T;Y)\)</span> for which only the latter Markov chain has to be satisfied. The actual mutual information consists of the lower bound which is optimised in DVIB and cognate models in practice and of two terms measuring how much the former requirement <span class="math inline">\(T-X-Y\)</span> is violated. Finally, we propose to interpret the family of information bottleneck models as directed graphical models and show that in this framework the original and deep information bottlenecks are special cases of a fundamental IB model.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13464v1" style="color: #d9230f">Model Inversion Networks for Model-Based Optimization</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13464v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In this work, we aim to solve data-driven optimization problems, where the goal is to find an input that maximizes an unknown score function given access to a dataset of inputs with corresponding scores. When the inputs are high-dimensional and valid inputs constitute a small subset of this space (e. …</summary><br>g., valid protein sequences or valid natural images), such model-based optimization problems become exceptionally difficult, since the optimizer must avoid out-of-distribution and invalid inputs. We propose to address such problem with model inversion networks (MINs), which learn an inverse mapping from scores to inputs. MINs can scale to high-dimensional input spaces and leverage offline logged data for both contextual and non-contextual optimization problems. MINs can also handle both purely offline data sources and active data collection. We evaluate MINs on tasks from the Bayesian optimization literature, high-dimensional model-based optimization problems over images and protein designs, and contextual bandit optimization from logged data.
</details>
</td>
</tr>
<tr grouplength="2">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Software Engineering (cs.SE)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13357v1" style="color: #d9230f">A Dynamic Sampling Adaptive-SGD Method for Machine Learning</a></b><br><em>Machine Learning, Machine Learning, Optimization and Control</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13357v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We propose a stochastic optimization method for minimizing loss functions, which can be expressed as an expected value, that adaptively controls the batch size used in the computation of gradient approximations and the step size used to move along such directions, eliminating the need for the user to tune the learning rate. The proposed method exploits local curvature information and ensures that search directions are descent directions with high probability using an acute-angle test. …</summary><br> The method is proved to have, under reasonable assumptions, a global linear rate of convergence on self-concordant functions with high probability. Numerical experiments show that this method is able to choose the best learning rates and compares favorably to fine-tuned SGD for training logistic regression and Deep Neural Networks (DNNs). We also propose an adaptive version of ADAM that eliminates the need to tune the base learning rate and compares favorably to fine-tuned ADAM for training DNNs.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13455v1" style="color: #d9230f">Essential Sentences for Navigating Stack Overflow Answers</a></b><br><em>Software Engineering, Computation and Language, Information Retrieval</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13455v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Stack Overflow (SO) has become an essential resource for software development. Despite its success and prevalence, navigating SO remains a challenge. …</summary><br> Ideally, SO users could benefit from highlighted navigational cues that help them decide if an answer is relevant to their task and context. Such navigational cues could be in the form of essential sentences that help the searcher decide whether they want to read the answer or skip over it. In this paper, we compare four potential approaches for identifying essential sentences. We adopt two existing approaches and develop two new approaches based on the idea that contextual information in a sentence (e.g., “if using windows”) could help identify essential sentences. We compare the four techniques using a survey of 43 participants. Our participants indicate that it is not always easy to figure out what the best solution for their specific problem is, given the options, and that they would indeed like to easily spot contextual information that may narrow down the search. Our quantitative comparison of the techniques shows that there is no single technique sufficient for identifying essential sentences that can serve as navigational cues, while our qualitative analysis shows that participants valued explanations and specific conditions, and did not value filler sentences or speculations. Our work sheds light on the importance of navigational cues, and our findings can be used to guide future research to find the best combination of techniques to identify such cues.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Computation and Language (cs.CL)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13263v1" style="color: #d9230f">Intrinsic motivations and open-ended learning</a></b><br><em>Machine Learning, Artificial Intelligence, Machine Learning</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13263v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>There is a growing interest and literature on intrinsic motivations and open-ended learning in both cognitive robotics and machine learning on one side, and in psychology and neuroscience on the other. This paper aims to review some relevant contributions from the two literature threads and to draw links between them. …</summary><br> To this purpose, the paper starts by defining intrinsic motivations and by presenting a computationally-driven theoretical taxonomy of their different types. Then it presents relevant contributions from the psychological and neuroscientific literature related to intrinsic motivations, interpreting them based on the grid, and elucidates the mechanisms and functions they play in animals and humans. Endowed with such concepts and their biological underpinnings, the paper next presents a selection of models from cognitive robotics and machine learning that computationally operationalise the concepts of intrinsic motivations and links them to biology concepts. The contribution finally presents some of the open challenges of the field from both the psychological/neuroscientific and computational perspectives.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Digital Libraries (cs.DL)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13186v1" style="color: #d9230f">Definitions and Semantic Simulations Based on Object-Oriented Analysis and Modeling</a></b><br><em>Artificial Intelligence</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13186v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We have proposed going beyond traditional ontologies to use rich semantics implemented in programming languages for modeling. In this paper, we discuss the application of executable semantic models to two examples, first a structured definition of a waterfall and second the cardiopulmonary system. …</summary><br> We examine the components of these models and the way those components interact. Ultimately, such models should provide the basis for direct representation.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Emerging Technologies (cs.ET)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13122v1" style="color: #d9230f">Towards Regulated Deep Learning</a></b><br><em>Machine Learning, Artificial Intelligence, Multiagent Systems, Logic in Computer Science, Programming Languages</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13122v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Regulation of Multi-Agent Systems (MAS) was a research topic of the past decade and one of these proposals was Electronic Institutions. However, with the recent reformulation of Artificial Neural Networks (ANN) as Deep Learning (DL), Security, Privacy, Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. …</summary><br> Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of ANN as Agent-based Training of a special type of regulated ANN that we call Institutional Neural Network. This paper introduces the former concept and provides <span class="math inline">\(\mathcal{I}\)</span>, a language previously used to model and extend Electronic Institutions, as a means to implement and regulate DL.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Robotics (cs.RO)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13452v1" style="color: #d9230f">BIRL: Benchmark on Image Registration methods with Landmark validation</a></b><br><em>Image and Video Processing, Computer Vision and Pattern Recognition, Performance</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13452v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>This report presents a generic image registration benchmark with automatic evaluation using landmark annotations. The BIRL framework has a few key features, such as: easily extendable, performance evaluation, parallel experimenting, simple visualisations, experiment’s time-out limit, pause/resume experiments. …</summary><br> The main use-cases are (a) compare your (newly developed) method with some State-of-the-Art (SOTA) methods on a common dataset and (b) experiment SOTA methods on your custom dataset (which should contain landmark annotation). In this paper, we present mixed-methods aiming at bio-medical imaging and experimental result on CIMA dataset. However, any other methods for other domain can be added or costume dataset to be used. <a href="https://borda.github.io/BIRL" class="uri">https://borda.github.io/BIRL</a>
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="statistics">Statistics</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="4">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Machine Learning (stat.ML)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13242v1" style="color: #d9230f">Statistical Models in Forensic Voice Comparison</a></b><br><em>Audio and Speech Processing, Sound, Applications</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13242v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>This chapter describes a number of signal-processing and statistical-modeling techniques that are commonly used to calculate likelihood ratios in human-supervised automatic approaches to forensic voice comparison. Techniques described include mel-frequency cepstral coefficients (MFCCs) feature extraction, Gaussian mixture model - universal background model (GMM-UBM) systems, i-vector - probabilistic linear discriminant analysis (i-vector PLDA) systems, deep neural network (DNN) based systems (including senone posterior i-vectors, bottleneck features, and embeddings / x-vectors), mismatch compensation, and score-to-likelihood-ratio conversion (aka calibration). …</summary><br> Empirical validation of forensic-voice-comparison systems is also covered. The aim of the chapter is to bridge the gap between general introductions to forensic voice comparison and the highly technical automatic-speaker-recognition literature from which the signal-processing and statistical-modeling techniques are mostly drawn. Knowledge of the likelihood-ratio framework for the evaluation of forensic evidence is assumed. It is hoped that the material presented here will be of value to students of forensic voice comparison and to researchers interested in learning about statistical modeling techniques that could potentially also be applied to data from other branches of forensic science.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13170v1" style="color: #d9230f">Schrödinger Bridge Samplers</a></b><br><em>Computation, Machine Learning</em>. 4 authors. <a href="http://arxiv.org/pdf/1912.13170v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Consider a reference Markov process with initial distribution <span class="math inline">\(\pi_{0}\)</span> and transition kernels <span class="math inline">\(\{M_{t}\}_{t\in[1:T]}\)</span>, for some <span class="math inline">\(T\in\mathbb{N}\)</span>. Assume that you are given distribution <span class="math inline">\(\pi_{T}\)</span>, which is not equal to the marginal distribution of the reference process at time <span class="math inline">\(T\)</span>. …</summary><br> In this scenario, Schr&quot;odinger addressed the problem of identifying the Markov process with initial distribution <span class="math inline">\(\pi_{0}\)</span> and terminal distribution equal to <span class="math inline">\(\pi_{T}\)</span> which is the closest to the reference process in terms of Kullback–Leibler divergence. This special case of the so-called Schr&quot;odinger bridge problem can be solved using iterative proportional fitting, also known as the Sinkhorn algorithm. We leverage these ideas to develop novel Monte Carlo schemes, termed Schr&quot;odinger bridge samplers, to approximate a target distribution <span class="math inline">\(\pi\)</span> on <span class="math inline">\(\mathbb{R}^{d}\)</span> and to estimate its normalizing constant. This is achieved by iteratively modifying the transition kernels of the reference Markov chain to obtain a process whose marginal distribution at time <span class="math inline">\(T\)</span> becomes closer to <span class="math inline">\(\pi_T = \pi\)</span>, via regression-based approximations of the corresponding iterative proportional fitting recursion. We report preliminary experiments and make connections with other problems arising in the optimal transport, optimal control and physics literatures.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13515v1" style="color: #d9230f">Stochastic Recursive Variance Reduction for Efficient Smooth Non-Convex Compositional Optimization</a></b><br><em>Machine Learning, Machine Learning, Optimization and Control</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13515v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Stochastic compositional optimization arises in many important machine learning tasks such as value function evaluation in reinforcement learning and portfolio management. The objective function is the composition of two expectations of stochastic functions, and is more challenging to optimize than vanilla stochastic optimization problems. …</summary><br> In this paper, we investigate the stochastic compositional optimization in the general smooth non-convex setting. We employ a recently developed idea of  to design a novel algorithm named SARAH-Compositional, and prove a sharp Incremental First-order Oracle (IFO) complexity upper bound for stochastic compositional optimization: <span class="math inline">\(\mathcal{O}((n+m)^{1/2} \varepsilon^{-2})\)</span> in the finite-sum case and <span class="math inline">\(\mathcal{O}(\varepsilon^{-3})\)</span> in the online case. Such a complexity is known to be the best one among IFO complexity results for non-convex stochastic compositional optimization, and is believed to be optimal. Our experiments validate the theoretical performance of our algorithm.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13445v1" style="color: #d9230f">Robust Aggregation for Federated Learning</a></b><br><em>Cryptography and Security, Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13445v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We present a robust aggregation approach to make federated learning robust to settings when a fraction of the devices may be sending corrupted updates to the server. The proposed approach relies on a robust secure aggregation oracle based on the geometric median, which returns a robust aggregate using a constant number of calls to a regular non-robust secure average oracle. …</summary><br> The robust aggregation oracle is privacy-preserving, similar to the secure average oracle it builds upon. We provide experimental results of the proposed approach with linear models and deep networks for two tasks in computer vision and natural language processing. The robust aggregation approach is agnostic to the level of corruption; it outperforms the classical aggregation approach in terms of robustness when the level of corruption is high, while being competitive in the regime of low corruption.
</details>
</td>
</tr>
<tr grouplength="3">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Methodology (stat.ME)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13188v1" style="color: #d9230f">On Testing for Biases in Peer Review</a></b><br><em>Statistics Theory, Methodology, Statistics Theory, Applications</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13188v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider the issue of biases in scholarly research, specifically, in peer review. There is a long standing debate on whether exposing author identities to reviewers induces biases against certain groups, and our focus is on designing tests to detect the presence of such biases. …</summary><br> Our starting point is a remarkable recent work by Tomkins, Zhang and Heavlin which conducted a controlled, large-scale experiment to investigate existence of biases in the peer reviewing of the WSDM conference. We present two sets of results in this paper. The first set of results is negative, and pertains to the statistical tests and the experimental setup used in the work of Tomkins et al. We show that the test employed therein does not guarantee control over false alarm probability and under correlations between relevant variables coupled with any of the following conditions, with high probability, can declare a presence of bias when it is in fact absent: (a) measurement error, (b) model mismatch, (c) reviewer calibration. Moreover, we show that the setup of their experiment may itself inflate false alarm probability if (d) bidding is performed in non-blind manner or (e) popular reviewer assignment procedure is employed. Our second set of results is positive and is built around a novel approach to testing for biases that we propose. We present a general framework for testing for biases in (single vs. double blind) peer review. We then design hypothesis tests that under minimal assumptions guarantee control over false alarm probability and non-trivial power even under conditions (a)–(c) as well as propose an alternative experimental setup which mitigates issues (d) and (e). Finally, we show that no statistical test can improve over the non-parametric tests we consider in terms of the assumptions required to control for the false alarm probability.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13351v1" style="color: #d9230f">Driver fatigue EEG signals detection by using robust univariate analysis</a></b><br><em>Signal Processing, Methodology, Applications</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13351v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Driver fatigue is a major cause of traffic accidents and the electroencephalogram (EEG) is considered one of the most reliable predictors of fatigue. This paper proposes a novel, simple and fast method for driver fatigue detection that can be implemented in real-time systems by using a single-channel on the scalp. …</summary><br> The method based on the robust univariate analysis of EEG signals is composed of two stages. First, the most significant channel from EEG raw is selected according to the maximum variance. In the second stage, this single channel will be used to detect the fatigue EEG signal by extracting four feature parameters. Two parameters estimated from the robust univariate analysis, namely mean and covariance, and two classical statistics parameters such as variance and covariance that help to tune the robust analysis. Next, an ensemble bagged decision trees classifier is used in order to discriminate fatigue signals from alert signals. The proposed algorithm is demonstrated on 24 EEG signals from the Jiangxi University of Technology database using only the most significant channel found, which is located in the left tempo-parietal region where spatial awareness and visual-spatial navigation are shared, in terms of 92.7% accuracy with 1.8 seconds of time delay.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13119v1" style="color: #d9230f">Prediction in the Presence of Missing Covariates</a></b><br><em>Methodology</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13119v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In many applied fields incomplete covariate vectors are commonly encountered. It is well known that this can be problematic when making inference on model parameters, but its impact on prediction performance is less understood. …</summary><br> We develop a method based on covariate dependent partition models that seamlessly handles missing covariates while avoiding completely any type of imputation. The method we develop is not only able to make in-sample predictions, but out-of-sample as well even if the missing pattern in the new subjects’ incomplete covariate vector was not seen in the training data. Further, both categorical and continuous covariates are permitted. Our proposal fares well when compared to other alternatives based on imputations. We illustrate the method using simulation studies and an ozone dataset.
</details>
</td>
</tr>
<tr grouplength="2">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Applications (stat.AP)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13440v1" style="color: #d9230f">Approximate Inference for Fully Bayesian Gaussian Process Regression</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13440v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Learning in Gaussian Process models occurs through the adaptation of hyperparameters of the mean and the covariance function. The classical approach entails maximizing the marginal likelihood yielding fixed point estimates (an approach called  or ML-II). …</summary><br> An alternative learning procedure is to infer the posterior over hyperparameters in a hierarchical specification of GPs we call  (GPR). This work considers two approximation schemes for the intractable hyperparameter posterior: 1) Hamiltonian Monte Carlo (HMC) yielding a sampling-based approximation and 2) Variational Inference (VI) where the posterior over hyperparameters is approximated by a factorized Gaussian (mean-field) or a full-rank Gaussian accounting for correlations between hyperparameters. We analyze the predictive performance for fully Bayesian GPR on a range of benchmark data sets.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13421v1" style="color: #d9230f">Asymptotic Risk of Least Squares Minimum Norm Estimator under the Spike Covariance Model</a></b><br><em>Machine Learning, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13421v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>One of the recent approaches to explain good performance of neural networks has focused on their ability to fit training data perfectly (interpolate) without overfitting. It has been shown that this is not unique to neural nets, and that it happens with simpler models such as kernel regression, too Belkin et al. …</summary><br> (2018b); Tengyuan Liang (2018). Consequently, there has been quite a few works that give conditions for low risk or optimality of interpolating models, see for example Belkin et al. (2018a, 2019b). One of the simpler models where interpolation has been studied recently is least squares solution for linear regression. In this case, interpolation is only guaranteed to happen in high dimensional setting where the number of parameters exceeds number of samples; therefore, least squares solution is not necessarily unique. However, minimum norm solution is unique, can be written in closed form, and gradient descent starting at the origin converges to it (Hastie et al., 2019). This has, at least partially, motivated several works that study risk of minimum norm least squares estimator for linear regression. Continuing in a similar vein, we study the asymptotic risk of minimum norm least squares estimator when number of parameters <span class="math inline">\(d\)</span> depends on <span class="math inline">\(n\)</span>, and <span class="math inline">\(\frac{d}{n} \rightarrow \infty\)</span>. In this high dimensional setting, to make inference feasible, it is usually assumed that true parameters or data have some underlying low dimensional structure such as sparsity, or vanishing eigenvalues of population covariance matrix. Here, we restrict ourselves to spike covariance matrices, where a fixed finite number of eigenvalues grow with <span class="math inline">\(n\)</span> and are much larger than the rest of the eigenvalues, which are (asymptotically) in the same order. We show that in this setting the risk can vanish.
</details>
</td>
</tr>
<tr grouplength="2">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Computation (stat.CO)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13132v1" style="color: #d9230f">Parallel cross-validation: a scalable fitting method for Gaussian process models</a></b><br><em>Computation</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13132v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Gaussian process (GP) models are widely used to analyze spatially referenced data and to predict values at locations without observations. In contrast to many algorithmic procedures, GP models are based on a statistical framework, which enables uncertainty quantification of the model structure and predictions. …</summary><br> Both the evaluation of the likelihood and the prediction involve solving linear systems. Hence, the computational costs are large and limit the amount of data that can be handled. While there are many approximation strategies that lower the computational cost of GP models, they often provide only sub-optimal support for the parallel computing capabilities of current (high-performance) computing environments. We aim at bridging this gap with a parameter estimation and prediction method that is designed to be parallelizable. More precisely, we divide the spatial domain into overlapping subsets and use cross-validation (CV) to estimate the covariance parameters in parallel. We present simulation studies, which assess the accuracy of the parameter estimates and predictions. Moreover, we show that our implementation has good weak and strong parallel scaling properties. For illustration, we fit an exponential covariance model to a scientifically relevant canopy height dataset with 5 million observations. Using 512 processor cores in parallel brings the evaluation time of one covariance parameter configuration to less than 1.5 minutes. The parallel CV method can be easily extended to include approximate likelihood methods, multivariate and spatio-temporal data, as well as non-stationary covariance models.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13144v1" style="color: #d9230f">A Dynamic Process Reference Model for Sparse Networks with Reciprocity</a></b><br><em>Social and Information Networks, Methodology</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13144v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Many social and other networks exhibit stable size scaling relationships, such that features such as mean degree or reciprocation rates change slowly or are approximately constant as the number of vertices increases. Statistical network models built on top of simple Bernoulli baseline (or reference) measures often behave unrealistically in this respect, leading to the development of sparse reference models that preserve features such as mean degree scaling. …</summary><br> In this paper, we generalize recent work on the micro-foundations of such reference models to the case of sparse directed graphs with non-vanishing reciprocity, providing a dynamic process interpretation of the emergence of stable macroscopic behavior.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="mathematics">Mathematics</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="4">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Statistics Theory (math.ST)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13292v1" style="color: #d9230f">True and false discoveries with e-values</a></b><br><em>Statistics Theory, Statistics Theory</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13292v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We discuss controlling the number of false discoveries using e-values instead of p-values. Using e-values simplifies the known algorithms radically. …</summary><br>
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13289v1" style="color: #d9230f">Bayesian Generalization Error of Poisson Mixture and Simplex Vandermonde Matrix Type Singularity</a></b><br><em>Statistics Theory, Statistics Theory</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13289v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>A Poisson mixture is one of the practically important models in computer science, biology, and sociology. However, the theoretical property has not been studied because the posterior distribution can not be approximated by any normal distribution. …</summary><br> Such a model is called singular and it is known that Real Log Canonical Threshold (RLCT) is equal to the coefficient of the asymptotically main term of the Bayesian generalization error. In this paper, we derive RLCT of a simplex Vandermonde matrix type singularity which is equal to that of a Poisson mixture in general cases.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13185v1" style="color: #d9230f">Model-free Bootstrap for a General Class of Stationary Time Series</a></b><br><em>Statistics Theory, Statistics Theory</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13185v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>A model-free bootstrap procedure for a general class of stationary time series is introduced. The theoretical framework is established, showing asymptotic validity of bootstrap confidence intervals for many statistics of interest. …</summary><br> In addition, asymptotic validity of one-step ahead bootstrap prediction intervals is also demonstrated. Finite-sample experiments are conducted to empirically confirm the performance of the new method, and to compare with popular methods such as the block bootstrap and the autoregressive (AR)-sieve bootstrap.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13494v1" style="color: #d9230f">A frequency-domain analysis of inexact gradient descent</a></b><br><em>Numerical Analysis, Optimization and Control, Machine Learning, Numerical Analysis, Systems and Control</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13494v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We study robustness properties of inexact gradient descent for strongly convex functions, as well as for the larger class of functions with sector-bounded gradients, under a relative error model. Proofs of the corresponding convergence rates are based on frequency-domain criteria for the stability of nonlinear systems perturbed by additive noise. …</summary><br>
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Numerical Analysis (math.NA)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13463v1" style="color: #d9230f">Some compact notations for concentration inequalities and user-friendly results</a></b><br><em>Information Theory, Statistics Theory, Machine Learning, Information Theory, Machine Learning, Statistics Theory, Probability</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13463v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>This paper presents compact notations for concentration inequalities and convenient results to streamline probabilistic analysis. The new expressions describe the typical sizes and tails of random variables, allowing for simple operations without heavy use of inessential constants. …</summary><br> They bridge classical asymptotic notations and modern non-asymptotic tail bounds together. Examples of different kinds demonstrate their efficacy.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Optimization and Control (math.OC)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13336v1" style="color: #d9230f">Numerical Linear Algebra in Data Assimilation</a></b><br><em>Numerical Analysis, Numerical Analysis, Computation, Applications</em>. 1 authors. <a href="http://arxiv.org/pdf/1912.13336v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Data assimilation is a method that combines observations (e.g. …</summary><br> real world data) of a state of a system with model output for that system in order to improve the estimate of the state of the system and thereby the model output. The model is usually represented by a discretised partial differential equation. The data assimilation problem can be formulated as a large scale Bayesian inverse problem. Based on this interpretation we will derive the most important variational and sequential data assimilation approaches, in particular three-dimensional and four-dimensional variational data assimilation (3D-Var and 4D-Var) and the Kalman filter. We will then consider more advanced methods which are extensions of the Kalman filter and variational data assimilation and will pay particular attention to their advantages and disadvantages. The data assimilation problem usually results in a very large optimisation problem and/or a very large linear system to solve (due to inclusion of time and space dimensions). Therefore, the second part of this article aims to review advances and challenges, in particular from the numerical linear algebra perspective, within the various data assimilation approaches.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="elec.-eng.-and-systems-science">Elec. Eng. and Systems Science</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="4">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Image and Video Processing (eess.IV)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13290v1" style="color: #d9230f">Automatic segmentation and determining radiodensity of the liver in a large-scale CT database</a></b><br><em>Image and Video Processing, Medical Physics, Computer Vision and Pattern Recognition</em>. 9 authors. <a href="http://arxiv.org/pdf/1912.13290v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>This study proposes an automatic technique for liver segmentation in computed tomography (CT) images. Localization of the liver volume is based on the correlation with an optimized set of liver templates developed by the authors that allows clear geometric interpretation. …</summary><br> Radiodensity values are calculated based on the boundaries of the segmented liver, which allows identifying liver abnormalities. The performance of the technique was evaluated on 700 CT images from dataset of the Unified Radiological Information System (URIS) of Moscow. Despite the decrease in accuracy, the technique is applicable to CT volumes with a partially visible region of the liver. The technique can be used to process CT images obtained in various patient positions in a wide range of exposition parameters. It is capable in dealing with low dose CT scans in real large-scale medical database with over 1 million of studies.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13335v1" style="color: #d9230f">Volumetric Lung Nodule Segmentation using Adaptive ROI with Multi-View Residual Learning</a></b><br><em>Image and Video Processing, Machine Learning, Computer Vision and Pattern Recognition, Machine Learning</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13335v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Accurate quantification of pulmonary nodules can greatly assist the early diagnosis of lung cancer, which can enhance patient survival possibilities. A number of nodule segmentation techniques have been proposed, however, all of the existing techniques rely on radiologist 3-D volume of interest (VOI) input or use the constant region of interest (ROI) and only investigate the presence of nodule voxels within the given VOI. …</summary><br> Such approaches restrain the solutions to investigate the nodule presence outside the given VOI and also include the redundant structures into VOI, which may lead to inaccurate nodule segmentation. In this work, a novel semi-automated approach for 3-D segmentation of nodule in volumetric computerized tomography (CT) lung scans has been proposed. The proposed technique can be segregated into two stages, at the first stage, it takes a 2-D ROI containing the nodule as input and it performs patch-wise investigation along the axial axis with a novel adaptive ROI strategy. The adaptive ROI algorithm enables the solution to dynamically select the ROI for the surrounding slices to investigate the presence of nodule using deep residual U-Net architecture. The first stage provides the initial estimation of nodule which is further utilized to extract the VOI. At the second stage, the extracted VOI is further investigated along the coronal and sagittal axis with two different networks and finally, all the estimated masks are fed into the consensus module to produce the final volumetric segmentation of nodule. The proposed approach has been rigorously evaluated on the LIDC dataset, which is the largest publicly available dataset. The result suggests that the approach is significantly robust and accurate as compared to the previous state of the art techniques.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13423v1" style="color: #d9230f">Learning Wavefront Coding for Extended Depth of Field Imaging</a></b><br><em>Image and Video Processing, Computer Vision and Pattern Recognition, Optics</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13423v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>The depth of field constitutes an important quality factor of imaging systems that highly affects the content of the acquired spatial information in the captured images. Extended depth of field (EDoF) imaging is a challenging problem due to its highly ill-posed nature, hence it has been extensively addressed in the literature. …</summary><br> We propose a computational imaging approach for EDoF, where we employ wavefront coding via a diffractive optical element (DOE) and we achieve deblurring through a convolutional neural network. Thanks to the end-to-end differentiable modeling of optical image formation and computational post-processing, we jointly optimize the optical design, i.e., DOE, and the deblurring through standard gradient descent methods. Based on the properties of the underlying refractive lens and the desired EDoF range, we provide an analytical expression for the search space of the DOE, which helps in the convergence of the end-to-end network. We achieve superior EDoF imaging performance compared to state of the art, where we demonstrate results with minimal artifacts in various scenarios, including deep 3D scenes and broadband imaging.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13298v1" style="color: #d9230f">Microlens array grid estimation, light field decoding, and calibration</a></b><br><em>Image and Video Processing, Computer Vision and Pattern Recognition</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13298v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We quantitatively investigate multiple algorithms for microlens array grid estimation for microlens array-based light field cameras. Explicitly taking into account natural and mechanical vignetting effects, we propose a new method for microlens array grid estimation that outperforms the ones previously discussed in the literature. …</summary><br> To quantify the performance of the algorithms, we propose an evaluation pipeline utilizing application-specific ray-traced white images with known microlens positions. Using a large dataset of synthesized white images, we thoroughly compare the performance of the different estimation algorithms. As an example, we apply our results to the decoding and calibration of light fields taken with a Lytro Illum camera. We observe that decoding as well as calibration benefit from a more accurate, vignetting-aware grid estimation, especially in peripheral subapertures of the light field.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="physics">Physics</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Computational Physics (physics.comp-ph)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13403v1" style="color: #d9230f">ELSI – An Open Infrastructure for Electronic Structure Solvers</a></b><br><em>Computational Physics, Materials Science</em>. 19 authors. <a href="http://arxiv.org/pdf/1912.13403v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Routine applications of electronic structure theory to molecules and periodic systems need to compute the electron density from given Hamiltonian and, in case of non-orthogonal basis sets, overlap matrices. System sizes can range from few to thousands or, in some examples, millions of atoms. …</summary><br> Different discretization schemes (basis sets) and different system geometries (finite non-periodic vs. infinite periodic boundary conditions) yield matrices with different structures. The ELectronic Structure Infrastructure (ELSI) project provides an open-source software interface to facilitate the implementation and optimal use of high-performance solver libraries covering cubic scaling eigensolvers, linear scaling density-matrix-based algorithms, and other reduced scaling methods in between. In this paper, we present recent improvements and developments inside ELSI, mainly covering (1) new solvers connected to the interface, (2) matrix layout and communication adapted for parallel calculations of periodic and/or spin-polarized systems, (3) routines for density matrix extrapolation in geometry optimization and molecular dynamics calculations, and (4) general utilities such as parallel matrix I/O and JSON output. The ELSI interface has been integrated into four electronic structure code projects (DFTB+, DGDFT, FHI-aims, SIESTA), allowing us to rigorously benchmark the performance of the solvers on an equal footing. Based on results of a systematic set of large-scale benchmarks performed with Kohn-Sham density-functional theory and density-functional tight-binding theory, we identify factors that strongly affect the efficiency of the solvers, and propose a decision layer that assists with the solver selection process. Finally, we describe a reverse communication interface encoding matrix-free iterative solver strategies that are amenable, e.g., for use with planewave basis sets.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Physics and Society (physics.soc-ph)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13467v1" style="color: #d9230f">The brevity law as a scaling law, and a possible origin of Zipf’s law for word frequencies</a></b><br><em>Physics and Society, Adaptation and Self-Organizing Systems, Data Analysis, Statistics and Probability</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13467v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>An important body of quantitative linguistics is constituted by a series of statistical laws about language usage. Despite the importance of these linguistic laws, some of them are poorly formulated, and, more importantly, there is no unified framework that encompasses all them. …</summary><br> This paper presents a new perspective to establish a connection between different statistical linguistic laws. Characterizing each word type by two random variables, length (in number of characters) and absolute frequency, we show that the corresponding bivariate joint probability distribution shows a rich and precise phenomenology, with the type-length and the type-frequency distributions as its two marginals, and the conditional distribution of frequency at fixed length providing a clear formulation for the brevity-frequency phenomenon. The type-length distribution turns out to be well fitted by a gamma distribution (much better than with the previously proposed lognormal), and the conditional frequency distributions at fixed length display power-law-decay behavior with a fixed exponent <span class="math inline">\(\alpha\simeq 1.4\)</span> and a characteristic-frequency crossover that scales as an inverse power <span class="math inline">\(\delta\simeq 2.8\)</span> of length, which implies the fulfilment of a scaling law analogous to those found in the thermodynamics of critical phenomena. As a by-product, we find a possible model-free explanation for the origin of Zipf’s law, which should arise as a mixture of conditional frequency distributions governed by the crossover length-dependent frequency.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="quantitative-biology">Quantitative Biology</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Biomolecules (q-bio.BM)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13190v1" style="color: #d9230f">LinearPartition: Linear-Time Approximation of RNA Folding Partition Function and Base Pairing Probabilities</a></b><br><em>Data Structures and Algorithms, Quantitative Methods, Biological Physics, Biomolecules</em>. 4 authors. <a href="http://arxiv.org/pdf/1912.13190v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>RNA secondary structure prediction is widely used to understand RNA function. Recently, there has been a shift away from the classical minimum free energy (MFE) methods to partition function-based methods that account for folding ensembles and can therefore estimate structure and base pair probabilities. …</summary><br> However, the classic partition function algorithm scales cubically with sequence length, and is therefore a slow calculation for long sequences. This slowness is even more severe than cubic-time MFE-based methods due to a larger constant factor in runtime. Inspired by the success of LinearFold algorithm that computes the MFE structure in linear time, we address this issue by proposing a similar linear-time heuristic algorithm, LinearPartition, to approximate the partition function and base pairing probabilities. LinearPartition is 256x faster than Vienna RNAfold for a sequence with length 15,780, and 2,771x faster than CONTRAfold for a sequence with length 32,753. Interestingly, although LinearPartition is approximate, it runs in linear time without sacrificing accuracy when base pair probabilities are used to assemble structures, and even leads to a small accuracy improvement on longer families (16S and 23S rRNA).
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Populations and Evolution (q-bio.PE)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13433v1" style="color: #d9230f">Optimal evolutionary control for artificial selection on molecular phenotypes</a></b><br><em>Biological Physics, Quantitative Methods, Populations and Evolution</em>. 2 authors. <a href="http://arxiv.org/pdf/1912.13433v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Controlling an evolving population is an important task in modern molecular genetics, including in directed evolution to improve the activity of molecules and enzymes, breeding experiments in animals and in plants, and in devising public health strategies to suppress evolving pathogens. An optimal intervention to direct evolution should be designed by considering its impact over an entire stochastic evolutionary trajectory that follows. …</summary><br> As a result, a seemingly suboptimal intervention at a given time can be globally optimal as it can open opportunities for desirable actions in the future. Here, we propose a feedback control formalism to devise globally optimal artificial selection protocol to direct the evolution of molecular phenotypes. We show that artificial selection should be designed to counter evolutionary tradeoffs among multi-variate phenotypes to avoid undesirable outcomes in one phenotype by imposing selection on another. Control by artificial selection is challenged by our ability to predict molecular evolution. We develop an information theoretical framework and show that molecular time-scales for evolution under natural selection can inform how to monitor a population to acquire sufficient predictive information for an effective intervention with artificial selection. Our formalism opens a new avenue for devising optimal artificial selection for directed evolution of molecular functions.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="other">Other</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Earth and Planetary Astrophysics (astro-ph.EP)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13130v1" style="color: #d9230f">Production of nitric oxide by a fragmenting bolide: An exploratory numerical study</a></b><br><em>Earth and Planetary Astrophysics, Computational Physics</em>. 3 authors. <a href="http://arxiv.org/pdf/1912.13130v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>A meteoroid’s hypersonic passage through the Earth’s atmosphere results in ablational and fragmentational mass loss. Potential shock waves associated with a parent object as well as its fragments can modify the surrounding atmosphere and produce a range of physico-chemical effects. …</summary><br> Some of the thermally driven chemical and physical processes induced by meteoroid-fragment generated shock waves, such as nitric oxide (NO) production, are less understood. Any estimates of meteoric NO production depend not only on a quantifiable meteoroid population and a rate of fragmentation, with a size capable of producing high temperature flows, but also on understanding the physical properties of the meteor flows along with their thermal history. We performed an exploratory pilot numerical study using ANSYS Fluent, the CFD code, to investigate the production of NO in the upper atmosphere by small meteoroids (or fragments of meteoroids after they undergo a disruption episode) in the size range from 10-2 m to 1 m. Our model uses the simulation of a spherical body in the continuum flow at 70 and 80 km altitude to approximate the behaviour of a small meteoroid capable of producing NO. The results presented in this exploratory study are in good agreement with previous studies.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="quantum-physics">Quantum Physics</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Quantum Physics (quant-ph)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/1912.13251v1" style="color: #d9230f">Quantum annealing approach to Ionic Diffusion in Solid</a></b><br><em>Computational Physics, Quantum Physics</em>. 5 authors. <a href="http://arxiv.org/pdf/1912.13251v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We developed a framework to evaluate a key quantity to describe the ionic diffusion in solids, the correlation factor, for which we can apply the quantum annealing computation to get rid of the current limitation for the evaluation being possible only for the simple models far from the practical diffusion paths. Though the current <span class="math inline">\({\it ab initio}\)</span> technique can provide the quantitative information about the diffusion paths network, the difficulty to evaluate the coefficient has hampered to connect such microscopic <span class="math inline">\({\it ab initio}\)</span> works with macroscopic quantities like diffusion coefficients. …</summary><br> By applying the framework, we can evaluate how the diffusion coefficients are controlled by temperatures, pressures, atomic substitutions <span class="math inline">\({\it etc.}\)</span> when coupled with <span class="math inline">\({\it ab initio}\)</span> technique. We formulated the problem in terms of the mapping into a quantum spin systems described by the Ising Hamiltonian. Calibrating verifications to get a value of the coefficient already known for a simple model is performed on the comparisons amongst various possible methods including simulated quantum annealing on the spin models, the classical random walk, and the matrix description for the problem. We have confirmed that all the evaluations give consistent results with each other though some of the conventional approaches require infeasible computational costs supporting the advantage of the quantum annealing application.
</details>
</td>
</tr>
</tbody>
</table>
</div>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2020-01-01/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Articles%20from%202019-12-31&amp;url=bryanwhiting.github.io%2Fds-arxiv%2Fposts%2F2020-01-01%2F">
        <i class="fab fa-twitter"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=bryanwhiting.github.io%2Fds-arxiv%2Fposts%2F2020-01-01%2F&amp;title=Articles%20from%202019-12-31">
        <i class="fab fa-linkedin"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://dsarxiv.disqus.com/count.js" async></script>
  <div id="disqus_thread"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'bryanwhiting.github.io/ds-arxiv/posts/2020-01-01/';
  this.page.identifier = 'posts/2020-01-01/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://dsarxiv.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
