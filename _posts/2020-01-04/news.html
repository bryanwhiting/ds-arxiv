<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Articles from 2020-01-02</title>
  
  <meta property="description" itemprop="description" content="38 new data science research articles were published on 2020-01-02. 17 discussed machine learning."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2020-01-04"/>
  <meta property="article:created" itemprop="dateCreated" content="2020-01-04"/>
  <meta name="article:author" content="Bryan Whiting"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Articles from 2020-01-02"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="38 new data science research articles were published on 2020-01-02. 17 discussed machine learning."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Articles from 2020-01-02"/>
  <meta property="twitter:description" content="38 new data science research articles were published on 2020-01-02. 17 discussed machine learning."/>
  
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","date","author","output"]}},"value":[{"type":"character","attributes":{},"value":["Articles from 2020-01-02"]},{"type":"character","attributes":{},"value":["38 new data science research articles were published on 2020-01-02. 17 discussed machine learning.\n"]},{"type":"character","attributes":{},"value":["2020-01-04"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Bryan Whiting"]},{"type":"character","attributes":{},"value":["https://www.bryanwhiting.com"]}]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]}]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["arxiv.csv","news_files/bowser-1.9.3/bowser.min.js","news_files/distill-2.2.21/template.v2.js","news_files/jquery-1.11.3/jquery.min.js","news_files/kePrint-0.0.1/kePrint.js","news_files/webcomponents-2.0.0/webcomponents.js","news.Rmd.bak","output_df_summary.Rda","tweet.txt"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="news_files/kePrint-0.0.1/kePrint.js"></script>
  <script src="news_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="news_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="news_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="news_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Articles from 2020-01-02","description":"38 new data science research articles were published on 2020-01-02. 17 discussed machine learning.","authors":[{"author":"Bryan Whiting","authorURL":"https://www.bryanwhiting.com","affiliation":"&nbsp;","affiliationURL":"#"}],"publishedDate":"2020-01-04T00:00:00.000-05:00","citationText":"Whiting, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Articles from 2020-01-02</h1>
<p>38 new data science research articles were published on 2020-01-02. 17 discussed machine learning.</p>
</div>

<div class="d-byline">
  Bryan Whiting <a href="https://www.bryanwhiting.com" class="uri">https://www.bryanwhiting.com</a> 
  
<br/>2020-01-04
</div>

<div class="d-article">
<h3 class="d-toc-header">Table of Contents</h3>
<nav class="d-toc" id="TOC">
<ul>
<li><a href="#breakdown-of-arxiv-publication-counts">Breakdown of arXiv Publication Counts</a></li>
<li><a href="#articles-for-statitstics-machine-learning-econonmetrics-and-finance">Articles for Statitstics, Machine Learning Econonmetrics, and Finance</a><ul>
<li><a href="#applications--stat-ap-">Applications (stat.AP): 3 new</a></li>
<li><a href="#machine-learning--stat-ml-">Machine Learning (stat.ML): 14 new</a></li>
<li><a href="#machine-learning--cs-lg-">Machine Learning (cs.LG): 16 new</a></li>
</ul></li>
<li><a href="#data-science-arxiv-by-primary-tag">Data Science arXiv by Primary Tag</a><ul>
<li><a href="#computer-science">Computer Science</a></li>
<li><a href="#statistics">Statistics</a></li>
<li><a href="#elec.-eng.-and-systems-science">Elec. Eng. and Systems Science</a></li>
<li><a href="#mathematics">Mathematics</a></li>
<li><a href="#physics">Physics</a></li>
<li><a href="#other">Other</a></li>
<li><a href="#quantitative-biology">Quantitative Biology</a></li>
<li><a href="#quantitative-finance">Quantitative Finance</a></li>
<li><a href="#quantum-physics">Quantum Physics</a></li>
</ul></li>
</ul>
</nav>
<hr class="d-toc-separator"/>
<h2 id="breakdown-of-arxiv-publication-counts">Breakdown of arXiv Publication Counts</h2>
<p>Yesterday’s counts of submitted papers on www.arxiv.org grouped by primary subject. Click the links in the table to be re-directed to the abstracts below. The links under <code>Subject</code> will redirect you to abstracts with the primary subject (there can only be one primary subject on arXiv). The links under <code>Category</code> will redirect you to all publications yesterday with a given tag (primary or secondary).</p>
<div class="layout-chunk" data-layout="l-body">
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:summary-table-with-counts">Table 1: </span>Number of articles by subject and primary category. Colored titles represent hyperlinks that take you below to abstracts. Key - Subject: Computer Science (5) means there were 5 articles with primary tag CS. Category: Machine Learning (cs.LG) N = 8 (16) means there were 8 primary articles with the (cs.LG) tag but 16 articles had it as a secondary tag, so there should be 24 in total. Click this link to be taken to all 24. Only select categories are highlighted because they are of particular interest to applied data scientists.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Subject
</th>
<th style="text-align:left;">
Category
</th>
<th style="text-align:left;">
N
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="5">
<a href="#computer-science" style=" font-weight: bold;    color: #d9230f !important;">Computer Science (18)</a>
</td>
<td style="text-align:left;">
<a href="#machine-learning--cs-lg-" style=" font-weight: bold;    color: #d9230f !important;">Machine Learning (cs.LG)</a>
</td>
<td style="text-align:left;">
10 (6)
</td>
</tr>
<tr>
<td style="text-align:left;">
Computer Vision and Pattern Recognition (cs.CV)
</td>
<td style="text-align:left;">
5 (6)
</td>
</tr>
<tr>
<td style="text-align:left;">
Artificial Intelligence (cs.AI)
</td>
<td style="text-align:left;">
1 (2)
</td>
</tr>
<tr>
<td style="text-align:left;">
Databases (cs.DB)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Software Engineering (cs.SE)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="3">
<a href="#statistics" style=" font-weight: bold;    color: #d9230f !important;">Statistics (6)</a>
</td>
<td style="text-align:left;">
<a href="#machine-learning--stat-ml-" style=" font-weight: bold;    color: #d9230f !important;">Machine Learning (stat.ML)</a>
</td>
<td style="text-align:left;">
2 (12)
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="#applications--stat-ap-" style=" font-weight: bold;    color: #d9230f !important;">Applications (stat.AP)</a>
</td>
<td style="text-align:left;">
2 (1)
</td>
</tr>
<tr>
<td style="text-align:left;">
Methodology (stat.ME)
</td>
<td style="text-align:left;">
2 (1)
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
<a href="#elec.-eng.%20and%20systems%20science" style=" font-weight: bold;    color: #d9230f !important;">Elec. Eng. and Systems Science (4)</a>
</td>
<td style="text-align:left;">
Image and Video Processing (eess.IV)
</td>
<td style="text-align:left;">
4
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="2">
<a href="#mathematics" style=" font-weight: bold;    color: #d9230f !important;">Mathematics (3)</a>
</td>
<td style="text-align:left;">
Statistics Theory (math.ST)
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
Numerical Analysis (math.NA)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;vertical-align: top !important;" rowspan="3">
<a href="#physics" style=" font-weight: bold;    color: #d9230f !important;">Physics (3)</a>
</td>
<td style="text-align:left;">
Computational Physics (physics.comp-ph)
</td>
<td style="text-align:left;">
1 (1)
</td>
</tr>
<tr>
<td style="text-align:left;">
Chemical Physics (physics.chem-ph)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Data Analysis, Statistics and Probability (physics.data-an)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
<a href="#other" style=" font-weight: bold;    color: #d9230f !important;">Other (1)</a>
</td>
<td style="text-align:left;">
Instrumentation and Methods for Astrophysics (astro-ph.IM)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
<a href="#quantitative-biology" style=" font-weight: bold;    color: #d9230f !important;">Quantitative Biology (1)</a>
</td>
<td style="text-align:left;">
Populations and Evolution (q-bio.PE)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
<a href="#quantitative-finance" style=" font-weight: bold;    color: #d9230f !important;">Quantitative Finance (1)</a>
</td>
<td style="text-align:left;">
Risk Management (q-fin.RM)
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
<a href="#quantum-physics" style=" font-weight: bold;    color: #d9230f !important;">Quantum Physics (1)</a>
</td>
<td style="text-align:left;">
Quantum Physics (quant-ph)
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
</table>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<h2 id="articles-for-statitstics-machine-learning-econonmetrics-and-finance">Articles for Statitstics, Machine Learning Econonmetrics, and Finance</h2>
<p>This section contains all articles with any tag of <code>stat.AP</code>, <code>stat.co</code>, <code>stat.ML</code>, <code>cs.LG</code>, <code>q-fin.ST</code>, <code>q-fin.EC</code>, or <code>econ-EM</code>. Only the first two sentences are shown - click the links for more detail.</p>
<div class="layout-chunk" data-layout="l-screen-inset">

<h3 id="applications--stat-ap-">Applications (stat.AP): 3 new</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="3">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Applications (stat.AP)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00405v1" style="color: #d9230f">CircSpaceTime: an R package for spatial and spatio-temporal modeling of Circular data</a></b><br><em>Applications</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00405v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>CircSpaceTime is the only R package currently available that implements Bayesian models for spatial and spatio-temporal interpolation of circular data. Such data are often found in applications where, among the many, wind directions, animal movement directions, and wave directions are involved. …</summary><br> To analyze such data we need models for observations at locations s and times t, as the so-called geostatistical models, providing structured dependence assumed to decay in distance and time. The approach we take begins with Gaussian processes defined for linear variables over space and time. Then, we use either wrapping or projection to obtain processes for circular data. The models are cast as hierarchical, with fitting and inference within a Bayesian framework. Altogether, this package implements work developed by a series of papers; the most relevant being Jona Lasinio, Gelfand, and Jona Lasinio (2012); Wang and Gelfand (2014); Mastrantonio, Jona Lasinio, and Gelfand (2016). All procedures are written using Rcpp. Estimates are obtained by MCMC allowing parallelized multiple chains run. The implementation of the proposed models is considerably improved on the simple routines adopted in the research papers. As original running examples, for the spatial and spatio-temporal settings, we use wind directions datasets over central Italy.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00299v1" style="color: #d9230f">Concentration of Benefit index: A threshold-free summary metric for quantifying the capacity of covariates to yield efficient treatment rules</a></b><br><em>Applications, Methodology</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00299v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>When data on treatment assignment, outcomes, and covariates from a randomized trial are available, a question of interest is to what extent covariates can be used to optimize treatment decisions. Statistical hypothesis testing of covariate-by-treatment interaction is ill-suited for this purpose. …</summary><br> The application of decision theory results in treatment rules that compare the expected benefit of treatment given the patient’s covariates against a treatment threshold. However, determining treatment threshold is often context-specific, and any given threshold might seem arbitrary when the overall capacity towards predicting treatment benefit is of concern. We propose the Concentration of Benefit index (Cb), a threshold-free metric that quantifies the combined performance of covariates towards finding individuals who will benefit the most from treatment. The construct of the proposed index is comparing expected treatment outcomes with and without knowledge of covariates when one of a two randomly selected patients are to be treated. We show that the resulting index can also be expressed in terms of the integrated efficiency of individualized treatment decision over the entire range of treatment thresholds. We propose parametric and semi-parametric estimators, the latter being suitable for out-of-sample validation and correction for optimism. We used data from a clinical trial to demonstrate the calculations in a step-by-step fashion, and have provided the R code for implementation (<a href="https://github.com/msadatsafavi/txBenefit" class="uri">https://github.com/msadatsafavi/txBenefit</a>). The proposed index has intuitive and theoretically sound interpretation and can be estimated with relative ease for a wide class of regression models. Beyond the conceptual developments, various aspects of estimation and inference for such a metric need to be pursued in future research.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00529v1" style="color: #d9230f">The Impact of the Choice of Risk and Dispersion Measure on Procyclicality</a></b><br><em>Risk Management, Applications</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00529v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Procyclicality of historical risk measure estimation means that one tends to over-estimate future risk when present realized volatility is high and vice versa under-estimate future risk when the realized volatility is low. Out of it different questions arise, relevant for applications and theory: What are the factors which affect the degree of procyclicality? More specifically, how does the choice of risk measure affect this? How does this behaviour vary with the choice of realized volatility estimator? How do different underlying model assumptions influence the pro-cyclical effect? In this paper we consider three different well-known risk measures (Value-at-Risk, Expected Shortfall, Expectile), the r-th absolute centred sample moment, for any integer <span class="math inline">\(r&amp;gt;0\)</span>, as realized volatility estimator (this includes the sample variance and the sample mean absolute deviation around the sample mean) and two models (either an iid model or an augmented GARCH(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) model). …</summary><br> We show that the strength of procyclicality depends on these three factors, the choice of risk measure, the realized volatility estimator and the model considered. But, no matter the choices, the procyclicality will always be present.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="machine-learning--stat-ml-">Machine Learning (stat.ML): 14 new</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="14">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Machine Learning (stat.ML)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00449v1" style="color: #d9230f">Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics</a></b><br><em>Machine Learning, Machine Learning, Robotics</em>. 10 authors. <a href="http://arxiv.org/pdf/2001.00449v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Many real-world control problems involve both discrete decision variables - such as the choice of control modes, gear switching or digital outputs - as well as continuous decision variables - such as velocity setpoints, control gains or analogue outputs. However, when defining the corresponding optimal control or reinforcement learning problem, it is commonly approximated with fully continuous or fully discrete action spaces. …</summary><br> These simplifications aim at tailoring the problem to a particular algorithm or solver which may only support one type of action space. Alternatively, expert heuristics are used to remove discrete actions from an otherwise continuous space. In contrast, we propose to treat hybrid problems in their ‘native’ form by solving them with hybrid reinforcement learning, which optimizes for discrete and continuous actions simultaneously. In our experiments, we first demonstrate that the proposed approach efficiently solves such natively hybrid reinforcement learning problems. We then show, both in simulation and on robotic hardware, the benefits of removing possibly imperfect expert-designed heuristics. Lastly, hybrid reinforcement learning encourages us to rethink problem definitions. We propose reformulating control problems, e.g. by adding meta actions, to improve exploration or reduce mechanical wear and tear.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00563v1" style="color: #d9230f">Using Data Imputation for Signal Separation in High Contrast Imaging</a></b><br><em>Machine Learning, Earth and Planetary Astrophysics, Instrumentation and Methods for Astrophysics, Solar and Stellar Astrophysics</em>. 8 authors. <a href="http://arxiv.org/pdf/2001.00563v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>To characterize circumstellar systems in high contrast imaging, the fundamental step is to construct a best point spread function (PSF) template for the non-circumstellar signals (i.e. …</summary><br>, star light and speckles) and separate it from the observation. With existing PSF construction methods, the circumstellar signals (e.g., planets, circumstellar disks) are unavoidably altered by over-fitting and/or self-subtraction, making forward modeling a necessity to recover these signals. We present a forward modeling–free solution to these problems with data imputation using sequential non-negative matrix factorization (DI-sNMF). DI-sNMF first converts this signal separation problem to a “missing data” problem in statistics by flagging the regions which host circumstellar signals as missing data, then attributes PSF signals to these regions. We mathematically prove it to have negligible alteration to circumstellar signals when the imputation region is relatively small, which thus enables precise measurement for these circumstellar objects. We apply it to simulated point source and circumstellar disk observations to demonstrate its proper recovery of them. We apply it to Gemini Planet Imager (GPI) K1-band observations of the debris disk surrounding HR 4796A, finding a tentative trend that the dust is more forward scattering as the wavelength increases. We expect DI-sNMF to be applicable to other general scenarios where the separation of signals is needed.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00461v1" style="color: #d9230f">Reasoning on Knowledge Graphs with Debate Dynamics</a></b><br><em>Machine Learning, Machine Learning</em>. 6 authors. <a href="http://arxiv.org/pdf/2001.00461v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We propose a novel method for automatic reasoning on knowledge graphs based on debate dynamics. The main idea is to frame the task of triple classification as a debate game between two reinforcement learning agents which extract arguments – paths in the knowledge graph – with the goal to promote the fact being true (thesis) or the fact being false (antithesis), respectively. …</summary><br> Based on these arguments, a binary classifier, called the judge, decides whether the fact is true or false. The two agents can be considered as sparse, adversarial feature generators that present interpretable evidence for either the thesis or the antithesis. In contrast to other black-box methods, the arguments allow users to get an understanding of the decision of the judge. Since the focus of this work is to create an explainable method that maintains a competitive predictive accuracy, we benchmark our method on the triple classification and link prediction task. Thereby, we find that our method outperforms several baselines on the benchmark datasets FB15k-237, WN18RR, and Hetionet. We also conduct a survey and find that the extracted arguments are informative for users.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00503v1" style="color: #d9230f">Joint Goal and Strategy Inference across Heterogeneous Demonstrators via Reward Network Distillation</a></b><br><em>Machine Learning, Machine Learning, Robotics, Artificial Intelligence</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00503v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Reinforcement learning (RL) has achieved tremendous success as a general framework for learning how to make decisions. However, this success relies on the interactive hand-tuning of a reward function by RL experts. …</summary><br> On the other hand, inverse reinforcement learning (IRL) seeks to learn a reward function from readily-obtained human demonstrations. Yet, IRL suffers from two major limitations: 1) reward ambiguity - there are an infinite number of possible reward functions that could explain an expert’s demonstration and 2) heterogeneity - human experts adopt varying strategies and preferences, which makes learning from multiple demonstrators difficult due to the common assumption that demonstrators seeks to maximize the same reward. In this work, we propose a method to jointly infer a task goal and humans’ strategic preferences via network distillation. This approach enables us to distill a robust task reward (addressing reward ambiguity) and to model each strategy’s objective (handling heterogeneity). We demonstrate our algorithm can better recover task reward and strategy rewards and imitate the strategies in two simulated tasks and a real-world table tennis task.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00396v1" style="color: #d9230f">Restricting the Flow: Information Bottlenecks for Attribution</a></b><br><em>Machine Learning, Machine Learning, Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00396v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. …</summary><br> In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method’s information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network’s decision.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00360v1" style="color: #d9230f">Kernelized Support Tensor Train Machines</a></b><br><em>Machine Learning, Machine Learning, Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00360v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. …</summary><br> In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for image classification. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. Extensive experiments are performed on real-world tensor data, which demonstrates the superiority of the proposed scheme under few-sample high-dimensional inputs.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00564v1" style="color: #d9230f">Robust Marine Buoy Placement for Ship Detection Using Dropout K-Means</a></b><br><em>Machine Learning, Machine Learning</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00564v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Marine buoys aid in the battle against Illegal, Unreported and Unregulated (IUU) fishing by detecting fishing vessels in their vicinity. Marine buoys, however, may be disrupted by natural causes and buoy vandalism. …</summary><br> To minimize the effects of buoy disruption on a buoy network, we propose a more robust buoy placement using dropout k-means and dropout k-median. We apply dropout k-means and dropout k-median to determine locations for deploying marine buoys in the Gabonese waters near West Africa. We simulated the passage of ships using historical Automatic Identification System (AIS) data, then compared the ship detection probability of dropout k-means to classic k-means and dropout k-median to classic k-median, taking into account that the current sensor detection radius is 10km. With 5 buoys, the buoy arrangement computed by classic k-means, dropout k-means, classic k-median and dropout k-median have ship detection probabilities of 38%, 45%, 48% and 52%.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00528v1" style="color: #d9230f">Non-Parametric Learning of Gaifman Models</a></b><br><em>Machine Learning, Machine Learning</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00528v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider the problem of structure learning for Gaifman models and learn relational features that can be used to derive feature representations from a knowledge base. These relational features are first-order rules that are then partially grounded and counted over local neighborhoods of a Gaifman model to obtain the feature representations. …</summary><br> We propose a method for learning these relational features for a Gaifman model by using relational tree distances. Our empirical evaluation on real data sets demonstrates the superiority of our approach over classical rule-learning.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00559v1" style="color: #d9230f">A Deep Structural Model for Analyzing Correlated Multivariate Time Series</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00559v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Multivariate time series are routinely encountered in real-world applications, and in many cases, these time series are strongly correlated. In this paper, we present a deep learning structural time series model which can (i) handle correlated multivariate time series input, and (ii) forecast the targeted temporal sequence by explicitly learning/extracting the trend, seasonality, and event components. …</summary><br> The trend is learned via a 1D and 2D temporal CNN and LSTM hierarchical neural net. The CNN-LSTM architecture can (i) seamlessly leverage the dependency among multiple correlated time series in a natural way, (ii) extract the weighted differencing feature for better trend learning, and (iii) memorize the long-term sequential pattern. The seasonality component is approximated via a non-liner function of a set of Fourier terms, and the event components are learned by a simple linear function of regressor encoding the event dates. We compare our model with several state-of-the-art methods through a comprehensive set of experiments on a variety of time series data sets, such as forecasts of Amazon AWS Simple Storage Service (S3) and Elastic Compute Cloud (EC2) billings, and the closing prices for corporate stocks in the same category.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00543v1" style="color: #d9230f">Adversarial Policies in Learning Systems with Malicious Experts</a></b><br><em>Multiagent Systems, Machine Learning, Machine Learning, Cryptography and Security</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00543v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider a learning system based on the conventional multiplicative weight (MW) rule that combines experts’ advice to predict a sequence of true outcomes. It is assumed that one of the experts is malicious and aims to impose the maximum loss on the system. …</summary><br> The loss of the system is naturally defined to be the aggregate absolute difference between the sequence of predicted outcomes and the true outcomes. We consider this problem under both offline and online settings. In the offline setting where the malicious expert must choose its entire sequence of decisions a priori, we show somewhat surprisingly that a simple greedy policy of always reporting false prediction is asymptotically optimal with an approximation ratio of <span class="math inline">\(1+O(\sqrt{\frac{\ln N}{N}})\)</span>, where <span class="math inline">\(N\)</span> is the total number of prediction stages. In particular, we describe a policy that closely resembles the structure of the optimal offline policy. For the online setting where the malicious expert can adaptively make its decisions, we show that the optimal online policy can be efficiently computed by solving a dynamic program in <span class="math inline">\(O(N^2)\)</span>. Our results provide a new direction for vulnerability assessment of commonly used learning algorithms to adversarial attacks where the threat is an integral part of the system.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00448v1" style="color: #d9230f">Inter- and Intra-domain Knowledge Transfer for Related Tasks in Deep Character Recognition</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00448v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Pre-training a deep neural network on the ImageNet dataset is a common practice for training deep learning models, and generally yields improved performance and faster training times. The technique of pre-training on one task and then retraining on a new one is called transfer learning. …</summary><br> In this paper we analyse the effectiveness of using deep transfer learning for character recognition tasks. We perform three sets of experiments with varying levels of similarity between source and target tasks to investigate the behaviour of different types of knowledge transfer. We transfer both parameters and features and analyse their behaviour. Our results demonstrate that no significant advantage is gained by using a transfer learning approach over a traditional machine learning approach for our character recognition tasks. This suggests that using transfer learning does not necessarily presuppose a better performing model in all cases.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00329v1" style="color: #d9230f">On Consequentialism and Fairness</a></b><br><em>Computers and Society, Machine Learning, Artificial Intelligence, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00329v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage “fair” outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. …</summary><br> Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00479v1" style="color: #d9230f">Thresholds of descending algorithms in inference problems</a></b><br><em>Machine Learning, Machine Learning, Disordered Systems and Neural Networks</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00479v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We review recent works on analyzing the dynamics of gradient-based algorithms in a prototypical statistical inference problem. Using methods and insights from the physics of glassy systems, these works showed how to understand quantitatively and qualitatively the performance of gradient-based algorithms. …</summary><br> Here we review the key results and their interpretation in non-technical terms accessible to a wide audience of physicists in the context of related works.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00483v1" style="color: #d9230f">Reject Illegal Inputs with Generative Classifier Derived from Any Discriminative Classifier</a></b><br><em>Machine Learning, Machine Learning</em>. 1 authors. <a href="http://arxiv.org/pdf/2001.00483v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Generative classifiers have been shown promising to detect illegal inputs including adversarial examples and out-of-distribution samples. Supervised Deep Infomax~(SDIM) is a scalable end-to-end framework to learn generative classifiers. …</summary><br> In this paper, we propose a modification of SDIM termed SDIM-. Instead of training generative classifier from scratch, SDIM- first takes as input the logits produced any given discriminative classifier, and generate logit representations; then a generative classifier is derived by imposing statistical constraints on logit representations. SDIM- could inherit the performance of the discriminative classifier without loss. SDIM- incurs a negligible number of additional parameters, and can be efficiently trained with base classifiers fixed. We perform , where test samples whose class conditionals are smaller than pre-chosen thresholds will be rejected without predictions. Experiments on illegal inputs, including adversarial examples, samples with common corruptions, and out-of-distribution~(OOD) samples show that allowed to reject a portion of test samples, SDIM- significantly improves the performance on the left test sets.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="machine-learning--cs-lg-">Machine Learning (cs.LG): 16 new</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="16">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Machine Learning (cs.LG)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00449v1" style="color: #d9230f">Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics</a></b><br><em>Machine Learning, Machine Learning, Robotics</em>. 10 authors. <a href="http://arxiv.org/pdf/2001.00449v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Many real-world control problems involve both discrete decision variables - such as the choice of control modes, gear switching or digital outputs - as well as continuous decision variables - such as velocity setpoints, control gains or analogue outputs. However, when defining the corresponding optimal control or reinforcement learning problem, it is commonly approximated with fully continuous or fully discrete action spaces. …</summary><br> These simplifications aim at tailoring the problem to a particular algorithm or solver which may only support one type of action space. Alternatively, expert heuristics are used to remove discrete actions from an otherwise continuous space. In contrast, we propose to treat hybrid problems in their ‘native’ form by solving them with hybrid reinforcement learning, which optimizes for discrete and continuous actions simultaneously. In our experiments, we first demonstrate that the proposed approach efficiently solves such natively hybrid reinforcement learning problems. We then show, both in simulation and on robotic hardware, the benefits of removing possibly imperfect expert-designed heuristics. Lastly, hybrid reinforcement learning encourages us to rethink problem definitions. We propose reformulating control problems, e.g. by adding meta actions, to improve exploration or reduce mechanical wear and tear.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00461v1" style="color: #d9230f">Reasoning on Knowledge Graphs with Debate Dynamics</a></b><br><em>Machine Learning, Machine Learning</em>. 6 authors. <a href="http://arxiv.org/pdf/2001.00461v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We propose a novel method for automatic reasoning on knowledge graphs based on debate dynamics. The main idea is to frame the task of triple classification as a debate game between two reinforcement learning agents which extract arguments – paths in the knowledge graph – with the goal to promote the fact being true (thesis) or the fact being false (antithesis), respectively. …</summary><br> Based on these arguments, a binary classifier, called the judge, decides whether the fact is true or false. The two agents can be considered as sparse, adversarial feature generators that present interpretable evidence for either the thesis or the antithesis. In contrast to other black-box methods, the arguments allow users to get an understanding of the decision of the judge. Since the focus of this work is to create an explainable method that maintains a competitive predictive accuracy, we benchmark our method on the triple classification and link prediction task. Thereby, we find that our method outperforms several baselines on the benchmark datasets FB15k-237, WN18RR, and Hetionet. We also conduct a survey and find that the extracted arguments are informative for users.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00550v1" style="color: #d9230f">Cost-Function-Dependent Barren Plateaus in Shallow Quantum Neural Networks</a></b><br><em>Machine Learning, Quantum Physics</em>. 5 authors. <a href="http://arxiv.org/pdf/2001.00550v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Variational quantum algorithms (VQAs) optimize the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> of a quantum neural network <span class="math inline">\(V(\boldsymbol{\theta})\)</span> to minimize a cost function <span class="math inline">\(C\)</span>. While VQAs may enable practical applications of noisy quantum computers, they are nevertheless heuristic methods with unproven scaling. …</summary><br> Here, we rigorously prove two results. Our first result states that defining <span class="math inline">\(C\)</span> in terms of global observables leads to an exponentially vanishing gradient (i.e., a barren plateau) even when <span class="math inline">\(V(\boldsymbol{\theta})\)</span> is shallow. This implies that several VQAs in the literature must revise their proposed cost functions. On the other hand, our second result states that defining <span class="math inline">\(C\)</span> with local observables leads to a polynomially vanishing gradient, so long as the depth of <span class="math inline">\(V(\boldsymbol{\theta})\)</span> is <span class="math inline">\(\mathcal{O}(\log n)\)</span>. Taken together, our results establish a precise connection between locality and trainability. Finally, we illustrate these ideas with large-scale simulations, up to 100 qubits, of a particular VQA known as quantum autoencoders.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00503v1" style="color: #d9230f">Joint Goal and Strategy Inference across Heterogeneous Demonstrators via Reward Network Distillation</a></b><br><em>Machine Learning, Machine Learning, Robotics, Artificial Intelligence</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00503v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Reinforcement learning (RL) has achieved tremendous success as a general framework for learning how to make decisions. However, this success relies on the interactive hand-tuning of a reward function by RL experts. …</summary><br> On the other hand, inverse reinforcement learning (IRL) seeks to learn a reward function from readily-obtained human demonstrations. Yet, IRL suffers from two major limitations: 1) reward ambiguity - there are an infinite number of possible reward functions that could explain an expert’s demonstration and 2) heterogeneity - human experts adopt varying strategies and preferences, which makes learning from multiple demonstrators difficult due to the common assumption that demonstrators seeks to maximize the same reward. In this work, we propose a method to jointly infer a task goal and humans’ strategic preferences via network distillation. This approach enables us to distill a robust task reward (addressing reward ambiguity) and to model each strategy’s objective (handling heterogeneity). We demonstrate our algorithm can better recover task reward and strategy rewards and imitate the strategies in two simulated tasks and a real-world table tennis task.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00396v1" style="color: #d9230f">Restricting the Flow: Information Bottlenecks for Attribution</a></b><br><em>Machine Learning, Machine Learning, Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00396v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. …</summary><br> In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method’s information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network’s decision.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00360v1" style="color: #d9230f">Kernelized Support Tensor Train Machines</a></b><br><em>Machine Learning, Machine Learning, Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00360v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. …</summary><br> In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for image classification. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. Extensive experiments are performed on real-world tensor data, which demonstrates the superiority of the proposed scheme under few-sample high-dimensional inputs.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00564v1" style="color: #d9230f">Robust Marine Buoy Placement for Ship Detection Using Dropout K-Means</a></b><br><em>Machine Learning, Machine Learning</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00564v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Marine buoys aid in the battle against Illegal, Unreported and Unregulated (IUU) fishing by detecting fishing vessels in their vicinity. Marine buoys, however, may be disrupted by natural causes and buoy vandalism. …</summary><br> To minimize the effects of buoy disruption on a buoy network, we propose a more robust buoy placement using dropout k-means and dropout k-median. We apply dropout k-means and dropout k-median to determine locations for deploying marine buoys in the Gabonese waters near West Africa. We simulated the passage of ships using historical Automatic Identification System (AIS) data, then compared the ship detection probability of dropout k-means to classic k-means and dropout k-median to classic k-median, taking into account that the current sensor detection radius is 10km. With 5 buoys, the buoy arrangement computed by classic k-means, dropout k-means, classic k-median and dropout k-median have ship detection probabilities of 38%, 45%, 48% and 52%.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00528v1" style="color: #d9230f">Non-Parametric Learning of Gaifman Models</a></b><br><em>Machine Learning, Machine Learning</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00528v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider the problem of structure learning for Gaifman models and learn relational features that can be used to derive feature representations from a knowledge base. These relational features are first-order rules that are then partially grounded and counted over local neighborhoods of a Gaifman model to obtain the feature representations. …</summary><br> We propose a method for learning these relational features for a Gaifman model by using relational tree distances. Our empirical evaluation on real data sets demonstrates the superiority of our approach over classical rule-learning.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00561v1" style="color: #d9230f">PrivacyNet: Semi-Adversarial Networks for Multi-attribute Face Privacy</a></b><br><em>Machine Learning, Computer Vision and Pattern Recognition</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00561v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In recent years, the utilization of biometric information has become more and more common for various forms of identity verification and user authentication. However, as a consequence of the widespread use and storage of biometric information, concerns regarding sensitive information leakage and the protection of users’ privacy have been raised. …</summary><br> Recent research efforts targeted these concerns by proposing the Semi-Adversarial Networks (SAN) framework for imparting gender privacy to face images. The objective of SAN is to perturb face image data such that it cannot be reliably used by a gender classifier but can still be used by a face matcher for matching purposes. In this work, we propose a novel Generative Adversarial Networks-based SAN model, PrivacyNet, that is capable of imparting selective soft biometric privacy to multiple soft-biometric attributes such as gender, age, and race. While PrivacyNet is capable of perturbing different sources of soft biometric information reliably and simultaneously, it also allows users to choose to obfuscate specific attributes, while preserving others. The results from extensive experiments on five independent face image databases demonstrate the efficacy of our proposed model in imparting selective multi-attribute privacy to face images.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00559v1" style="color: #d9230f">A Deep Structural Model for Analyzing Correlated Multivariate Time Series</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00559v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Multivariate time series are routinely encountered in real-world applications, and in many cases, these time series are strongly correlated. In this paper, we present a deep learning structural time series model which can (i) handle correlated multivariate time series input, and (ii) forecast the targeted temporal sequence by explicitly learning/extracting the trend, seasonality, and event components. …</summary><br> The trend is learned via a 1D and 2D temporal CNN and LSTM hierarchical neural net. The CNN-LSTM architecture can (i) seamlessly leverage the dependency among multiple correlated time series in a natural way, (ii) extract the weighted differencing feature for better trend learning, and (iii) memorize the long-term sequential pattern. The seasonality component is approximated via a non-liner function of a set of Fourier terms, and the event components are learned by a simple linear function of regressor encoding the event dates. We compare our model with several state-of-the-art methods through a comprehensive set of experiments on a variety of time series data sets, such as forecasts of Amazon AWS Simple Storage Service (S3) and Elastic Compute Cloud (EC2) billings, and the closing prices for corporate stocks in the same category.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00543v1" style="color: #d9230f">Adversarial Policies in Learning Systems with Malicious Experts</a></b><br><em>Multiagent Systems, Machine Learning, Machine Learning, Cryptography and Security</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00543v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider a learning system based on the conventional multiplicative weight (MW) rule that combines experts’ advice to predict a sequence of true outcomes. It is assumed that one of the experts is malicious and aims to impose the maximum loss on the system. …</summary><br> The loss of the system is naturally defined to be the aggregate absolute difference between the sequence of predicted outcomes and the true outcomes. We consider this problem under both offline and online settings. In the offline setting where the malicious expert must choose its entire sequence of decisions a priori, we show somewhat surprisingly that a simple greedy policy of always reporting false prediction is asymptotically optimal with an approximation ratio of <span class="math inline">\(1+O(\sqrt{\frac{\ln N}{N}})\)</span>, where <span class="math inline">\(N\)</span> is the total number of prediction stages. In particular, we describe a policy that closely resembles the structure of the optimal offline policy. For the online setting where the malicious expert can adaptively make its decisions, we show that the optimal online policy can be efficiently computed by solving a dynamic program in <span class="math inline">\(O(N^2)\)</span>. Our results provide a new direction for vulnerability assessment of commonly used learning algorithms to adversarial attacks where the threat is an integral part of the system.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00448v1" style="color: #d9230f">Inter- and Intra-domain Knowledge Transfer for Related Tasks in Deep Character Recognition</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00448v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Pre-training a deep neural network on the ImageNet dataset is a common practice for training deep learning models, and generally yields improved performance and faster training times. The technique of pre-training on one task and then retraining on a new one is called transfer learning. …</summary><br> In this paper we analyse the effectiveness of using deep transfer learning for character recognition tasks. We perform three sets of experiments with varying levels of similarity between source and target tasks to investigate the behaviour of different types of knowledge transfer. We transfer both parameters and features and analyse their behaviour. Our results demonstrate that no significant advantage is gained by using a transfer learning approach over a traditional machine learning approach for our character recognition tasks. This suggests that using transfer learning does not necessarily presuppose a better performing model in all cases.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00329v1" style="color: #d9230f">On Consequentialism and Fairness</a></b><br><em>Computers and Society, Machine Learning, Artificial Intelligence, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00329v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage “fair” outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. …</summary><br> Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00526v1" style="color: #d9230f">Lightweight Residual Densely Connected Convolutional Neural Network</a></b><br><em>Machine Learning, Computer Vision and Pattern Recognition</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00526v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Extremely efficient convolutional neural network architectures are one of the most important requirements for limited computing power devices (such as embedded and mobile devices). Recently, some architectures have been proposed to overcome this limitation by considering specific hardware-software equipment. …</summary><br> In this paper, the residual densely connected blocks are proposed to guaranty the deep supervision, efficient gradient flow, and feature reuse abilities of convolutional neural network. The proposed method decreases the cost of training and inference processes without using any special hardware-software equipment by just reducing the number of parameters and computational operations while achieving a feasible accuracy. Extensive experimental results demonstrate that the proposed architecture is more efficient than the AlexNet and VGGNet in terms of model size, required parameters, and even accuracy. The proposed model is evaluated on the ImageNet, MNIST, Fashion MNIST, SVHN, CIFAR-10, and CIFAR-100. It achieves state-of-the-art results on the Fashion MNIST dataset and reasonable results on the others. The obtained results show that the proposed model is superior to efficient models such as the SqueezNet and is also comparable with the state-of-the-art efficient models such as CondenseNet and ShuffleNet.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00479v1" style="color: #d9230f">Thresholds of descending algorithms in inference problems</a></b><br><em>Machine Learning, Machine Learning, Disordered Systems and Neural Networks</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00479v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We review recent works on analyzing the dynamics of gradient-based algorithms in a prototypical statistical inference problem. Using methods and insights from the physics of glassy systems, these works showed how to understand quantitatively and qualitatively the performance of gradient-based algorithms. …</summary><br> Here we review the key results and their interpretation in non-technical terms accessible to a wide audience of physicists in the context of related works.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00483v1" style="color: #d9230f">Reject Illegal Inputs with Generative Classifier Derived from Any Discriminative Classifier</a></b><br><em>Machine Learning, Machine Learning</em>. 1 authors. <a href="http://arxiv.org/pdf/2001.00483v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Generative classifiers have been shown promising to detect illegal inputs including adversarial examples and out-of-distribution samples. Supervised Deep Infomax~(SDIM) is a scalable end-to-end framework to learn generative classifiers. …</summary><br> In this paper, we propose a modification of SDIM termed SDIM-. Instead of training generative classifier from scratch, SDIM- first takes as input the logits produced any given discriminative classifier, and generate logit representations; then a generative classifier is derived by imposing statistical constraints on logit representations. SDIM- could inherit the performance of the discriminative classifier without loss. SDIM- incurs a negligible number of additional parameters, and can be efficiently trained with base classifiers fixed. We perform , where test samples whose class conditionals are smaller than pre-chosen thresholds will be rejected without predictions. Experiments on illegal inputs, including adversarial examples, samples with common corruptions, and out-of-distribution~(OOD) samples show that allowed to reject a portion of test samples, SDIM- significantly improves the performance on the left test sets.
</details>
</td>
</tr>
</tbody>
</table>
</div>
<h2 id="data-science-arxiv-by-primary-tag">Data Science arXiv by Primary Tag</h2>
<p>The tables below show abstracts organized by category with hyperlinks back to the arXiv site.</p>
<div class="layout-chunk" data-layout="l-page">

<h3 id="computer-science">Computer Science</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="10">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Machine Learning (cs.LG)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00449v1" style="color: #d9230f">Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics</a></b><br><em>Machine Learning, Machine Learning, Robotics</em>. 10 authors. <a href="http://arxiv.org/pdf/2001.00449v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Many real-world control problems involve both discrete decision variables - such as the choice of control modes, gear switching or digital outputs - as well as continuous decision variables - such as velocity setpoints, control gains or analogue outputs. However, when defining the corresponding optimal control or reinforcement learning problem, it is commonly approximated with fully continuous or fully discrete action spaces. …</summary><br> These simplifications aim at tailoring the problem to a particular algorithm or solver which may only support one type of action space. Alternatively, expert heuristics are used to remove discrete actions from an otherwise continuous space. In contrast, we propose to treat hybrid problems in their ‘native’ form by solving them with hybrid reinforcement learning, which optimizes for discrete and continuous actions simultaneously. In our experiments, we first demonstrate that the proposed approach efficiently solves such natively hybrid reinforcement learning problems. We then show, both in simulation and on robotic hardware, the benefits of removing possibly imperfect expert-designed heuristics. Lastly, hybrid reinforcement learning encourages us to rethink problem definitions. We propose reformulating control problems, e.g. by adding meta actions, to improve exploration or reduce mechanical wear and tear.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00461v1" style="color: #d9230f">Reasoning on Knowledge Graphs with Debate Dynamics</a></b><br><em>Machine Learning, Machine Learning</em>. 6 authors. <a href="http://arxiv.org/pdf/2001.00461v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We propose a novel method for automatic reasoning on knowledge graphs based on debate dynamics. The main idea is to frame the task of triple classification as a debate game between two reinforcement learning agents which extract arguments – paths in the knowledge graph – with the goal to promote the fact being true (thesis) or the fact being false (antithesis), respectively. …</summary><br> Based on these arguments, a binary classifier, called the judge, decides whether the fact is true or false. The two agents can be considered as sparse, adversarial feature generators that present interpretable evidence for either the thesis or the antithesis. In contrast to other black-box methods, the arguments allow users to get an understanding of the decision of the judge. Since the focus of this work is to create an explainable method that maintains a competitive predictive accuracy, we benchmark our method on the triple classification and link prediction task. Thereby, we find that our method outperforms several baselines on the benchmark datasets FB15k-237, WN18RR, and Hetionet. We also conduct a survey and find that the extracted arguments are informative for users.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00491v1" style="color: #d9230f">Dataset of Video Game Development Problems</a></b><br><em>Software Engineering</em>. 5 authors. <a href="http://arxiv.org/pdf/2001.00491v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Different from traditional software development, there is little information about the software-engineering process and techniques in video-game development. One popular way to share knowledge among the video-game developers’ community is the publishing of postmortems, which are documents summarizing what happened during the video-game development project. …</summary><br> However, these documents are written without formal structure and often providing disparate information. Through this paper, we provide developers and researchers with grounded dataset describing software-engineering problems in video-game development extracted from postmortems. We created the dataset using an iterative method through which we manually coded more than 200 postmortems spanning 20 years (1998 to 2018) and extracted 1,035 problems related to software engineering while maintaining traceability links to the postmortems. We grouped the problems in 20 different types. This dataset is useful to understand the problems faced by developers during video-game development, providing researchers and practitioners a starting point to study video-game development in the context of software engineering.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00503v1" style="color: #d9230f">Joint Goal and Strategy Inference across Heterogeneous Demonstrators via Reward Network Distillation</a></b><br><em>Machine Learning, Machine Learning, Robotics, Artificial Intelligence</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00503v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Reinforcement learning (RL) has achieved tremendous success as a general framework for learning how to make decisions. However, this success relies on the interactive hand-tuning of a reward function by RL experts. …</summary><br> On the other hand, inverse reinforcement learning (IRL) seeks to learn a reward function from readily-obtained human demonstrations. Yet, IRL suffers from two major limitations: 1) reward ambiguity - there are an infinite number of possible reward functions that could explain an expert’s demonstration and 2) heterogeneity - human experts adopt varying strategies and preferences, which makes learning from multiple demonstrators difficult due to the common assumption that demonstrators seeks to maximize the same reward. In this work, we propose a method to jointly infer a task goal and humans’ strategic preferences via network distillation. This approach enables us to distill a robust task reward (addressing reward ambiguity) and to model each strategy’s objective (handling heterogeneity). We demonstrate our algorithm can better recover task reward and strategy rewards and imitate the strategies in two simulated tasks and a real-world table tennis task.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00361v1" style="color: #d9230f">Butterfly detection and classification based on integrated YOLO algorithm</a></b><br><em>Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00361v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Insects are abundant species on the earth, and the task of identification and identification of insects is complex and arduous. How to apply artificial intelligence technology and digital image processing methods to automatic identification of insect species is a hot issue in current research. …</summary><br> In this paper, the problem of automatic detection and classification recognition of butterfly photographs is studied, and a method of bio-labeling suitable for butterfly classification is proposed. On the basis of YOLO algorithm, by synthesizing the results of YOLO models with different training mechanisms, a butterfly automatic detection and classification recognition algorithm based on YOLO algorithm is proposed. It greatly improves the generalization ability of YOLO algorithm and makes it have better ability to solve small sample problems. The experimental results show that the proposed annotation method and integrated YOLO algorithm have high accuracy and recognition rate in butterfly automatic detection and recognition.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00360v1" style="color: #d9230f">Kernelized Support Tensor Train Machines</a></b><br><em>Machine Learning, Machine Learning, Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00360v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. …</summary><br> In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for image classification. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. Extensive experiments are performed on real-world tensor data, which demonstrates the superiority of the proposed scheme under few-sample high-dimensional inputs.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00564v1" style="color: #d9230f">Robust Marine Buoy Placement for Ship Detection Using Dropout K-Means</a></b><br><em>Machine Learning, Machine Learning</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00564v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Marine buoys aid in the battle against Illegal, Unreported and Unregulated (IUU) fishing by detecting fishing vessels in their vicinity. Marine buoys, however, may be disrupted by natural causes and buoy vandalism. …</summary><br> To minimize the effects of buoy disruption on a buoy network, we propose a more robust buoy placement using dropout k-means and dropout k-median. We apply dropout k-means and dropout k-median to determine locations for deploying marine buoys in the Gabonese waters near West Africa. We simulated the passage of ships using historical Automatic Identification System (AIS) data, then compared the ship detection probability of dropout k-means to classic k-means and dropout k-median to classic k-median, taking into account that the current sensor detection radius is 10km. With 5 buoys, the buoy arrangement computed by classic k-means, dropout k-means, classic k-median and dropout k-median have ship detection probabilities of 38%, 45%, 48% and 52%.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00528v1" style="color: #d9230f">Non-Parametric Learning of Gaifman Models</a></b><br><em>Machine Learning, Machine Learning</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00528v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider the problem of structure learning for Gaifman models and learn relational features that can be used to derive feature representations from a knowledge base. These relational features are first-order rules that are then partially grounded and counted over local neighborhoods of a Gaifman model to obtain the feature representations. …</summary><br> We propose a method for learning these relational features for a Gaifman model by using relational tree distances. Our empirical evaluation on real data sets demonstrates the superiority of our approach over classical rule-learning.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00561v1" style="color: #d9230f">PrivacyNet: Semi-Adversarial Networks for Multi-attribute Face Privacy</a></b><br><em>Machine Learning, Computer Vision and Pattern Recognition</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00561v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In recent years, the utilization of biometric information has become more and more common for various forms of identity verification and user authentication. However, as a consequence of the widespread use and storage of biometric information, concerns regarding sensitive information leakage and the protection of users’ privacy have been raised. …</summary><br> Recent research efforts targeted these concerns by proposing the Semi-Adversarial Networks (SAN) framework for imparting gender privacy to face images. The objective of SAN is to perturb face image data such that it cannot be reliably used by a gender classifier but can still be used by a face matcher for matching purposes. In this work, we propose a novel Generative Adversarial Networks-based SAN model, PrivacyNet, that is capable of imparting selective soft biometric privacy to multiple soft-biometric attributes such as gender, age, and race. While PrivacyNet is capable of perturbing different sources of soft biometric information reliably and simultaneously, it also allows users to choose to obfuscate specific attributes, while preserving others. The results from extensive experiments on five independent face image databases demonstrate the efficacy of our proposed model in imparting selective multi-attribute privacy to face images.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00346v1" style="color: #d9230f">First image then video: A two-stage network for spatiotemporal video denoising</a></b><br><em>Computer Vision and Pattern Recognition</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00346v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Video denoising is to remove noise from noise-corrupted data, thus recovering true signals via spatiotemporal processing. Existing approaches for spatiotemporal video denoising tend to suffer from motion blur artifacts, that is, the boundary of a moving object tends to appear blurry especially when the object undergoes a fast motion, causing optical flow calculation to break down. …</summary><br> In this paper, we address this challenge by designing a first-image-then-video two-stage denoising neural network, consisting of an image denoising module for spatially reducing intra-frame noise followed by a regular spatiotemporal video denoising module. The intuition is simple yet powerful and effective: the first stage of image denoising effectively reduces the noise level and, therefore, allows the second stage of spatiotemporal denoising for better modeling and learning everywhere, including along the moving object boundaries. This two-stage network, when trained in an end-to-end fashion, yields the state-of-the-art performances on the video denoising benchmark Vimeo90K dataset in terms of both denoising quality and computation. It also enables an unsupervised approach that achieves comparable performance to existing supervised approaches.
</details>
</td>
</tr>
<tr grouplength="5">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Computer Vision and Pattern Recognition (cs.CV)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00543v1" style="color: #d9230f">Adversarial Policies in Learning Systems with Malicious Experts</a></b><br><em>Multiagent Systems, Machine Learning, Machine Learning, Cryptography and Security</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00543v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We consider a learning system based on the conventional multiplicative weight (MW) rule that combines experts’ advice to predict a sequence of true outcomes. It is assumed that one of the experts is malicious and aims to impose the maximum loss on the system. …</summary><br> The loss of the system is naturally defined to be the aggregate absolute difference between the sequence of predicted outcomes and the true outcomes. We consider this problem under both offline and online settings. In the offline setting where the malicious expert must choose its entire sequence of decisions a priori, we show somewhat surprisingly that a simple greedy policy of always reporting false prediction is asymptotically optimal with an approximation ratio of <span class="math inline">\(1+O(\sqrt{\frac{\ln N}{N}})\)</span>, where <span class="math inline">\(N\)</span> is the total number of prediction stages. In particular, we describe a policy that closely resembles the structure of the optimal offline policy. For the online setting where the malicious expert can adaptively make its decisions, we show that the optimal online policy can be efficiently computed by solving a dynamic program in <span class="math inline">\(O(N^2)\)</span>. Our results provide a new direction for vulnerability assessment of commonly used learning algorithms to adversarial attacks where the threat is an integral part of the system.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00448v1" style="color: #d9230f">Inter- and Intra-domain Knowledge Transfer for Related Tasks in Deep Character Recognition</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00448v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Pre-training a deep neural network on the ImageNet dataset is a common practice for training deep learning models, and generally yields improved performance and faster training times. The technique of pre-training on one task and then retraining on a new one is called transfer learning. …</summary><br> In this paper we analyse the effectiveness of using deep transfer learning for character recognition tasks. We perform three sets of experiments with varying levels of similarity between source and target tasks to investigate the behaviour of different types of knowledge transfer. We transfer both parameters and features and analyse their behaviour. Our results demonstrate that no significant advantage is gained by using a transfer learning approach over a traditional machine learning approach for our character recognition tasks. This suggests that using transfer learning does not necessarily presuppose a better performing model in all cases.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00338v1" style="color: #d9230f">Informal Data Transformation Considered Harmful</a></b><br><em>Artificial Intelligence, Databases</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00338v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>NA …</summary><br>NA
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00329v1" style="color: #d9230f">On Consequentialism and Fairness</a></b><br><em>Computers and Society, Machine Learning, Artificial Intelligence, Machine Learning</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00329v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage “fair” outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. …</summary><br> Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00526v1" style="color: #d9230f">Lightweight Residual Densely Connected Convolutional Neural Network</a></b><br><em>Machine Learning, Computer Vision and Pattern Recognition</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00526v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Extremely efficient convolutional neural network architectures are one of the most important requirements for limited computing power devices (such as embedded and mobile devices). Recently, some architectures have been proposed to overcome this limitation by considering specific hardware-software equipment. …</summary><br> In this paper, the residual densely connected blocks are proposed to guaranty the deep supervision, efficient gradient flow, and feature reuse abilities of convolutional neural network. The proposed method decreases the cost of training and inference processes without using any special hardware-software equipment by just reducing the number of parameters and computational operations while achieving a feasible accuracy. Extensive experimental results demonstrate that the proposed architecture is more efficient than the AlexNet and VGGNet in terms of model size, required parameters, and even accuracy. The proposed model is evaluated on the ImageNet, MNIST, Fashion MNIST, SVHN, CIFAR-10, and CIFAR-100. It achieves state-of-the-art results on the Fashion MNIST dataset and reasonable results on the others. The obtained results show that the proposed model is superior to efficient models such as the SqueezNet and is also comparable with the state-of-the-art efficient models such as CondenseNet and ShuffleNet.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Artificial Intelligence (cs.AI)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00487v1" style="color: #d9230f">Using CNNs For Users Segmentation In Video See-Through Augmented Virtuality</a></b><br><em>Computer Vision and Pattern Recognition</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00487v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>In this paper, we present preliminary results on the use of deep learning techniques to integrate the users self-body and other participants into a head-mounted video see-through augmented virtuality scenario. It has been previously shown that seeing users bodies in such simulations may improve the feeling of both self and social presence in the virtual environment, as well as user performance. …</summary><br> We propose to use a convolutional neural network for real time semantic segmentation of users bodies in the stereoscopic RGB video streams acquired from the perspective of the user. We describe design issues as well as implementation details of the system and demonstrate the feasibility of using such neural networks for merging users bodies in an augmented virtuality simulation.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Databases (cs.DB)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00479v1" style="color: #d9230f">Thresholds of descending algorithms in inference problems</a></b><br><em>Machine Learning, Machine Learning, Disordered Systems and Neural Networks</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00479v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We review recent works on analyzing the dynamics of gradient-based algorithms in a prototypical statistical inference problem. Using methods and insights from the physics of glassy systems, these works showed how to understand quantitatively and qualitatively the performance of gradient-based algorithms. …</summary><br> Here we review the key results and their interpretation in non-technical terms accessible to a wide audience of physicists in the context of related works.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Software Engineering (cs.SE)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00483v1" style="color: #d9230f">Reject Illegal Inputs with Generative Classifier Derived from Any Discriminative Classifier</a></b><br><em>Machine Learning, Machine Learning</em>. 1 authors. <a href="http://arxiv.org/pdf/2001.00483v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Generative classifiers have been shown promising to detect illegal inputs including adversarial examples and out-of-distribution samples. Supervised Deep Infomax~(SDIM) is a scalable end-to-end framework to learn generative classifiers. …</summary><br> In this paper, we propose a modification of SDIM termed SDIM-. Instead of training generative classifier from scratch, SDIM- first takes as input the logits produced any given discriminative classifier, and generate logit representations; then a generative classifier is derived by imposing statistical constraints on logit representations. SDIM- could inherit the performance of the discriminative classifier without loss. SDIM- incurs a negligible number of additional parameters, and can be efficiently trained with base classifiers fixed. We perform , where test samples whose class conditionals are smaller than pre-chosen thresholds will be rejected without predictions. Experiments on illegal inputs, including adversarial examples, samples with common corruptions, and out-of-distribution~(OOD) samples show that allowed to reject a portion of test samples, SDIM- significantly improves the performance on the left test sets.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="statistics">Statistics</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="2">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Applications (stat.AP)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00412v1" style="color: #d9230f">Circular Regression Trees and Forests with an Application to Probabilistic Wind Direction Forecasting</a></b><br><em>Methodology</em>. 6 authors. <a href="http://arxiv.org/pdf/2001.00412v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>While circular data occur in a wide range of scientific fields, the methodology for distributional modeling and probabilistic forecasting of circular response variables is rather limited. Most of the existing methods are built on the framework of generalized linear and additive models, which are often challenging to optimize and to interpret. …</summary><br> Therefore, we suggest circular regression trees and random forests as an intuitive alternative approach that is relatively easy to fit. Building on previous ideas for trees modeling circular means, we suggest a distributional approach for both trees and forests yielding probabilistic forecasts based on the von Mises distribution. The resulting tree-based models simplify the estimation process by using the available covariates for partitioning the data into sufficiently homogeneous subgroups so that a simple von Mises distribution without further covariates can be fitted to the circular response in each subgroup. These circular regression trees are straightforward to interpret, can capture nonlinear effects and interactions, and automatically select the relevant covariates that are associated with either location and/or scale changes in the von Mises distribution. Combining an ensemble of circular regression trees to a circular regression forest yields a local adaptive likelihood estimator for the von Mises distribution that can regularize and smooth the covariate effects. The new methods are evaluated in a case study on probabilistic wind direction forecasting at two Austrian airports, considering other common approaches as a benchmark.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00396v1" style="color: #d9230f">Restricting the Flow: Information Bottlenecks for Attribution</a></b><br><em>Machine Learning, Machine Learning, Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00396v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. …</summary><br> In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method’s information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network’s decision.
</details>
</td>
</tr>
<tr grouplength="2">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Machine Learning (stat.ML)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00559v1" style="color: #d9230f">A Deep Structural Model for Analyzing Correlated Multivariate Time Series</a></b><br><em>Machine Learning, Machine Learning</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00559v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Multivariate time series are routinely encountered in real-world applications, and in many cases, these time series are strongly correlated. In this paper, we present a deep learning structural time series model which can (i) handle correlated multivariate time series input, and (ii) forecast the targeted temporal sequence by explicitly learning/extracting the trend, seasonality, and event components. …</summary><br> The trend is learned via a 1D and 2D temporal CNN and LSTM hierarchical neural net. The CNN-LSTM architecture can (i) seamlessly leverage the dependency among multiple correlated time series in a natural way, (ii) extract the weighted differencing feature for better trend learning, and (iii) memorize the long-term sequential pattern. The seasonality component is approximated via a non-liner function of a set of Fourier terms, and the event components are learned by a simple linear function of regressor encoding the event dates. We compare our model with several state-of-the-art methods through a comprehensive set of experiments on a variety of time series data sets, such as forecasts of Amazon AWS Simple Storage Service (S3) and Elastic Compute Cloud (EC2) billings, and the closing prices for corporate stocks in the same category.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00405v1" style="color: #d9230f">CircSpaceTime: an R package for spatial and spatio-temporal modeling of Circular data</a></b><br><em>Applications</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00405v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>CircSpaceTime is the only R package currently available that implements Bayesian models for spatial and spatio-temporal interpolation of circular data. Such data are often found in applications where, among the many, wind directions, animal movement directions, and wave directions are involved. …</summary><br> To analyze such data we need models for observations at locations s and times t, as the so-called geostatistical models, providing structured dependence assumed to decay in distance and time. The approach we take begins with Gaussian processes defined for linear variables over space and time. Then, we use either wrapping or projection to obtain processes for circular data. The models are cast as hierarchical, with fitting and inference within a Bayesian framework. Altogether, this package implements work developed by a series of papers; the most relevant being Jona Lasinio, Gelfand, and Jona Lasinio (2012); Wang and Gelfand (2014); Mastrantonio, Jona Lasinio, and Gelfand (2016). All procedures are written using Rcpp. Estimates are obtained by MCMC allowing parallelized multiple chains run. The implementation of the proposed models is considerably improved on the simple routines adopted in the research papers. As original running examples, for the spatial and spatio-temporal settings, we use wind directions datasets over central Italy.
</details>
</td>
</tr>
<tr grouplength="2">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Methodology (stat.ME)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00299v1" style="color: #d9230f">Concentration of Benefit index: A threshold-free summary metric for quantifying the capacity of covariates to yield efficient treatment rules</a></b><br><em>Applications, Methodology</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00299v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>When data on treatment assignment, outcomes, and covariates from a randomized trial are available, a question of interest is to what extent covariates can be used to optimize treatment decisions. Statistical hypothesis testing of covariate-by-treatment interaction is ill-suited for this purpose. …</summary><br> The application of decision theory results in treatment rules that compare the expected benefit of treatment given the patient’s covariates against a treatment threshold. However, determining treatment threshold is often context-specific, and any given threshold might seem arbitrary when the overall capacity towards predicting treatment benefit is of concern. We propose the Concentration of Benefit index (Cb), a threshold-free metric that quantifies the combined performance of covariates towards finding individuals who will benefit the most from treatment. The construct of the proposed index is comparing expected treatment outcomes with and without knowledge of covariates when one of a two randomly selected patients are to be treated. We show that the resulting index can also be expressed in terms of the integrated efficiency of individualized treatment decision over the entire range of treatment thresholds. We propose parametric and semi-parametric estimators, the latter being suitable for out-of-sample validation and correction for optimism. We used data from a clinical trial to demonstrate the calculations in a step-by-step fashion, and have provided the R code for implementation (<a href="https://github.com/msadatsafavi/txBenefit" class="uri">https://github.com/msadatsafavi/txBenefit</a>). The proposed index has intuitive and theoretically sound interpretation and can be estimated with relative ease for a wide class of regression models. Beyond the conceptual developments, various aspects of estimation and inference for such a metric need to be pursued in future research.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00419v1" style="color: #d9230f">Prediction in locally stationary time series</a></b><br><em>Methodology, Econometrics</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00419v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We develop an estimator for the high-dimensional covariance matrix of a locally stationary process with a smoothly varying trend and use this statistic to derive consistent predictors in non-stationary time series. In contrast to the currently available methods for this problem the predictor developed here does not rely on fitting an autoregressive model and does not require a vanishing trend. …</summary><br> The finite sample properties of the new methodology are illustrated by means of a simulation study and a financial indices study.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="elec.-eng.-and-systems-science">Elec. Eng. and Systems Science</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="4">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Image and Video Processing (eess.IV)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00425v1" style="color: #d9230f">Kalman Filtering and Expectation Maximization for Multitemporal Spectral Unmixing</a></b><br><em>Image and Video Processing, Computer Vision and Pattern Recognition</em>. 5 authors. <a href="http://arxiv.org/pdf/2001.00425v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>The recent evolution of hyperspectral imaging technology and the proliferation of new emerging applications presses for the processing of multiple temporal hyperspectral images. In this work, we propose a novel spectral unmixing (SU) strategy using physically motivated parametric endmember representations to account for temporal spectral variability. …</summary><br> By representing the multitemporal mixing process using a state-space formulation, we are able to exploit the Bayesian filtering machinery to estimate the endmember variability coefficients. Moreover, by assuming that the temporal variability of the abundances is small over short intervals, an efficient implementation of the expectation maximization (EM) algorithm is employed to estimate the abundances and the other model parameters. Simulation results indicate that the proposed strategy outperforms state-of-the-art multitemporal SU algorithms.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00340v1" style="color: #d9230f">DuDoNet++: Encoding mask projection to reduce CT metal artifacts</a></b><br><em>Image and Video Processing, Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00340v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>CT metal artifact reduction (MAR) is a notoriously challenging task because the artifacts are structured and non-local in the image domain. However, they are inherently local in the sinogram domain. …</summary><br> DuDoNet is the state-of-the-art MAR algorithm which exploits the latter characteristic by learning to reduce artifacts in the sinogram and image domain jointly. By design, DuDoNet treats the metal-affected regions in sinogram as missing and replaces them with the surrogate data generated by a neural network. Since fine-grained details within the metal-affected regions are completely ignored, the artifact-reduced CT images by DuDoNet tend to be over-smoothed and distorted. In this work, we investigate the issue by theoretical derivation. We propose to address the problem by (1) retaining the metal-affected regions in sinogram and (2) replacing the binarized metal trace with the metal mask projection such that the geometry information of metal implants is encoded. Extensive experiments on simulated datasets and expert evaluations on clinical images demonstrate that our network called DuDoNet++ yields anatomically more precise artifact-reduced images than DuDoNet, especially when the metallic objects are large.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00339v1" style="color: #d9230f">Joint Unsupervised Learning for the Vertebra Segmentation, Artifact Reduction and Modality Translation of CBCT Images</a></b><br><em>Image and Video Processing, Computer Vision and Pattern Recognition</em>. 4 authors. <a href="http://arxiv.org/pdf/2001.00339v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We investigate the unsupervised learning of the vertebra segmentation, artifact reduction and modality translation of CBCT images. To this end, we formulate this problem under a unified framework that jointly addresses these three tasks and intensively leverages the knowledge sharing. …</summary><br> The unsupervised learning of this framework is enabled by 1) a novel shape-aware artifact disentanglement network that supports different forms of image synthesis and vertebra segmentation and 2) a deliberate fusion of knowledge from an independent CT dataset. Specifically, the proposed framework takes a random pair of CBCT and CT images as the input, and manipulates the synthesis and segmentation via different combinations of the decodings of the disentangled latent codes. Then, by discovering various forms of consistencies between the synthesized images and segmented , the learning is achieved via self-learning from the given CBCT and CT images obviating the need for the paired (i.e., anatomically identical) groundtruth data. Extensive experiments on clinical CBCT and CT datasets show that the proposed approach performs significantly better than other state-of-the-art unsupervised methods trained independently for each task and, remarkably, the proposed approach achieves a dice coefficient of 0.879 for unsupervised CBCT vertebra segmentation.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00558v1" style="color: #d9230f">Physically Plausible Spectral Reconstruction from RGB Images</a></b><br><em>Image and Video Processing, Computer Vision and Pattern Recognition</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00558v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Recently Convolutional Neural Networks (CNN) have been used to reconstruct hyperspectral information from RGB images. Moreover, this spectral reconstruction problem (SR) can often be solved with good (low) error. …</summary><br> However, these methods are not physically plausible: that is when the recovered spectra are reintegrated with the underlying camera sensitivities, the resulting predicted RGB is not the same as the actual RGB, and sometimes this discrepancy can be large. The problem is further compounded by exposure change. Indeed, most learning-based SR models train for a fixed exposure setting and we show that this can result in poor performance when exposure varies. In this paper we show how CNN learning can be extended so that physical plausibility is enforced and the problem resulting from changing exposures is mitigated. Our SR solution improves the state-of-the-art spectral recovery performance under varying exposure conditions while simultaneously ensuring physical plausibility (the recovered spectra reintegrate to the input RGBs exactly).
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="mathematics">Mathematics</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="2">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Statistics Theory (math.ST)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00370v1" style="color: #d9230f">On the modelling, linear stability, and numerical simulation for advection-diffusion-reaction in poroelastic media</a></b><br><em>Numerical Analysis, Quantitative Methods, Numerical Analysis</em>. 5 authors. <a href="http://arxiv.org/pdf/2001.00370v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We perform the linear growth analysis for a new PDE-based model for poromechanical processes (formulated in mixed form using the solid deformation, fluid pressure, and total pressure) interacting with diffusing and reacting solutes in the medium. We find parameter regions that lead to interesting behaviour of the coupled system. …</summary><br> These mutual dependences between deformation and diffusive patterns are of substantial relevance in the study of morphoelastic changes in biomaterials. We provide a set of computational examples in 2D and 3D that can be used to form a better understanding on how, and up to which extent, the deformations of the porous structure dictate the generation and suppression of spatial patterning dynamics, also related to the onset of mechanochemical waves.
</details>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00397v1" style="color: #d9230f">Modified Pillai’s trace statistics for two high-dimensional sample covariance matrices</a></b><br><em>Statistics Theory, Statistics Theory</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00397v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>The goal of this study was to test the equality of two covariance matrices by using modified Pillai’s trace statistics under a high-dimensional framework, i.e. …</summary><br>, the dimension and sample sizes go to infinity proportionally. In this paper, we introduce two modified Pillai’s trace statistics and obtain their asymptotic distributions under the null hypothesis. The benefits of the proposed statistics include the following: (1) the sample size can be smaller than the dimensions; (2) the limiting distributions of the proposed statistics are universal; and (3) we do not restrict the structure of the population covariance matrices. The theoretical results are established under mild and practical assumptions, and their properties are demonstrated numerically by simulations and a real data analysis.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Numerical Analysis (math.NA)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00519v1" style="color: #d9230f">On the Distribution of an Arbitrary Subset of the Eigenvalues for some Finite Dimensional Random Matrices</a></b><br><em>Statistics Theory, Statistics Theory, Information Theory, Information Theory</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00519v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We present some new results on the joint distribution of an arbitrary subset of the ordered eigenvalues of complex Wishart, double Wishart, and Gaussian hermitian random matrices of finite dimensions, using a tensor pseudo-determinant operator. Specifically, we derive compact expressions for the joint probability distribution function of the eigenvalues and the expectation of functions of the eigenvalues, including joint moments, for the case of both ordered and unordered eigenvalues. …</summary><br>
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="physics">Physics</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Chemical Physics (physics.chem-ph)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00416v1" style="color: #d9230f">The Quest For Highly Accurate Excitation Energies: A Computational Perspective</a></b><br><em>Chemical Physics, Other Condensed Matter, Strongly Correlated Electrons, Computational Physics</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00416v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We provide an overview of the successive steps that made possible to obtain increasingly accurate excitation energies and properties with computational chemistry tools, eventually leading to chemically accurate vertical transition energies for small- and medium-size molecules. First, we describe the evolution of  state-of-the-art methods employed to define benchmark values, with originally Roos’ CASPT2 method, and then third-order coupled cluster (CC3) methods as in the renowned Thiel set of vertical excitation energies described in a remarkable series of papers in the 2000’s. …</summary><br> More recently, this quest for highly accurate excitation energies was reinitiated thanks to the resurgence of selected configuration interaction (SCI) methods and their efficient parallel implementation. These methods have been able to routinely deliver highly accurate excitation energies for small molecules, as well as medium-size molecules with compact basis sets, for both single and double excitations. Second, we describe how these high-level methods and the creation of large, diverse, and accurate benchmark sets of excitation energies have allowed to assess fairly and accurately the performance of computationally lighter theoretical models (, TD-DFT, BSE@, ADC, EOM-CC, etc.) for different types of excited states (<span class="math inline">\(\pi \rightarrow \pi^*\)</span>, <span class="math inline">\(n \rightarrow \pi^*\)</span>, valence, Rydberg, singlet, triplet, double excitation, etc). We conclude this  by discussing the current potentiality of these methods from both an expert and a non-expert points of view, and what we believe could be the future theoretical and technological developments in the field.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Computational Physics (physics.comp-ph)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00433v1" style="color: #d9230f">Estimation of roughness measurement bias originating from background subtraction</a></b><br><em>Data Analysis, Statistics and Probability</em>. 3 authors. <a href="http://arxiv.org/pdf/2001.00433v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>When measuring the roughness of rough surfaces, the limited sizes of scanned areas lead to its systematic underestimation. Levelling by polynomials and other filtering used in real-world processing of atomic force microscopy data increases this bias considerably. …</summary><br> Here a framework is developed providing explicit expressions for the bias of squared mean square roughness in the case of levelling by fitting a model background function using linear least squares. The framework is then applied to polynomial levelling, for both one-dimensional and two-dimensional data processing, and basic models of surface autocorrelation function, Gaussian and exponential. Several other common scenarios are covered as well, including median levelling, intermediate Gaussian–exponential autocorrelation model and frequency space filtering. Application of the results to other quantities, such as Rq, Sq, Ra and~Sa is discussed. The results are summarized in overview plots covering a range of autocorrelation functions and polynomial degrees, which allow graphical estimation of the bias.
</details>
</td>
</tr>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Data Analysis, Statistics and Probability (physics.data-an)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00379v1" style="color: #d9230f">The free and freer XY models</a></b><br><em>Disordered Systems and Neural Networks, Computational Physics</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00379v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>We study two versions of the XY model where the spins but also the interaction topology is allowed to change. In the free XY model, the number of links is fixed, but their positions in the network are not. …</summary><br> We also study a more relaxed version where even the number of links is allowed to vary, we call it the freer XY model. When the interaction networks are dense enough, both models have phase transitions visible both in spin configurations and the network structure. The low-temperature phase in the free XY model, is characterized by tightly connected clusters of spins pointing in the same direction, and isolated spins disconnected from the rest. For the freer XY model the low-temperature phase is almost completely connected. In both models, exponents describing the magnetic ordering are mostly consistent with values of the mean-field theory of the standard XY model.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="other">Other</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Instrumentation and Methods for Astrophysics (astro-ph.IM)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00563v1" style="color: #d9230f">Using Data Imputation for Signal Separation in High Contrast Imaging</a></b><br><em>Machine Learning, Earth and Planetary Astrophysics, Instrumentation and Methods for Astrophysics, Solar and Stellar Astrophysics</em>. 8 authors. <a href="http://arxiv.org/pdf/2001.00563v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>To characterize circumstellar systems in high contrast imaging, the fundamental step is to construct a best point spread function (PSF) template for the non-circumstellar signals (i.e. …</summary><br>, star light and speckles) and separate it from the observation. With existing PSF construction methods, the circumstellar signals (e.g., planets, circumstellar disks) are unavoidably altered by over-fitting and/or self-subtraction, making forward modeling a necessity to recover these signals. We present a forward modeling–free solution to these problems with data imputation using sequential non-negative matrix factorization (DI-sNMF). DI-sNMF first converts this signal separation problem to a “missing data” problem in statistics by flagging the regions which host circumstellar signals as missing data, then attributes PSF signals to these regions. We mathematically prove it to have negligible alteration to circumstellar signals when the imputation region is relatively small, which thus enables precise measurement for these circumstellar objects. We apply it to simulated point source and circumstellar disk observations to demonstrate its proper recovery of them. We apply it to Gemini Planet Imager (GPI) K1-band observations of the debris disk surrounding HR 4796A, finding a tentative trend that the dust is more forward scattering as the wavelength increases. We expect DI-sNMF to be applicable to other general scenarios where the separation of signals is needed.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="quantitative-biology">Quantitative Biology</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Populations and Evolution (q-bio.PE)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00492v1" style="color: #d9230f">The Effect of Treatment-Related Deaths and “Sticky” Diagnoses on Recorded Prostate Cancer Mortality</a></b><br><em>Populations and Evolution, Quantitative Methods</em>. 5 authors. <a href="http://arxiv.org/pdf/2001.00492v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Background: Although recorded cancer mortality should include both deaths from cancer and deaths from cancer treatment, there is evidence suggesting that the measure may be incomplete. To investigate the completeness of recorded prostate cancer mortality, we compared other-cause (non-prostate cancer) mortality in men found and not found to have prostate cancer following a needle biopsy. …</summary><br> Methods: We linked Medicare claims data to SEER data to analyze survival in the population of men aged 65+ enrolled in Medicare who resided in a SEER area and received a needle biopsy in 1993-2001. We compared other-cause mortality in men found to have prostate cancer (n=53,462) to that in men not found to have prostate cancer (n=103,659). Results: The age-race adjusted other-cause mortality rate was 471 per 10,000 person-years in men found to have prostate cancer vs. 468 per 10,000 in men not found to have prostate cancer (RR = 1.01;95% CI:0.98-1.03). The effect was modified, however, by age. The RR declined in a stepwise fashion from 1.08 (95% CI:1.03-1.14) in men age 65-69 to 0.89 (95% CI:0.83-0.95) in men age 85 and older. If the excess (or deficit) in other-cause mortality were added to the recorded prostate cancer mortality, prostate cancer mortality would rise 23% in the youngest age group (from 90 to 111 per 10,000) and would fall 30% in the oldest age group (from 551 to 388 per 10,000). Conclusion: Although recorded prostate cancer mortality appears to be an accurate measure overall, it systematically underestimates the mortality associated with prostate cancer diagnosis and treatment in younger men and overestimates it in the very old. We surmise that in younger men treatment-related deaths are incompletely captured in recorded prostate cancer mortality, while in older men the diagnosis “sticks”– once diagnosed, they are more likely to be said to have died from the disease.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="quantitative-finance">Quantitative Finance</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Risk Management (q-fin.RM)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00529v1" style="color: #d9230f">The Impact of the Choice of Risk and Dispersion Measure on Procyclicality</a></b><br><em>Risk Management, Applications</em>. 2 authors. <a href="http://arxiv.org/pdf/2001.00529v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Procyclicality of historical risk measure estimation means that one tends to over-estimate future risk when present realized volatility is high and vice versa under-estimate future risk when the realized volatility is low. Out of it different questions arise, relevant for applications and theory: What are the factors which affect the degree of procyclicality? More specifically, how does the choice of risk measure affect this? How does this behaviour vary with the choice of realized volatility estimator? How do different underlying model assumptions influence the pro-cyclical effect? In this paper we consider three different well-known risk measures (Value-at-Risk, Expected Shortfall, Expectile), the r-th absolute centred sample moment, for any integer <span class="math inline">\(r&amp;gt;0\)</span>, as realized volatility estimator (this includes the sample variance and the sample mean absolute deviation around the sample mean) and two models (either an iid model or an augmented GARCH(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) model). …</summary><br> We show that the strength of procyclicality depends on these three factors, the choice of risk measure, the realized volatility estimator and the model considered. But, no matter the choices, the procyclicality will always be present.
</details>
</td>
</tr>
</tbody>
</table>
<h3 id="quantum-physics">Quantum Physics</h3>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr grouplength="1">
<td colspan="1" style="background-color: #666; color: #fff;">
<strong>Quantum Physics (quant-ph)</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
<b><a href="http://arxiv.org/abs/2001.00550v1" style="color: #d9230f">Cost-Function-Dependent Barren Plateaus in Shallow Quantum Neural Networks</a></b><br><em>Machine Learning, Quantum Physics</em>. 5 authors. <a href="http://arxiv.org/pdf/2001.00550v1" style="color: #d9230f;
    text-decoration: none;">pdf</a><br>
<details>
<summary>Variational quantum algorithms (VQAs) optimize the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> of a quantum neural network <span class="math inline">\(V(\boldsymbol{\theta})\)</span> to minimize a cost function <span class="math inline">\(C\)</span>. While VQAs may enable practical applications of noisy quantum computers, they are nevertheless heuristic methods with unproven scaling. …</summary><br> Here, we rigorously prove two results. Our first result states that defining <span class="math inline">\(C\)</span> in terms of global observables leads to an exponentially vanishing gradient (i.e., a barren plateau) even when <span class="math inline">\(V(\boldsymbol{\theta})\)</span> is shallow. This implies that several VQAs in the literature must revise their proposed cost functions. On the other hand, our second result states that defining <span class="math inline">\(C\)</span> with local observables leads to a polynomially vanishing gradient, so long as the depth of <span class="math inline">\(V(\boldsymbol{\theta})\)</span> is <span class="math inline">\(\mathcal{O}(\log n)\)</span>. Taken together, our results establish a precise connection between locality and trainability. Finally, we illustrate these ideas with large-scale simulations, up to 100 qubits, of a particular VQA known as quantum autoencoders.
</details>
</td>
</tr>
</tbody>
</table>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
