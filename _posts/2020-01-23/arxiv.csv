title,summary,primary_tag,tags,n_tags,primary_category,categories,author,authors,n_authors,url_pdf,url_href,date
StarAI: Reducing incompleteness in the game of Bridge using PLP,"Bridge is a trick-taking card game requiring the ability to evaluate
probabilities since it is a game of incomplete information where each player
only sees its cards. In order to choose a strategy, a player needs to gather
information about the hidden cards in the other players' hand. We present a
methodology allowing us to model a part of card playing in Bridge using
Probabilistic Logic Programming.",cs.AI,cs.AI,1,Artificial Intelligence,Artificial Intelligence,V Ventos,"J Li, S Thepaut, V Ventos",3,http://arxiv.org/pdf/2001.08193v1,http://arxiv.org/abs/2001.08193v1,2020-01-22
"Automatic phantom test pattern classification through transfer learning
  with deep neural networks","Imaging phantoms are test patterns used to measure image quality in computer
tomography (CT) systems. A new phantom platform (Mercury Phantom, Gammex)
provides test patterns for estimating the task transfer function (TTF) or noise
power spectrum (NPF) and simulates different patient sizes. Determining which
image slices are suitable for analysis currently requires manual annotation of
these patterns by an expert, as subtle defects may make an image unsuitable for
measurement. We propose a method of automatically classifying these test
patterns in a series of phantom images using deep learning techniques. By
adapting a convolutional neural network based on the VGG19 architecture with
weights trained on ImageNet, we use transfer learning to produce a classifier
for this domain. The classifier is trained and evaluated with over 3,500
phantom images acquired at a university medical center. Input channels for
color images are successfully adapted to convey contextual information for
phantom images. A series of ablation studies are employed to verify design
aspects of the classifier and evaluate its performance under varying training
conditions. Our solution makes extensive use of image augmentation to produce a
classifier that accurately classifies typical phantom images with 98% accuracy,
while maintaining as much as 86% accuracy when the phantom is improperly
imaged.",eess.IV,"cs.NE, cs.LG, eess.IV, cs.CV, physics.med-ph",5,Image and Video Processing,"Neural and Evolutionary Computing, Machine Learning, Image and Video Processing, Computer Vision and Pattern Recognition, Medical Physics",Ehsan Samei,"Rafael B. Fricks, Justin Solomon, Ehsan Samei",3,http://arxiv.org/pdf/2001.08189v1,http://arxiv.org/abs/2001.08189v1,2020-01-22
"DeepEnroll: Patient-Trial Matching with Deep Embeddingand Entailment
  Prediction","Clinical trials are essential for drug development but often suffer from
expensive, inaccurate and insufficient patient recruitment. The core problem of
patient-trial matching is to find qualified patients for a trial, where patient
information is stored in electronic health records (EHR) while trial
eligibility criteria (EC) are described in text documents available on the web.
How to represent longitudinal patient EHR? How to extract complex logical rules
from EC? Most existing works rely on manual rule-based extraction, which is
time consuming and inflexible for complex inference. To address these
challenges, we proposed DeepEnroll, a cross-modal inference learning model to
jointly encode enrollment criteria (text) and patients records (tabular data)
into a shared latent space for matching inference. DeepEnroll applies a
pre-trained Bidirectional Encoder Representations from Transformers(BERT) model
to encode clinical trial information into sentence embedding. And uses a
hierarchical embedding model to represent patient longitudinal EHR. In
addition, DeepEnroll is augmented by a numerical information embedding and
entailment module to reason over numerical information in both EC and EHR.
These encoders are trained jointly to optimize patient-trial matching score. We
evaluated DeepEnroll on the trial-patient matching task with demonstrated on
real world datasets. DeepEnroll outperformed the best baseline by up to 12.4%
in average F1.",cs.AI,cs.AI,1,Artificial Intelligence,Artificial Intelligence,Jimeng Sun,"Xingyao Zhang, Cao Xiao, Lucas M. Glass, Jimeng Sun",4,http://arxiv.org/pdf/2001.08179v1,http://arxiv.org/abs/2001.08179v1,2020-01-22
Causality based Feature Fusion for Brain Neuro-Developmental Analysis,"Human brain development is a complex and dynamic process that is affected by
several factors such as genetics, sex hormones, and environmental changes. A
number of recent studies on brain development have examined functional
connectivity (FC) defined by the temporal correlation between time series of
different brain regions. We propose to add the directional flow of information
during brain maturation. To do so, we extract effective connectivity (EC)
through Granger causality (GC) for two different groups of subjects, i.e.,
children and young adults. The motivation is that the inclusion of causal
interaction may further discriminate brain connections between two age groups
and help to discover new connections between brain regions. The contributions
of this study are threefold. First, there has been a lack of attention to
EC-based feature extraction in the context of brain development. To this end,
we propose a new kernel-based GC (KGC) method to learn nonlinearity of complex
brain network, where a reduced Sine hyperbolic polynomial (RSP) neural network
was used as our proposed learner. Second, we used causality values as the
weight for the directional connectivity between brain regions. Our findings
indicated that the strength of connections was significantly higher in young
adults relative to children. In addition, our new EC-based feature outperformed
FC-based analysis from Philadelphia neurocohort (PNC) study with better
discrimination of the different age groups. Moreover, the fusion of these two
sets of features (FC + EC) improved brain age prediction accuracy by more than
4%, indicating that they should be used together for brain development studies.",cs.CV,"q-bio.NC, cs.CV, eess.IV",3,Computer Vision and Pattern Recognition,"Neurons and Cognition, Computer Vision and Pattern Recognition, Image and Video Processing",Yu Ping Wang,"Peyman Hosseinzadeh Kassani, Li Xiao, Gemeng Zhang, Julia M. Stephen, Tony W. Wilson, Vince D. Calhoun, Yu Ping Wang",7,http://arxiv.org/pdf/2001.08173v1,http://arxiv.org/abs/2001.08173v1,2020-01-22
"Q-Learning in enormous action spaces via amortized approximate
  maximization","Applying Q-learning to high-dimensional or continuous action spaces can be
difficult due to the required maximization over the set of possible actions.
Motivated by techniques from amortized inference, we replace the expensive
maximization over all actions with a maximization over a small subset of
possible actions sampled from a learned proposal distribution. The resulting
approach, which we dub Amortized Q-learning (AQL), is able to handle discrete,
continuous, or hybrid action spaces while maintaining the benefits of
Q-learning. Our experiments on continuous control tasks with up to 21
dimensional actions show that AQL outperforms D3PG (Barth-Maron et al, 2018)
and QT-Opt (Kalashnikov et al, 2018). Experiments on structured discrete action
spaces demonstrate that AQL can efficiently learn good policies in spaces with
thousands of discrete actions.",cs.LG,"stat.ML, cs.AI, cs.LG",3,Machine Learning,"Machine Learning, Artificial Intelligence, Machine Learning",Volodymyr Mnih,"Tom Van de Wiele, David Warde-Farley, Andriy Mnih, Volodymyr Mnih",4,http://arxiv.org/pdf/2001.08116v1,http://arxiv.org/abs/2001.08116v1,2020-01-22
"Accelerating supply chains with Ant Colony Optimization across range of
  hardware solutions","Ant Colony algorithm has been applied to various optimization problems,
however most of the previous work on scaling and parallelism focuses on
Travelling Salesman Problems (TSPs). Although, useful for benchmarks and new
idea comparison, the algorithmic dynamics does not always transfer to complex
real-life problems, where additional meta-data is required during solution
construction. This paper looks at real-life outbound supply chain problem using
Ant Colony Optimization (ACO) and its scaling dynamics with two parallel ACO
architectures - Independent Ant Colonies (IAC) and Parallel Ants (PA). Results
showed that PA was able to reach a higher solution quality in fewer iterations
as the number of parallel instances increased. Furthermore, speed performance
was measured across three different hardware solutions - 16 core CPU, 68 core
Xeon Phi and up to 4 Geforce GPUs. State of the art, ACO vectorization
techniques such as SS-Roulette were implemented using C++ and CUDA. Although
excellent for TSP, it was concluded that for the given supply chain problem
GPUs are not suitable due to meta-data access footprint required. Furthermore,
compared to their sequential counterpart, vectorized CPU AVX2 implementation
achieved 25.4x speedup on CPU while Xeon Phi with its AVX512 instruction set
reached 148x on PA with Vectorized (PAwV). PAwV is therefore able to scale at
least up to 1024 parallel instances on the supply chain network problem solved.",cs.NE,"cs.NE, cs.DC",2,Neural and Evolutionary Computing,"Neural and Evolutionary Computing, Distributed, Parallel, and Cluster Computing",Tatiana Kalganova,"Ivars Dzalbs, Tatiana Kalganova",2,http://arxiv.org/pdf/2001.08102v1,http://arxiv.org/abs/2001.08102v1,2020-01-22
ManyModalQA: Modality Disambiguation and QA over Diverse Inputs,"We present a new multimodal question answering challenge, ManyModalQA, in
which an agent must answer a question by considering three distinct modalities:
text, images, and tables. We collect our data by scraping Wikipedia and then
utilize crowdsourcing to collect question-answer pairs. Our questions are
ambiguous, in that the modality that contains the answer is not easily
determined based solely upon the question. To demonstrate this ambiguity, we
construct a modality selector (or disambiguator) network, and this model gets
substantially lower accuracy on our challenge set, compared to existing
datasets, indicating that our questions are more ambiguous. By analyzing this
model, we investigate which words in the question are indicative of the
modality. Next, we construct a simple baseline ManyModalQA model, which, based
on the prediction from the modality selector, fires a corresponding pre-trained
state-of-the-art unimodal QA model. We focus on providing the community with a
new manymodal evaluation set and only provide a fine-tuning set, with the
expectation that existing datasets and approaches will be transferred for most
of the training, to encourage low-resource generalization without large,
monolithic training sets for each new task. There is a significant gap between
our baseline models and human performance; therefore, we hope that this
challenge encourages research in end-to-end modality disambiguation and
multimodal QA models, as well as transfer learning. Code and data available at:
https://github.com/hannandarryl/ManyModalQA",cs.CL,"cs.CV, cs.CL, cs.AI",3,Computation and Language,"Computer Vision and Pattern Recognition, Computation and Language, Artificial Intelligence",Mohit Bansal,"Darryl Hannan, Akshay Jain, Mohit Bansal",3,http://arxiv.org/pdf/2001.08034v1,http://arxiv.org/abs/2001.08034v1,2020-01-22
Subjective Knowledge and Reasoning about Agents in Multi-Agent Systems,"Though a lot of work in multi-agent systems is focused on reasoning about
knowledge and beliefs of artificial agents, an explicit representation and
reasoning about the presence/absence of agents, especially in the scenarios
where agents may be unaware of other agents joining in or going offline in a
multi-agent system, leading to partial knowledge/asymmetric knowledge of the
agents is mostly overlooked by the MAS community. Such scenarios lay the
foundations of cases where an agent can influence other agents' mental states
by (mis)informing them about the presence/absence of collaborators or
adversaries. In this paper, we investigate how Kripke structure-based epistemic
models can be extended to express the above notion based on an agent's
subjective knowledge and we discuss the challenges that come along.",cs.MA,"cs.AI, cs.MA",2,Multiagent Systems,"Artificial Intelligence, Multiagent Systems",Deepak Khemani,"Shikha Singh, Deepak Khemani",2,http://arxiv.org/pdf/2001.08016v1,http://arxiv.org/abs/2001.08016v1,2020-01-22
A Neural Architecture for Person Ontology population,"A person ontology comprising concepts, attributes and relationships of people
has a number of applications in data protection, didentification, population of
knowledge graphs for business intelligence and fraud prevention. While
artificial neural networks have led to improvements in Entity Recognition,
Entity Classification, and Relation Extraction, creating an ontology largely
remains a manual process, because it requires a fixed set of semantic relations
between concepts. In this work, we present a system for automatically
populating a person ontology graph from unstructured data using neural models
for Entity Classification and Relation Extraction. We introduce a new dataset
for these tasks and discuss our results.",cs.AI,"cs.IR, cs.CL, cs.AI",3,Artificial Intelligence,"Information Retrieval, Computation and Language, Artificial Intelligence",Berthold Reinwald,"Balaji Ganesan, Riddhiman Dasgupta, Akshay Parekh, Hima Patel, Berthold Reinwald",5,http://arxiv.org/pdf/2001.08013v1,http://arxiv.org/abs/2001.08013v1,2020-01-22
ARAACOM: ARAbic Algerian Corpus for Opinion Mining,"Nowadays, it is no more needed to do an enormous effort to distribute a lot
of forms to thousands of people and collect them, then convert this from into
electronic format to track people opinion about some subjects. A lot of web
sites can today reach a large spectrum with less effort. The majority of web
sites suggest to their visitors to leave backups about their feeling of the
site or events. So, this makes for us a lot of data which need powerful mean to
exploit. Opinion mining in the web becomes more and more an attracting task,
due the increasing need for individuals and societies to track the mood of
people against several subjects of daily life (sports, politics,
television,...). A lot of works in opinion mining was developed in western
languages especially English, such works in Arabic language still very scarce.
In this paper, we propose our approach, for opinion mining in Arabic Algerian
news paper. CCS CONCEPTS $\bullet$Information systems~Sentiment analysis
$\bullet$ Computing methodologies~Natural language processing",cs.CL,"cs.IR, cs.CL, cs.AI",3,Computation and Language,"Information Retrieval, Computation and Language, Artificial Intelligence",Mahieddine Djoudi,"Zitouni Abdelhafid, Hichem Rahab, Abdelhafid Zitouni, Mahieddine Djoudi",4,http://arxiv.org/pdf/2001.08010v1,http://arxiv.org/abs/2001.08010v1,2020-01-22
On Solving Cooperative MARL Problems with a Few Good Experiences,"Cooperative Multi-agent Reinforcement Learning (MARL) is crucial for
cooperative decentralized decision learning in many domains such as search and
rescue, drone surveillance, package delivery and fire fighting problems. In
these domains, a key challenge is learning with a few good experiences, i.e.,
positive reinforcements are obtained only in a few situations (e.g., on
extinguishing a fire or tracking a crime or delivering a package) and in most
other situations there is zero or negative reinforcement. Learning decisions
with a few good experiences is extremely challenging in cooperative MARL
problems due to three reasons. First, compared to the single agent case,
exploration is harder as multiple agents have to be coordinated to receive a
good experience. Second, environment is not stationary as all the agents are
learning at the same time (and hence change policies). Third, scale of problem
increases significantly with every additional agent.
  Relevant existing work is extensive and has focussed on dealing with a few
good experiences in single-agent RL problems or on scalable approaches for
handling non-stationarity in MARL problems. Unfortunately, neither of these
approaches (or their extensions) are able to address the problem of sparse good
experiences effectively. Therefore, we provide a novel fictitious self
imitation approach that is able to simultaneously handle non-stationarity and
sparse good experiences in a scalable manner. Finally, we provide a thorough
comparison (experimental or descriptive) against relevant cooperative MARL
algorithms to demonstrate the utility of our approach.",cs.LG,"cs.MA, cs.AI, cs.LG",3,Machine Learning,"Multiagent Systems, Artificial Intelligence, Machine Learning",Pradeep Varakantham,"Rajiv Ranjan Kumar, Pradeep Varakantham",2,http://arxiv.org/pdf/2001.07993v1,http://arxiv.org/abs/2001.07993v1,2020-01-22
"Get Rid of Suspended Animation Problem: Deep Diffusive Neural Network on
  Graph Semi-Supervised Classification","Existing graph neural networks may suffer from the ""suspended animation
problem"" when the model architecture goes deep. Meanwhile, for some graph
learning scenarios, e.g., nodes with text/image attributes or graphs with
long-distance node correlations, deep graph neural networks will be necessary
for effective graph representation learning. In this paper, we propose a new
graph neural network, namely DIFNET (Graph Diffusive Neural Network), for graph
representation learning and node classification. DIFNET utilizes both neural
gates and graph residual learning for node hidden state modeling, and includes
an attention mechanism for node neighborhood information diffusion. Extensive
experiments will be done in this paper to compare DIFNET against several
state-of-the-art graph neural network models. The experimental results can
illustrate both the learning performance advantages and effectiveness of
DIFNET, especially in addressing the ""suspended animation problem"".",cs.LG,"stat.ML, cs.NE, cs.LG",3,Machine Learning,"Machine Learning, Neural and Evolutionary Computing, Machine Learning",Jiawei Zhang,Jiawei Zhang,1,http://arxiv.org/pdf/2001.07922v1,http://arxiv.org/abs/2001.07922v1,2020-01-22
"Benchmarking Symbolic Execution Using Constraint Problems -- Initial
  Results","Symbolic execution is a powerful technique for bug finding and program
testing. It is successful in finding bugs in real-world code. The core
reasoning techniques use constraint solving, path exploration, and search,
which are also the same techniques used in solving combinatorial problems,
e.g., finite-domain constraint satisfaction problems (CSPs). We propose CSP
instances as more challenging benchmarks to evaluate the effectiveness of the
core techniques in symbolic execution. We transform CSP benchmarks into C
programs suitable for testing the reasoning capabilities of symbolic execution
tools. From a single CSP P, we transform P depending on transformation choice
into different C programs. Preliminary testing with the KLEE, Tracer-X, and
LLBMC tools show substantial runtime differences from transformation and solver
choice. Our C benchmarks are effective in showing the limitations of existing
symbolic execution tools. The motivation for this work is we believe that
benchmarks of this form can spur the development and engineering of improved
core reasoning in symbolic execution engines.",cs.SE,"I.2.0; I.2.1; I.2.3; I.2.4; I.2.8; I.2.11; D.2, cs.SE, cs.LO",3,Software Engineering,"Software Engineering, Logic in Computer Science",Roland H. C. Yap,"Sahil Verma, Roland H. C. Yap",2,http://arxiv.org/pdf/2001.07914v1,http://arxiv.org/abs/2001.07914v1,2020-01-22
"Convergence Time Optimization for Federated Learning over Wireless
  Networks","In this paper, the convergence time of federated learning (FL), when deployed
over a realistic wireless network, is studied. In particular, a wireless
network is considered in which wireless users transmit their local FL models
(trained using their locally collected data) to a base station (BS). The BS,
acting as a central controller, generates a global FL model using the received
local FL models and broadcasts it back to all users. Due to the limited number
of resource blocks (RBs) in a wireless network, only a subset of users can be
selected to transmit their local FL model parameters to the BS at each learning
step. Moreover, since each user has unique training data samples, the BS
prefers to include all local user FL models to generate a converged global FL
model. Hence, the FL performance and convergence time will be significantly
affected by the user selection scheme. Therefore, it is necessary to design an
appropriate user selection scheme that enables users of higher importance to be
selected more frequently. This joint learning, wireless resource allocation,
and user selection problem is formulated as an optimization problem whose goal
is to minimize the FL convergence time while optimizing the FL performance. To
solve this problem, a probabilistic user selection scheme is proposed such that
the BS is connected to the users whose local FL models have significant effects
on its global FL model with high probabilities. Given the user selection
policy, the uplink RB allocation can be determined. To further reduce the FL
convergence time, artificial neural networks (ANNs) are used to estimate the
local FL models of the users that are not allocated any RBs for local FL model
transmission at each given learning step, which enables the BS to enhance its
global FL model and improve the FL convergence speed and performance.",cs.LG,"stat.ML, eess.SP, cs.NI, cs.LG",4,Machine Learning,"Machine Learning, Signal Processing, Networking and Internet Architecture, Machine Learning",Shuguang Cui,"Mingzhe Chen, H. Vincent Poor, Walid Saad, Shuguang Cui",4,http://arxiv.org/pdf/2001.07845v1,http://arxiv.org/abs/2001.07845v1,2020-01-22
"Coarse-Grain Cluster Analysis of Tensors With Application to Climate
  Biome Identification","A tensor provides a concise way to codify the interdependence of complex
data. Treating a tensor as a d-way array, each entry records the interaction
between the different indices. Clustering provides a way to parse the
complexity of the data into more readily understandable information. Clustering
methods are heavily dependent on the algorithm of choice, as well as the chosen
hyperparameters of the algorithm. However, their sensitivity to data scales is
largely unknown.
  In this work, we apply the discrete wavelet transform to analyze the effects
of coarse-graining on clustering tensor data. We are particularly interested in
understanding how scale effects clustering of the Earth's climate system. The
discrete wavelet transform allows classification of the Earth's climate across
a multitude of spatial-temporal scales. The discrete wavelet transform is used
to produce an ensemble of classification estimates, as opposed to a single
classification. Using information theory, we discover a sub-collection of the
ensemble that span the majority of the variance observed, allowing for
efficient consensus clustering techniques that can be used to identify climate
biomes.",cs.LG,"stat.ML, q-bio.QM, cs.LG",3,Machine Learning,"Machine Learning, Quantitative Methods, Machine Learning",Boian Alexandrov,"Derek DeSantis, Phillip J. Wolfram, Katrina Bennett, Boian Alexandrov",4,http://arxiv.org/pdf/2001.07827v1,http://arxiv.org/abs/2001.07827v1,2020-01-22
"Elephant in the Room: An Evaluation Framework for Assessing Adversarial
  Examples in NLP","An adversarial example is an input transformed by small perturbations that
machine learning models consistently misclassify. While there are a number of
methods proposed to generate adversarial examples for text data, it is not
trivial to assess the quality of these adversarial examples, as minor
perturbations (such as changing a word in a sentence) can lead to a significant
shift in their meaning, readability and classification label. In this paper, we
propose an evaluation framework to assess the quality of adversarial examples
based on the aforementioned properties. We experiment with five benchmark
attacking methods and an alternative approach based on an auto-encoder, and
found that these methods generate adversarial examples with poor readability
and content preservation. We also learned that there are multiple factors that
can influence the attacking performance, such as the the length of text
examples and the input domain.",cs.CL,"cs.CL, cs.LG",2,Computation and Language,"Computation and Language, Machine Learning",Jey Han Lau,"Ying Xu, Xu Zhong, Antonio Jose Jimeno Yepes, Jey Han Lau",4,http://arxiv.org/pdf/2001.07820v1,http://arxiv.org/abs/2001.07820v1,2020-01-22
"RDAnet: A Deep Learning Based Approach for Synthetic Aperture Radar
  Image Formation","Synthetic Aperture Radar (SAR) imaging systems operate by emitting radar
signals from a moving object, such as a satellite, towards the target of
interest. Reflected radar echoes are received and later used by image formation
algorithms to form a SAR image. There is great interest in using SAR images in
computer vision tasks such as automatic target recognition. Today, however, SAR
applications consist of multiple operations: image formation followed by image
processing. In this work, we show that deep learning can be used to train a
neural network able to form SAR images from echo data. Results show that our
neural network, RDAnet, can form SAR images comparable to images formed using a
traditional algorithm. This approach opens the possibility to end-to-end SAR
applications where image formation and image processing are integrated into a
single task. We believe that this work is the first demonstration of deep
learning based SAR image formation using real data.",eess.IV,"cs.CV, eess.IV",2,Image and Video Processing,"Computer Vision and Pattern Recognition, Image and Video Processing",John Paul Walters,"Andrew Rittenbach, John Paul Walters",2,http://arxiv.org/pdf/2001.08202v1,http://arxiv.org/abs/2001.08202v1,2020-01-22
Discovering Salient Anatomical Landmarks by Predicting Human Gaze,"Anatomical landmarks are a crucial prerequisite for many medical imaging
tasks. Usually, the set of landmarks for a given task is predefined by experts.
The landmark locations for a given image are then annotated manually or via
machine learning methods trained on manual annotations. In this paper, in
contrast, we present a method to automatically discover and localize anatomical
landmarks in medical images. Specifically, we consider landmarks that attract
the visual attention of humans, which we term visually salient landmarks. We
illustrate the method for fetal neurosonographic images. First, full-length
clinical fetal ultrasound scans are recorded with live sonographer
gaze-tracking. Next, a convolutional neural network (CNN) is trained to predict
the gaze point distribution (saliency map) of the sonographers on scan video
frames. The CNN is then used to predict saliency maps of unseen fetal
neurosonographic images, and the landmarks are extracted as the local maxima of
these saliency maps. Finally, the landmarks are matched across images by
clustering the landmark CNN features. We show that the discovered landmarks can
be used within affine image registration, with average landmark alignment
errors between 4.1% and 10.9% of the fetal head long axis length.",cs.CV,"eess.IV, cs.CV, cs.LG",3,Computer Vision and Pattern Recognition,"Image and Video Processing, Computer Vision and Pattern Recognition, Machine Learning",J. Alison Noble,"Richard Droste, Pierre Chatelain, Lior Drukker, Harshita Sharma, Aris T. Papageorghiou, J. Alison Noble",6,http://arxiv.org/pdf/2001.08188v1,http://arxiv.org/abs/2001.08188v1,2020-01-22
Pruning CNN's with linear filter ensembles,"Despite the promising results of convolutional neural networks (CNNs),
applying them on resource limited devices is still a challenge, mainly due to
the huge memory and computation requirements. To tackle these problems, pruning
can be applied to reduce the network size and number of floating point
operations (FLOPs). Contrary to the \emph{filter norm} method -- that is used
in network pruning and uses the assumption that the smaller this norm, the less
important is the associated component --, we develop a novel filter importance
norm that incorporates the loss caused by the elimination of a component from
the CNN.
  To estimate the importance of a set of architectural components, we measure
the CNN performance as different components are removed. The result is a
collection of filter ensembles -- filter masks -- and associated performance
values. We rank the filters based on a linear and additive model and remove the
least important ones such that the drop in network accuracy is minimal. We
evaluate our method on a fully connected network, as well as on the ResNet
architecture trained on the CIFAR-10 data-set. Using our pruning method, we
managed to remove $60\%$ of the parameters and $64\%$ of the FLOPs from the
ResNet with an accuracy drop of less than $0.6\%$.",cs.LG,"stat.ML, cs.CV, cs.LG",3,Machine Learning,"Machine Learning, Computer Vision and Pattern Recognition, Machine Learning",Lehel Csató,"Csanád Sándor, Szabolcs Pável, Lehel Csató",3,http://arxiv.org/pdf/2001.08142v1,http://arxiv.org/abs/2001.08142v1,2020-01-22
"Optimizing Generative Adversarial Networks for Image Super Resolution
  via Latent Space Regularization","Natural images can be regarded as residing in a manifold that is embedded in
a higher dimensional Euclidean space. Generative Adversarial Networks (GANs)
try to learn the distribution of the real images in the manifold to generate
samples that look real. But the results of existing methods still exhibit many
unpleasant artifacts and distortions even for the cases where the desired
ground truth target images are available for supervised learning such as in
single image super resolution (SISR). We probe for ways to alleviate these
problems for supervised GANs in this paper. We explicitly apply the Lipschitz
Continuity Condition (LCC) to regularize the GAN. An encoding network that maps
the image space to a new optimal latent space is derived from the LCC, and it
is used to augment the GAN as a coupling component. The LCC is also converted
to new regularization terms in the generator loss function to enforce local
invariance. The GAN is optimized together with the encoding network in an
attempt to make the generator converge to a more ideal and disentangled mapping
that can generate samples more faithful to the target images. When the proposed
models are applied to the single image super resolution problem, the results
outperform the state of the art.",eess.IV,"cs.LG, cs.CV, eess.IV",3,Image and Video Processing,"Machine Learning, Computer Vision and Pattern Recognition, Image and Video Processing",Shifu Zhou,"Sheng Zhong, Shifu Zhou",2,http://arxiv.org/pdf/2001.08126v1,http://arxiv.org/abs/2001.08126v1,2020-01-22
Are Accelerometers for Activity Recognition a Dead-end?,"Accelerometer-based (and by extension other inertial sensors) research for
Human Activity Recognition (HAR) is a dead-end. This sensor does not offer
enough information for us to progress in the core domain of HAR---to recognize
everyday activities from sensor data. Despite continued and prolonged efforts
in improving feature engineering and machine learning models, the activities
that we can recognize reliably have only expanded slightly and many of the same
flaws of early models are still present today. Instead of relying on
acceleration data, we should instead consider modalities with much richer
information---a logical choice are images. With the rapid advance in image
sensing hardware and modelling techniques, we believe that a widespread
adoption of image sensors will open many opportunities for accurate and robust
inference across a wide spectrum of human activities.
  In this paper, we make the case for imagers in place of accelerometers as the
default sensor for human activity recognition. Our review of past works has led
to the observation that progress in HAR had stalled, caused by our reliance on
accelerometers. We further argue for the suitability of images for activity
recognition by illustrating their richness of information and the marked
progress in computer vision. Through a feasibility analysis, we find that
deploying imagers and CNNs on device poses no substantial burden on modern
mobile hardware. Overall, our work highlights the need to move away from
accelerometers and calls for further exploration of using imagers for activity
recognition.",cs.CV,cs.CV,1,Computer Vision and Pattern Recognition,Computer Vision and Pattern Recognition,Nicholas D. Lane,"Catherine Tong, Shyam A. Tailor, Nicholas D. Lane",3,http://arxiv.org/pdf/2001.08111v1,http://arxiv.org/abs/2001.08111v1,2020-01-22
Learning to Correct 3D Reconstructions from Multiple Views,"This paper is about reducing the cost of building good large-scale 3D
reconstructions post-hoc. We render 2D views of an existing reconstruction and
train a convolutional neural network (CNN) that refines inverse-depth to match
a higher-quality reconstruction. Since the views that we correct are rendered
from the same reconstruction, they share the same geometry, so overlapping
views complement each other. We take advantage of that in two ways. Firstly, we
impose a loss during training which guides predictions on neighbouring views to
have the same geometry and has been shown to improve performance. Secondly, in
contrast to previous work, which corrects each view independently, we also make
predictions on sets of neighbouring views jointly. This is achieved by warping
feature maps between views and thus bypassing memory-intensive 3D computation.
We make the observation that features in the feature maps are
viewpoint-dependent, and propose a method for transforming features with
dynamic filters generated by a multi-layer perceptron from the relative poses
between views. In our experiments we show that this last step is necessary for
successfully fusing feature maps between views.",cs.CV,"cs.RO, cs.CV",2,Computer Vision and Pattern Recognition,"Robotics, Computer Vision and Pattern Recognition",Paul Newman,"Ştefan Săftescu, Paul Newman",2,http://arxiv.org/pdf/2001.08098v1,http://arxiv.org/abs/2001.08098v1,2020-01-22
UniPose: Unified Human Pose Estimation in Single Images and Videos,"We propose UniPose, a unified framework for human pose estimation, based on
our ""Waterfall"" Atrous Spatial Pooling architecture, that achieves
state-of-art-results on several pose estimation metrics. Current pose
estimation methods utilizing standard CNN architectures heavily rely on
statistical postprocessing or predefined anchor poses for joint localization.
UniPose incorporates contextual segmentation and joint localization to estimate
the human pose in a single stage, with high accuracy, without relying on
statistical postprocessing methods. The Waterfall module in UniPose leverages
the efficiency of progressive filtering in the cascade architecture, while
maintaining multi-scale fields-of-view comparable to spatial pyramid
configurations. Additionally, our method is extended to UniPose-LSTM for
multi-frame processing and achieves state-of-the-art results for temporal pose
estimation in Video. Our results on multiple datasets demonstrate that UniPose,
with a ResNet backbone and Waterfall module, is a robust and efficient
architecture for pose estimation obtaining state-of-the-art results in single
person pose detection for both single images and videos.",cs.CV,cs.CV,1,Computer Vision and Pattern Recognition,Computer Vision and Pattern Recognition,Andreas Savakis,"Bruno Artacho, Andreas Savakis",2,http://arxiv.org/pdf/2001.08095v1,http://arxiv.org/abs/2001.08095v1,2020-01-22
"Depthwise Non-local Module for Fast Salient Object Detection Using a
  Single Thread","Recently deep convolutional neural networks have achieved significant success
in salient object detection. However, existing state-of-the-art methods require
high-end GPUs to achieve real-time performance, which makes them hard to adapt
to low-cost or portable devices. Although generic network architectures have
been proposed to speed up inference on mobile devices, they are tailored to the
task of image classification or semantic segmentation, and struggle to capture
intra-channel and inter-channel correlations that are essential for contrast
modeling in salient object detection. Motivated by the above observations, we
design a new deep learning algorithm for fast salient object detection. The
proposed algorithm for the first time achieves competitive accuracy and high
inference efficiency simultaneously with a single CPU thread. Specifically, we
propose a novel depthwise non-local moudule (DNL), which implicitly models
contrast via harvesting intra-channel and inter-channel correlations in a
self-attention manner. In addition, we introduce a depthwise non-local network
architecture that incorporates both depthwise non-local modules and inverted
residual blocks. Experimental results show that our proposed network attains
very competitive accuracy on a wide range of salient object detection datasets
while achieving state-of-the-art efficiency among all existing deep learning
based algorithms.",cs.CV,cs.CV,1,Computer Vision and Pattern Recognition,Computer Vision and Pattern Recognition,Yizhou Yu,"Haofeng Li, Guanbin Li, Binbin Yang, Guanqi Chen, Liang Lin, Yizhou Yu",6,http://arxiv.org/pdf/2001.08057v1,http://arxiv.org/abs/2001.08057v1,2020-01-22
Attention! A Lightweight 2D Hand Pose Estimation Approach,"Vision based human pose estimation is an non-invasive technology for
Human-Computer Interaction (HCI). Direct use of the hand as an input device
provides an attractive interaction method, with no need for specialized sensing
equipment, such as exoskeletons, gloves etc, but a camera. Traditionally, HCI
is employed in various applications spreading in areas including manufacturing,
surgery, entertainment industry and architecture, to mention a few. Deployment
of vision based human pose estimation algorithms can give a breath of
innovation to these applications. In this letter, we present a novel
Convolutional Neural Network architecture, reinforced with a Self-Attention
module that it can be deployed on an embedded system, due to its lightweight
nature, with just 1.9 Million parameters. The source code and qualitative
results are publicly available.",cs.CV,"cs.HC, cs.CV, cs.LG",3,Computer Vision and Pattern Recognition,"Human-Computer Interaction, Computer Vision and Pattern Recognition, Machine Learning",Antonios Gasteratos,"Nicholas Santavas, Ioannis Kansizoglou, Loukas Bampis, Evangelos Karakasis, Antonios Gasteratos",5,http://arxiv.org/pdf/2001.08047v1,http://arxiv.org/abs/2001.08047v1,2020-01-22
ResDepth: Learned Residual Stereo Reconstruction,"We propose an embarrassingly simple, but very effective scheme for
high-quality dense stereo reconstruction: (i) generate an approximate
reconstruction with your favourite stereo matcher; (ii) rewarp the input images
with that approximate model; and (iii) with the initial reconstruction and the
warped images as input, train a deep network to enhance the reconstruction by
regressing a residual correction. The strategy to only learn the residual
greatly simplifies the learning problem. A standard Unet without bells and
whistles is enough to reconstruct even small surface details, like dormers and
roof substructures in satellite images. We also investigate residual
reconstruction with less information and find that even a single image is
enough to greatly improve an approximate reconstruction. Our full model reduces
the mean absolute error of state-of-the-art stereo reconstruction systems by
>50%, both in our target domain of satellite stereo and on stereo pairs from
the ETH3D benchmark.",cs.CV,cs.CV,1,Computer Vision and Pattern Recognition,Computer Vision and Pattern Recognition,Konrad Schindler,"Corinne Stucker, Konrad Schindler",2,http://arxiv.org/pdf/2001.08026v1,http://arxiv.org/abs/2001.08026v1,2020-01-22
"Safety Concerns and Mitigation Approaches Regarding the Use of Deep
  Learning in Safety-Critical Perception Tasks","Deep learning methods are widely regarded as indispensable when it comes to
designing perception pipelines for autonomous agents such as robots, drones or
automated vehicles. The main reasons, however, for deep learning not being used
for autonomous agents at large scale already are safety concerns. Deep learning
approaches typically exhibit a black-box behavior which makes it hard for them
to be evaluated with respect to safety-critical aspects. While there have been
some work on safety in deep learning, most papers typically focus on high-level
safety concerns. In this work, we seek to dive into the safety concerns of deep
learning methods and present a concise enumeration on a deeply technical level.
Additionally, we present extensive discussions on possible mitigation methods
and give an outlook regarding what mitigation methods are still missing in
order to facilitate an argumentation for the safety of a deep learning method.",cs.LG,"stat.ML, cs.CV, cs.LG",3,Machine Learning,"Machine Learning, Computer Vision and Pattern Recognition, Machine Learning",Stephanie Abrecht,"Oliver Willers, Sebastian Sudholt, Shervin Raafatnia, Stephanie Abrecht",4,http://arxiv.org/pdf/2001.08001v1,http://arxiv.org/abs/2001.08001v1,2020-01-22
"GraphGen: A Scalable Approach to Domain-agnostic Labeled Graph
  Generation","Graph generative models have been extensively studied in the data mining
literature. While traditional techniques are based on generating structures
that adhere to a pre-decided distribution, recent techniques have shifted
towards learning this distribution directly from the data. While learning-based
approaches have imparted significant improvement in quality, some limitations
remain to be addressed. First, learning graph distributions introduces
additional computational overhead, which limits their scalability to large
graph databases. Second, many techniques only learn the structure and do not
address the need to also learn node and edge labels, which encode important
semantic information and influence the structure itself. Third, existing
techniques often incorporate domain-specific rules and lack generalizability.
Fourth, the experimentation of existing techniques is not comprehensive enough
due to either using weak evaluation metrics or focusing primarily on synthetic
or small datasets. In this work, we develop a domain-agnostic technique called
GraphGen to overcome all of these limitations. GraphGen converts graphs to
sequences using minimum DFS codes. Minimum DFS codes are canonical labels and
capture the graph structure precisely along with the label information. The
complex joint distributions between structure and semantic labels are learned
through a novel LSTM architecture. Extensive experiments on million-sized, real
graph datasets show GraphGen to be 4 times faster on average than
state-of-the-art techniques while being significantly better in quality across
a comprehensive set of 11 different metrics. Our code is released at
https://github.com/idea-iitd/graphgen.",cs.LG,"stat.ML, cs.LG",2,Machine Learning,"Machine Learning, Machine Learning",Sayan Ranu,"Nikhil Goyal, Harsh Vardhan Jain, Sayan Ranu",3,http://arxiv.org/pdf/2001.08184v1,http://arxiv.org/abs/2001.08184v1,2020-01-22
"Unsupervised Domain Adaptation for Neural Machine Translation with
  Iterative Back Translation","State-of-the-art neural machine translation (NMT) systems are data-hungry and
perform poorly on domains with little supervised data. As data collection is
expensive and infeasible in many cases, unsupervised domain adaptation methods
are needed. We apply an Iterative Back Translation (IBT) training scheme on
in-domain monolingual data, which repeatedly uses a Transformer-based NMT model
to create in-domain pseudo-parallel sentence pairs in one translation direction
on the fly and then use them to train the model in the other direction.
Evaluated on three domains of German-to-English translation task with no
supervised data, this simple technique alone (without any out-of-domain
parallel data) can already surpass all previous domain adaptation methods---up
to +9.48 BLEU over the strongest previous method, and up to +27.77 BLEU over
the unadapted baseline. Moreover, given available supervised out-of-domain data
on German-to-English and Romanian-to-English language pairs, we can further
enhance the performance and obtain up to +19.31 BLEU improvement over the
strongest baseline, and +47.69 BLEU increment against the unadapted model.",cs.CL,"cs.CL, cs.LG",2,Computation and Language,"Computation and Language, Machine Learning",Peter Szolovits,"Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits",4,http://arxiv.org/pdf/2001.08140v1,http://arxiv.org/abs/2001.08140v1,2020-01-22
Local Policy Optimization for Trajectory-Centric Reinforcement Learning,"The goal of this paper is to present a method for simultaneous trajectory and
local stabilizing policy optimization to generate local policies for
trajectory-centric model-based reinforcement learning (MBRL). This is motivated
by the fact that global policy optimization for non-linear systems could be a
very challenging problem both algorithmically and numerically. However, a lot
of robotic manipulation tasks are trajectory-centric, and thus do not require a
global model or policy. Due to inaccuracies in the learned model estimates, an
open-loop trajectory optimization process mostly results in very poor
performance when used on the real system. Motivated by these problems, we try
to formulate the problem of trajectory optimization and local policy synthesis
as a single optimization problem. It is then solved simultaneously as an
instance of nonlinear programming. We provide some results for analysis as well
as achieved performance of the proposed technique under some simplifying
assumptions.",cs.LG,"cs.SY, stat.ML, eess.SY, cs.RO, cs.LG",5,Machine Learning,"Systems and Control, Machine Learning, Robotics, Machine Learning",Daniel Nikovski,"Patrik Kolaric, Devesh K. Jha, Arvind U. Raghunathan, Frank L. Lewis, Mouhacine Benosman, Diego Romeres, Daniel Nikovski",7,http://arxiv.org/pdf/2001.08092v1,http://arxiv.org/abs/2001.08092v1,2020-01-22
"Stratified cross-validation for unbiased and privacy-preserving
  federated learning","Large-scale collections of electronic records constitutes both an opportunity
for the development of more accurate prediction models and a threat for
privacy. To limit privacy exposure new privacy-enhancing techniques are
emerging such as federated learning which enables large-scale data analysis
while avoiding the centralization of records in a unique database that would
represent a critical point of failure. Although promising regarding privacy
protection, federated learning prevents using some data-cleaning algorithms
thus inducing new biases. In this work we focus on the recurrent problem of
duplicated records that, if not handled properly, may cause over-optimistic
estimations of a model's performances. We introduce and discuss stratified
cross-validation, a validation methodology that leverages stratification
techniques to prevent data leakage in federated learning settings without
relying on demanding deduplication algorithms.",stat.ML,"stat.ML, stat.ME, cs.LG",3,Machine Learning,"Machine Learning, Methodology, Machine Learning",R. Porcher,"R. Bey, R. Goussault, M. Benchoufi, R. Porcher",4,http://arxiv.org/pdf/2001.08090v1,http://arxiv.org/abs/2001.08090v1,2020-01-22
"Contextualized Embeddings in Named-Entity Recognition: An Empirical
  Study on Generalization","Contextualized embeddings use unsupervised language model pretraining to
compute word representations depending on their context. This is intuitively
useful for generalization, especially in Named-Entity Recognition where it is
crucial to detect mentions never seen during training. However, standard
English benchmarks overestimate the importance of lexical over contextual
features because of an unrealistic lexical overlap between train and test
mentions. In this paper, we perform an empirical analysis of the generalization
capabilities of state-of-the-art contextualized embeddings by separating
mentions by novelty and with out-of-domain evaluation. We show that they are
particularly beneficial for unseen mentions detection, especially
out-of-domain. For models trained on CoNLL03, language model contextualization
leads to a +1.2% maximal relative micro-F1 score increase in-domain against
+13% out-of-domain on the WNUT dataset",cs.CL,"cs.CL, cs.LG",2,Computation and Language,"Computation and Language, Machine Learning",Patrick Gallinari,"Bruno Taillé, Vincent Guigue, Patrick Gallinari",3,http://arxiv.org/pdf/2001.08053v1,http://arxiv.org/abs/2001.08053v1,2020-01-22
"On Last-Layer Algorithms for Classification: Decoupling Representation
  from Uncertainty Estimation","Uncertainty quantification for deep learning is a challenging open problem.
Bayesian statistics offer a mathematically grounded framework to reason about
uncertainties; however, approximate posteriors for modern neural networks still
require prohibitive computational costs. We propose a family of algorithms
which split the classification task into two stages: representation learning
and uncertainty estimation. We compare four specific instances, where
uncertainty estimation is performed via either an ensemble of Stochastic
Gradient Descent or Stochastic Gradient Langevin Dynamics snapshots, an
ensemble of bootstrapped logistic regressions, or via a number of Monte Carlo
Dropout passes. We evaluate their performance in terms of \emph{selective}
classification (risk-coverage), and their ability to detect out-of-distribution
samples. Our experiments suggest there is limited value in adding multiple
uncertainty layers to deep classifiers, and we observe that these simple
methods strongly outperform a vanilla point-estimate SGD in some complex
benchmarks like ImageNet.",stat.ML,"stat.ML, cs.LG",2,Machine Learning,"Machine Learning, Machine Learning",Éric Moulines,"Nicolas Brosse, Carlos Riquelme, Alice Martin, Sylvain Gelly, Éric Moulines",5,http://arxiv.org/pdf/2001.08049v1,http://arxiv.org/abs/2001.08049v1,2020-01-22
"CodeReef: an open platform for portable MLOps, reusable automation
  actions and reproducible benchmarking","We present CodeReef - an open platform to share all the components necessary
to enable cross-platform MLOps (MLSysOps), i.e. automating the deployment of ML
models across diverse systems in the most efficient way. We also introduce the
CodeReef solution - a way to package and share models as non-virtualized,
portable, customizable and reproducible archive files. Such ML packages include
JSON meta description of models with all dependencies, Python APIs, CLI actions
and portable workflows necessary to automatically build, benchmark, test and
customize models across diverse platforms, AI frameworks, libraries, compilers
and datasets. We demonstrate several CodeReef solutions to automatically build,
run and measure object detection based on SSD-Mobilenets, TensorFlow and COCO
dataset from the latest MLPerf inference benchmark across a wide range of
platforms from Raspberry Pi, Android phones and IoT devices to data centers.
Our long-term goal is to help researchers share their new techniques as
production-ready packages along with research papers to participate in
collaborative and reproducible benchmarking, compare the different
ML/software/hardware stacks and select the most efficient ones on a Pareto
frontier using online CodeReef dashboards.",cs.LG,"stat.ML, cs.SE, cs.LG",3,Machine Learning,"Machine Learning, Software Engineering, Machine Learning",Nicolas Essayan,"Grigori Fursin, Herve Guillou, Nicolas Essayan",3,http://arxiv.org/pdf/2001.07935v1,http://arxiv.org/abs/2001.07935v1,2020-01-22
Model-Based Cloud Resource Provisioning with TOSCA and OCCI,"With the advent of cloud computing, different cloud providers with
heterogeneous cloud services (compute, storage, network, applications, etc.)
and their related Application Programming Interfaces (APIs) have emerged. This
heterogeneity complicates the implementation of an interoperable cloud system.
Several standards have been proposed to address this challenge and provide a
unified interface to cloud resources. The Open Cloud Computing Interface (OCCI)
thereby focuses on the standardization of a common API for
Infrastructure-as-a-Service (IaaS) providers while the Topology and
Orchestration Specification for Cloud Applications (TOSCA) focuses on the
standardization of a template language to enable the proper definition of the
topology of cloud applications and their orchestrations on top of a cloud
system. TOSCA thereby does not define how the application topologies are
created on the cloud. Therefore, we analyse the conceptual similarities between
the two approaches and we study how we can integrate them to obtain a complete
standard-based approach to manage both cloud infrastructure and cloud
application layers. We propose an automated extensive mapping between the
concepts of the two standards and we provide TOSCA Studio, a model-driven tool
chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically
design cloud applications as well as to deploy and manage them at runtime using
a fully model-driven cloud orchestrator based on the two standards. Our
contribution is validated by successfully designing and deploying two cloud
applications: WordPress and Node Cellar.",cs.SE,cs.SE,1,Software Engineering,Software Engineering,Philippe Merle,"Stéphanie Challita, Fabian Korte, Johannes Erbel, Faiez Zalila, Jens Grabowski, Philippe Merle",6,http://arxiv.org/pdf/2001.07900v1,http://arxiv.org/abs/2001.07900v1,2020-01-22
CDST: A Toolkit for Testing Cockpit Display Systems of Avionics,"Avionics are highly critical systems that require extensive testing governed
by international safety standards. Cockpit Display Systems (CDS) are an
essential component of modern aircraft cockpits and display information from
the user application using various widgets. A significant step in the testing
of avionics is to evaluate whether these CDS are displaying the correct
information. A common industrial practice is to manually test the information
on these CDS by taking the aircraft into different scenarios during the
simulation. Given the large number of scenarios to test, manual testing of such
behavior is a laborious activity. In this paper, we present a CDST toolkit that
automates the testing of Cockpit Display Systems (CDS). We discuss the workflow
and architecture of the tool and also demonstrates the tool on an industrial
case study. The results show that the tool is able to generate, execute, and
evaluate the test cases and identify 3 bugs in the case study.",cs.SE,cs.SE,1,Software Engineering,Software Engineering,Muhammad Uzair Khan,"Hassan Sartaj, Muhammad Zohaib Iqbal, Muhammad Uzair Khan",3,http://arxiv.org/pdf/2001.07869v1,http://arxiv.org/abs/2001.07869v1,2020-01-22
"Oracle Efficient Estimation of Structural Breaks in Cointegrating
  Regressions","In this paper, we propose an adaptive group lasso procedure to efficiently
estimate structural breaks in cointegrating regressions. It is well-known that
the group lasso estimator is not simultaneously estimation consistent and model
selection consistent in structural break settings. Hence, we use a first step
group lasso estimation of a diverging number of breakpoint candidates to
produce weights for a second adaptive group lasso estimation. We prove that
parameter changes are estimated consistently by group lasso if it is tuned
correctly and show that the number of estimated breaks is greater than the true
number but still sufficiently close to it. Then, we use these results and prove
that the adaptive group lasso has oracle properties if weights are obtained
from our first step estimation and the tuning parameter satisfies some further
restrictions. Simulation results show that the proposed estimator delivers the
expected results. An economic application to the long-run US money demand
function demonstrates the practical importance of this methodology.",econ.EM,"stat.ML, 62E20, 62M10, 91B84, econ.EM",3,Econometrics,"Machine Learning, Econometrics",Karsten Schweikert,Karsten Schweikert,1,http://arxiv.org/pdf/2001.07949v1,http://arxiv.org/abs/2001.07949v1,2020-01-22
"Rank Bounds for Approximating Gaussian Densities in the Tensor-Train
  Format","Low rank tensor approximations have been employed successfully, for example,
to build surrogate models that can be used to speed up large-scale inference
problems in high dimensions. The success of this depends critically on the rank
that is necessary to represent or approximate the underlying distribution. In
this paper, we develop a-priori rank bounds for approximations in the
functional Tensor-Train representation for the case of a Gaussian (normally
distributed) model. We show that under suitable conditions on the precision
matrix, we can represent the Gaussian density to high accuracy without
suffering from an exponential growth of complexity as the dimension increases.
Our results provide evidence of the suitability and limitations of low rank
tensor methods in a simple but important model case. Numerical experiments
confirm that the rank bounds capture the qualitative behavior of the rank
structure when varying the parameters of the precision matrix and the accuracy
of the approximation.",math.NA,"math.ST, math.NA, 15A23, 15A69, 65C60, 65D32, 65D15, 41A10, cs.NA, stat.TH",5,Numerical Analysis,"Statistics Theory, Numerical Analysis, Numerical Analysis, Statistics Theory",Robert Scheichl,"Paul B. Rohrbach, Sergey Dolgov, Lars Grasedyck, Robert Scheichl",4,http://arxiv.org/pdf/2001.08187v1,http://arxiv.org/abs/2001.08187v1,2020-01-22
Estimating the reach of a manifold via its convexity defect function,"The reach of a submanifold is a crucial regularity parameter for manifold
learning and geometric inference from point clouds. This paper relates the
reach of a submanifold to its convexity defect function. Using the stability
properties of convexity defect functions, along with some new bounds and the
recent submanifold estimator of Aamari and Levrard [Ann. Statist. 47 177-204
(2019)], an estimator for the reach is given. A uniform expected loss bound
over a C^k model is found. Lower bounds for the minimax rate for estimating the
reach over these models are also provided. The estimator almost achieves these
rates in the C^3 and C^4 cases, with a gap given by a logarithmic factor.",math.ST,"math.ST, cs.CG, math.DG, stat.TH, 62G05 (Primary) 62C20, 53A07, 53C40 (Secondary)",5,Statistics Theory,"Statistics Theory, Computational Geometry, Differential Geometry, Statistics Theory",Krishnan Shankar,"Clément Berenfeld, John Harvey, Marc Hoffmann, Krishnan Shankar",4,http://arxiv.org/pdf/2001.08006v1,http://arxiv.org/abs/2001.08006v1,2020-01-22
"A new goodness of fit test for normal distribution based on Stein's
  characterization","We develop a new non-parametric test for testing normal distribution using
Stein's characterization. We study asymptotic properties of the test statistic.
We also develop jackknife empirical likelihood ratio test for testing
normality. Using Monte Carlo simulation study, we evaluate the finite sample
performance of the proposed JEL based test. Finally, we illustrate our test
procedure using two real data.",math.ST,"62G10, 62G20, stat.TH, math.ST",3,Statistics Theory,"Statistics Theory, Statistics Theory",Sudheesh Kattumannil,Sudheesh Kattumannil,1,http://arxiv.org/pdf/2001.07932v1,http://arxiv.org/abs/2001.07932v1,2020-01-22
Learning functions varying along an active subspace,"Many functions of interest are in a high-dimensional space but exhibit
low-dimensional structures. This paper studies regression of a $s$-H\""{o}lder
function $f$ in $\mathds{R}^D$ which varies along an active subspace of
dimension $d$ while $d\ll D$. A direct approximation of $f$ in $\mathds{R}^D$
with an $\varepsilon$ accuracy requires the number of samples $n$ in the order
of $\varepsilon^{-(2s+D)/s}$. %by the well-known curse of dimensionality. In
this paper, we modify the Generalized Contour Regression (GCR) algorithm to
estimate the active subspace and use piecewise polynomials for function
approximation. GCR is among the best estimators for the active subspace, but
its sample complexity is an open question. Our modified GCR improves the
efficiency over the original GCR and leads to an mean squared estimation error
of $O(n^{-1})$ for the active subspace, when $n$ is sufficiently large. The
mean squared regression error of $f$ is proved to be in the order of
$\left(n/\log n\right)^{-\frac{2s}{2s+d}}$ where the exponent depends on the
dimension of the active subspace $d$ instead of the ambient space $D$. This
result demonstrates that GCR is effective in learning low-dimensional active
subspaces. The convergence rate is validated through several numerical
experiments.",math.ST,"stat.ML, stat.TH, math.ST, 62-07, 62G08",4,Statistics Theory,"Machine Learning, Statistics Theory, Statistics Theory",Wenjing Liao,"Hao Liu, Wenjing Liao",2,http://arxiv.org/pdf/2001.07883v1,http://arxiv.org/abs/2001.07883v1,2020-01-22
Knockoffs with Side Information,"We consider the problem of assessing the importance of multiple variables or
factors from a dataset when side information is available. In principle, using
side information can allow the statistician to pay attention to variables with
a greater potential, which in turn, may lead to more discoveries. We introduce
an adaptive knockoff filter, which generalizes the knockoff procedure (Barber
and Cand\`es, 2015; Cand\`es et al., 2018) in that it uses both the data at
hand and side information to adaptively order the variables under study and
focus on those that are most promising. Adaptive knockoffs controls the
finite-sample false discovery rate (FDR) and we demonstrate its power by
comparing it with other structured multiple testing methods. We also apply our
methodology to real genetic data in order to find associations between genetic
variants and various phenotypes such as Crohn's disease and lipid levels. Here,
adaptive knockoffs makes more discoveries than reported in previous studies on
the same datasets.",stat.ME,"stat.AP, stat.ME, math.ST, stat.TH",4,Methodology,"Applications, Methodology, Statistics Theory, Statistics Theory",Emmanuel Candès,"Zhimei Ren, Emmanuel Candès",2,http://arxiv.org/pdf/2001.07835v1,http://arxiv.org/abs/2001.07835v1,2020-01-22
"Dielectric particle lofting from dielectric substrate exposed to low
  energy electron beam","The particle-in-cell simulation is applied to study a nanometer-sized
dielectric particle lofting from a dielectric substrate exposed to a low energy
electron beam. The article discusses the electron accumulation between such a
substrate and a particle lying on it, that can cause a particle lofting. The
results are of interest for dust mitigation in the semiconductor industry, the
lunar exploration and the explanation of the dust levitation.",physics.app-ph,"physics.app-ph, physics.comp-ph, physics.plasm-ph",3,Applied Physics,"Applied Physics, Computational Physics, Plasma Physics",A. M. Yakunin,"P. V. Krainov, V. V. Ivanov, D. I. Astakhov, M. A. van de Kerkhof, V. V. Kvon, V. V. Medvedev, A. M. Yakunin",7,http://arxiv.org/pdf/2001.07992v1,http://arxiv.org/abs/2001.07992v1,2020-01-22
"Cohort state-transition models in R: From conceptualization to
  implementation","Decision models can synthesize evidence from different sources to provide
estimates of long-term consequences of a decision with uncertainty. Cohort
state-transition models (cSTM) are decision models commonly used in medical
decision making because they can simulate hypothetical cohorts' transitions
across various health states over time. This tutorial shows how to
conceptualize cSTMs in a programming language environment and shows examples of
their implementation in R. We illustrate their use in a cost-effectiveness
analysis of a treatment using a previously published testbed cSTM. Both
time-independent cSTM where transition probabilities are constant over time and
time-dependent cSTM where transition probabilities vary over time are
represented. For the time-dependent cSTM, we consider transition probabilities
dependent on age and state residence. We also illustrate how this setup can
facilitate the computation of epidemiological outcomes of interest, such as
survival and prevalence. We conclude by demonstrating how to calculate economic
outcomes and conducting a cost-effectiveness analysis of a treatment compared
to usual care using the testbed model. We provide a link to a public repository
with all the R code described in this tutorial that can be used to replicate
the example or to be modified to suit different decision modeling needs.",stat.AP,"G.3; I.6.5; J.3; J.4, stat.AP, stat.CO, 97M60 (Primary) 92D30 92B15, 60J10 (Secondary), q-bio.QM",5,Applications,"Applications, Computation, Quantitative Methods",Hawre Jalal,"Fernando Alarid-Escudero, Eline M. Krijkamp, Eva A. Enns, M. G. Myriam Hunink, Petros Pechlivanoglou, Hawre Jalal",6,http://arxiv.org/pdf/2001.07824v1,http://arxiv.org/abs/2001.07824v1,2020-01-22
"Multiple imputation in functional regression with applications to EEG
  data in a depression study","Methods for estimating parameters in functional regression models require
complete data on both the response and the predictors. However, in many
applications, complete data are not available for all subjects. While many
methods are available to handle missingness in data sets with all scalar
variables, no such methods exist for data sets that include functional
variables. We propose an approach that is an extension of multiple imputation
by chained equations (fregMICE). fregMICE handles both scalar and functional
variables as predictors in the imputation models as well as scalar and
functional outcomes that need to be imputed. We also propose an extension to
Rubin's Rules that can be used to pool estimates from the multiply imputed data
sets and conduct valid inference. Simulation results suggest that the proposed
methods are superior to both complete case analysis and mean imputation in the
context of estimating parameters in functional regression models. We employ the
proposed methods in fitting a functional regression model for the relationship
between major depressive disorder and frontal asymmetry curves derived from
electroencephalography.",stat.AP,stat.AP,1,Applications,Applications,Ofer Harel,"Adam Ciarleglio, Eva Petkova, Ofer Harel",3,http://arxiv.org/pdf/2001.08175v1,http://arxiv.org/abs/2001.08175v1,2020-01-22
"Comparing the Performance of Statistical Adjustment Methods By
  Recovering the Experimental Benchmark from the REFLUX Trial","Much evidence in comparative effectiveness research is based on observational
studies. Researchers who conduct observational studies typically assume that
there are no unobservable differences between the treated and control groups.
Treatment effects are estimated after adjusting for observed differences
between treated and controls. However, treatment effect estimates may be biased
due to model misspecification. That is, if the method of treatment effect
estimation imposes unduly strong functional form assumptions, treatment effect
estimates may be significantly biased. In this study, we compare the
performance of a wide variety of treatment effect estimation methods. We do so
within the context of the REFLUX study from the UK. In REFLUX, after study
qualification, participants were enrolled in either a randomized trial arm or
patient preference arm. In the randomized trial, patients were randomly
assigned to either surgery or medical management. In the patient preference
arm, participants selected to either have surgery or medical management. We
attempt to recover the treatment effect estimate from the randomized trial arm
using the data from the patient preference arm of the study. We vary the method
of treatment effect estimation and record which methods are successful and
which are not. We apply over 20 different methods including standard regression
models as well as advanced machine learning methods. We find that simple
propensity score matching methods perform the worst. We also find significant
variation in performance across methods. The wide variation in performance
suggests analysts should use multiple methods of estimation as a robustness
check.",stat.AP,stat.AP,1,Applications,Applications,Richard Grieve,"Luke Keele, Stephen O'Neill, Richard Grieve",3,http://arxiv.org/pdf/2001.08170v1,http://arxiv.org/abs/2001.08170v1,2020-01-22
Estimation of Latent Network Flows in Bike-Sharing Systems,"Estimation of latent network flows is a common problem in statistical network
analysis. The typical setting is that we know the margins of the network, i.e.
in- and outdegrees, but the flows are unobserved. In this paper, we develop a
mixed regression model to estimate network flows in a bike-sharing network if
only the hourly differences of in- and outdegrees at bike stations are known.
We also include exogenous covariates such as weather conditions. Two different
parameterizations of the model are considered to estimate 1) the whole network
flow and 2) the network margins only. The estimation of the model parameters is
proposed via an iterative penalized maximum likelihood approach. This is
exemplified by modeling network flows in the Vienna Bike-Sharing Network.
Furthermore, a simulation study is conducted to show the performance of the
model. For practical purposes it is crucial to predict when and at which
station there is a lack or an excess of bikes. For this application, our model
shows to be well suited by providing quite accurate predictions.",stat.AP,stat.AP,1,Applications,Applications,Göran Kauermann,"Marc Schneble, Göran Kauermann",2,http://arxiv.org/pdf/2001.08146v1,http://arxiv.org/abs/2001.08146v1,2020-01-22
"Maximum Likelihood Estimation of Spatially Varying Coefficient Models
  for Large Data with an Application to Real Estate Price Prediction","In regression models for spatial data, it is often assumed that the marginal
effects of covariates on the response are constant over space. In practice,
this assumption might often be questionable. In this article, we show how a
Gaussian process-based spatially varying coefficient (SVC) model can be
estimated using maximum likelihood estimation (MLE). In addition, we present an
approach that scales to large data by applying covariance tapering. We compare
our methodology to existing methods such as a Bayesian approach using the
stochastic partial differential equation (SPDE) link, geographically weighted
regression (GWR), and eigenvector spatial filtering (ESF) in both a simulation
study and an application where the goal is to predict prices of real estate
apartments in Switzerland. The results from both the simulation study and
application show that the MLE approach results in increased predictive accuracy
and more precise estimates. Since we use a model-based approach, we can also
provide predictive variances. In contrast to existing model-based approaches,
our method scales better to data where both the number of spatial points is
large and the number of spatially varying covariates is moderately-sized, e.g.,
above ten.",stat.ME,"stat.AP, stat.ME",2,Methodology,"Applications, Methodology",Reinhard Furrer,"Jakob A. Dambon, Fabio Sigrist, Reinhard Furrer",3,http://arxiv.org/pdf/2001.08089v1,http://arxiv.org/abs/2001.08089v1,2020-01-22
"A numerically stable algorithm for integrating Bayesian models using
  Markov melding","When statistical analyses consider multiple data sources, Markov melding
provides a method for combining the source-specific Bayesian models. Models
often contain different quantities of information due to variation in the
richness of model-specific data, or availability of model-specific prior
information. We show that this can make the multi-stage Markov chain Monte
Carlo sampler employed by Markov melding unstable and unreliable. We propose a
robust multi-stage algorithm that estimates the required prior marginal
self-density ratios using weighted samples, dramatically improving accuracy in
the tails of the distribution, thus stabilising the algorithm and providing
reliable inference. We demonstrate our approach using an evidence synthesis for
inferring HIV prevalence, and an evidence synthesis of A/H1N1 influenza.",stat.ME,"stat.AP, stat.ME, stat.CO",3,Methodology,"Applications, Methodology, Computation",Robert J. B. Goudie,"Andrew A. Manderson, Robert J. B. Goudie",2,http://arxiv.org/pdf/2001.08038v1,http://arxiv.org/abs/2001.08038v1,2020-01-22
"A Deep Learning Algorithm for High-Dimensional Exploratory Item Factor
  Analysis","Deep learning methods are the gold standard for non-linear statistical
modeling in computer vision and in natural language processing but are rarely
used in psychometrics. To bridge this gap, we present a novel deep learning
algorithm for exploratory item factor analysis (IFA). Our approach combines a
deep artificial neural network (ANN) model called a variational autoencoder
(VAE) with recent work that uses regularization for exploratory factor
analysis. We first provide overviews of ANNs and VAEs. We then describe how to
conduct exploratory IFA with a VAE and demonstrate our approach in two
empirical examples and in two simulated examples. Our empirical results were
consistent with existing psychological theory across random starting values.
Our simulations suggest that the VAE consistently recovers the data generating
factor pattern with moderate-sized samples. Secondary loadings were
underestimated with a complex factor structure and intercept parameter
estimates were moderately biased with both simple and complex factor
structures. All models converged in minutes, even with hundreds of thousands of
observations, hundreds of items, and tens of factors. We conclude that the VAE
offers a powerful new approach to fitting complex statistical models in
psychological and educational measurement.",stat.ME,"stat.ML, stat.ME, cs.LG",3,Methodology,"Machine Learning, Methodology, Machine Learning",Daniel J. Bauer,"Christopher J. Urban, Daniel J. Bauer",2,http://arxiv.org/pdf/2001.07859v1,http://arxiv.org/abs/2001.07859v1,2020-01-22
Optimal binning: mathematical programming formulation,"The optimal binning is the optimal discretization of a variable into bins
given a discrete or continuous numeric target. We present a rigorous and
extensible mathematical programming formulation to solving the optimal binning
problem for a binary, continuous and multi-class target type, incorporating
constraints not previously addressed. For all three target types, we introduce
a convex mixed-integer programming formulation. Several algorithmic
enhancements such as automatic determination of the most suitable monotonic
trend via a Machine-Learning-based classifier and implementation aspects are
thoughtfully discussed. The new mathematical programming formulations are
carefully implemented in the open-source python library OptBinning.",cs.LG,"stat.ML, math.OC, cs.LG",3,Machine Learning,"Machine Learning, Optimization and Control, Machine Learning",Guillermo Navas-Palencia,Guillermo Navas-Palencia,1,http://arxiv.org/pdf/2001.08025v1,http://arxiv.org/abs/2001.08025v1,2020-01-22
